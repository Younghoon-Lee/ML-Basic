{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1[문제]__Gradient_Descent.ipynb의 사본",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPhRpnGbllOL"
      },
      "source": [
        "# Assignment 1: Gradient Descent\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "본 과제에서는 경사하강법(Gradient Descent)를 구현해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rPsh3Gm07LV"
      },
      "source": [
        "## 1. Gradient Descent (1)\n",
        "\n",
        "먼저 [SymPy library](https://www.sympy.org/en/index.html)를 사용하여 간단한 이차함수의 최솟값을 gradient descent 방법으로 찾는 문제입니다. - AI Math 3강 참고\n",
        "\n",
        "(SymPy: 수학 방정식의 기호(symbol)를 사용하게 해 주는 라이브러리)\n",
        "\n",
        " #### **def func**\n",
        "- `sym.poly` 함수는 함수식을 정의해줍니다.\n",
        "\n",
        "- `sym.subs` 함수는 변수를 다른변수로 치환하거나 값을 대입해줍니다.\n",
        "\n",
        "\n",
        "#### **func_gradient**\n",
        "- `sym.diff` 함수는 도함수를 구해줍니다.\n",
        "- `func` 함수를 사용하여 미분값과 함수를 return하는 코드를 짜야합니다.\n",
        "\n",
        "#### **gradient_descent**\n",
        "- `init_point`는 경사하강법의 시작점을 의미합니다.\n",
        "- `lr_rate`는 learning rate로 step의 크기를 정해줍니다.\n",
        "- `epsilon`은 gradient 크기의 lower bound입니다.\n",
        "- init_point부터 경사하강법을 시작해서, 최소점이 출력될 수 있도록 코드를 짜야합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bnWEgmKuJ1J"
      },
      "source": [
        "import numpy as np\n",
        "import sympy as sym\n",
        "from sympy.abc import x\n",
        "from sympy.plotting import plot\n",
        "\n",
        "def func(val):\n",
        "    fun = sym.poly(x**2 + 2*x + 3)\n",
        "    return fun.subs(x, val), fun\n",
        "\n",
        "def func_gradient(fun, val):\n",
        "    ## TODO\n",
        "    diff = sym.poly(sym.diff(fun))\n",
        "\n",
        "    return diff.subs(x, val), diff\n",
        "\n",
        "def gradient_descent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):\n",
        "    cnt = 0\n",
        "    val = init_point\n",
        "    _, fun = func(val)\n",
        "    diff, _ = func_gradient(fun,val)\n",
        "    while (diff > epsilon):\n",
        "      step = lr_rate * diff\n",
        "      val -= step\n",
        "      diff, _ = func_gradient(fun, val)\n",
        "      cnt +=1\n",
        "    ## Todo\n",
        "    \n",
        "    print(\"함수: {}\\n연산횟수: {}\\n최소점: ({}, {})\".format(func(val)[1], cnt, val, func(val)[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bROZY7uHCx3X"
      },
      "source": [
        "- 최종적으로 함수, 연산횟수, 최소점(-1,2)이 출력되면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFdjdR5vC0bG"
      },
      "source": [
        "gradient_descent(fun = func, init_point = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udMVpeMaXJhB"
      },
      "source": [
        "## 2. Gradient Descent (2)\n",
        "\n",
        "- 위의 예제에서 (-1,2)와 거의 동일한 최소점을 얻으셨나요? 그럼 이번에는 sympy library를 사용하지 않고 직접 Gradient Descent를 구현해봅시다!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ygwhh4m2ex"
      },
      "source": [
        "$$ f'(x) = \\lim_{h \\rightarrow\n",
        " 0} \\frac{f(x+h)-f(x)}{h} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nYj3r-Pm1Ln"
      },
      "source": [
        "- 원래의 미분 공식은 위와 같이 h를 0의 극한으로 보내면 되지만, 컴퓨터 상에서는 불가능하기 때문에 h에 1e-9와 같이 아주 작은 수를 넣어줌으로써 유사한 변화율을 구합니다.\n",
        "\n",
        "#### **difference_quotient(f, x, h)**\n",
        "- h만큼 움직였을 때의 변화율을 계산해주는 코드입니다.\n",
        "- h는 1e-9와 같이 매우 작은 수가 들어갑니다.\n",
        "- f에는 아래에 정의된 func 함수가 들어가고, x에는 변화율을 계산할 point가 들어갑니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4Lj4vl1KEDn"
      },
      "source": [
        "def func(val):\n",
        "    fun = sym.poly(x**2 + 2*x + 3)\n",
        "    return fun.subs(x, val)\n",
        "\n",
        "def difference_quotient(f, x, h=1e-9):\n",
        "    ## Todo\n",
        "    f = func\n",
        "    result = (f(x+h)-f(x))/h\n",
        "    return result\n",
        "\n",
        "def gradient_descent(func, init_point, lr_rate=1e-2, epsilon=1e-5):\n",
        "    cnt = 0\n",
        "    val = init_point\n",
        "    diff = difference_quotient(func, val)\n",
        "    while (diff > epsilon):\n",
        "      step = lr_rate * diff\n",
        "      val -= step\n",
        "      diff = difference_quotient(func, val)\n",
        "      cnt += 1\n",
        "    ## Todo\n",
        "\n",
        "    print(\"연산횟수: {}\\n최소점: ({}, {})\".format(cnt, val, func(val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HzMqhLxC60r"
      },
      "source": [
        "- 최종적으로 연산횟수, 최소점(-1,2)이 출력되면 됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg_K54iaC-rS"
      },
      "source": [
        "gradient_descent(func, init_point=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7C4-EzC4azr"
      },
      "source": [
        "## 3. Linear Regression\n",
        "- sympy library를 사용했을 때와 비슷한 결과값을 얻었나요?\n",
        "- 그럼 이제 본격적으로 Gradient Descent를 활용한 Linear Regression을 구현해 봅시다.\n",
        "\n",
        "### **3-1. Basic function**\n",
        "#### **Dataset 1** : train_x, train_y\n",
        "$$y = wx + b$$ $$y = 7x + 2$$\n",
        "- 위의 식을 알아내기 위해서 train_x, train_y의 data point를 가지고 Linear Regression을 진행합니다.\n",
        "\n",
        "#### **Gradient Descent**\n",
        "- error를 어떻게 계산하고, gradient를 통해 w와 b를 어떻게 조정해 나가는지에 대한 코드를 짜주시면 됩니다.\n",
        "    - error는 L2 norm으로 사용하고, np.sum 함수를 활용하시면 됩니다.\n",
        "- 결과값이 (7,2)와 유사하게 출력이 되면 성공입니다 !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4yOk4jR4aog"
      },
      "source": [
        "from sympy.abc import W, B\n",
        "train_x = (np.random.rand(1000) - 0.5) * 10\n",
        "train_y = np.zeros_like(train_x)\n",
        "\n",
        "def func(val):\n",
        "    fun = sym.poly(7*x + 2)\n",
        "    return fun.subs(x, val)\n",
        "\n",
        "for i in range(1000):\n",
        "    train_y[i] = func(train_x[i])\n",
        "\n",
        "# initialize\n",
        "w, b = 0.0, 0.0\n",
        "\n",
        "lr_rate = 1e-2\n",
        "n_data = len(train_x)\n",
        "errors = []\n",
        "\n",
        "for i in range(100):\n",
        "    ## Todo\n",
        "    # 예측값 y\n",
        "    _y = train_x*w +b\n",
        "\n",
        "\n",
        "    # gradient\n",
        "    gradient_w = sym.poly(sym.diff(train_y[i]-(train_x[i]*W+B),W)).subs(W,w)\n",
        "    gradient_b = sym.poly(sym.diff(train_y[i]-(train_x[i]*W+B),B)).subs(B,b)\n",
        "\n",
        "    # w, b update with gradient and learning rate\n",
        "    w -= lr_rate * gradient_w\n",
        "    b -= lr_rate * gradient_b\n",
        "\n",
        "    # L2 norm과 np_sum 함수 활용해서 error 정의\n",
        "    error = train_y[i]-(train_x[i]*w+b)\n",
        "    # Error graph 출력하기 위한 부분\n",
        "    errors.append(error)\n",
        "\n",
        "print(\"w : {} / b : {} / error : {}\".format(w, b, error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqXgO77MDMvN"
      },
      "source": [
        "* 이제 plot 함수를 이용해서 full-batch gradient descent를 사용한 경우, error가 어떻게 줄어드는지 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPfF6tHbDQSR"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def plot(errors):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.ylabel('error')\n",
        "    plt.xlabel('time step')\n",
        "    plt.plot(errors)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmNczAWrDRm2"
      },
      "source": [
        "plot(errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOU3Rq3TDVHp"
      },
      "source": [
        "### **3.2. More complicated function**\n",
        "#### **Dataset 2** : train_x, train_y\n",
        "- 이번에는 좀 더 복잡한 선형식에 대한 Regression을 진행해봅시다 !\n",
        "$$ y = w_0x_0 + w_1x_1 + w_2x_2 + b $$\n",
        "$$ y = x_0 + 3x_1 + 5x_2 + 7$$\n",
        "\n",
        "- 각 element의 계수를 beta_gd로 설정 : random initialize ( 목표 정답은 [1, 3, 5, 7] )\n",
        "- 이번에는 np.transpose 함수를 활용하여 gradient를 계산해봅시다 ( AI Math 4강 참고 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbCGTSZt1LRC"
      },
      "source": [
        "train_x = np.array([[1,1,1], [1,1,2], [1,2,2], [2,2,3], [2,3,3], [1,2,3]])\n",
        "train_y = np.dot(train_x, np.array([1,3,5])) + 7\n",
        "\n",
        "# random initialize\n",
        "beta_gd = [9.4, 10.6, -3.7, -1.2]\n",
        "# for constant element\n",
        "expand_x = np.array([np.append(x, [1]) for x in train_x])\n",
        "\n",
        "for t in range(5000):\n",
        "    ## Todo\n",
        "    error = train_y - expand_x @ beta_gd\n",
        "    grad = -np.transpose(expand_x) @ error\n",
        "    beta_gd = beta_gd - 0.01 * grad\n",
        "\n",
        "print(\"After gradient descent, beta_gd : {}\".format(beta_gd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHRiic4_6LlB"
      },
      "source": [
        "## 4. Stochastic Gradient Descent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH4lCXrGDezY"
      },
      "source": [
        "- 3-1의 문제와 동일한 문제에 대해서 Stochastic Gradient Descent를 사용해봅시다.\n",
        "\n",
        "- Code와 dataset 모두 동일하게 사용하되, 기존의 Dataset으로부터 mini-batch를 구성해서 Gradient Descent를 진행해주시면 됩니다.\n",
        "    - mini-batch의 경우, **np.random.choice** 함수를 활용하셔서 1,000개의 dataset 중 10개를 뽑아서 만들어주시면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn8133mq6obv"
      },
      "source": [
        "train_x = (np.random.rand(1000) - 0.5) * 10\n",
        "train_y = np.zeros_like(train_x)\n",
        "\n",
        "def func(val):\n",
        "    fun = sym.poly(7*x + 2)\n",
        "    return fun.subs(x, val)\n",
        "\n",
        "for i in range(1000):\n",
        "    train_y[i] = func(train_x[i])\n",
        "\n",
        "# initialize\n",
        "w, b = 0.0, 0.0\n",
        "\n",
        "lr_rate = 1e-2\n",
        "n_data = 10\n",
        "errors = []\n",
        "\n",
        "for i in range(100):\n",
        "    ## Todo\n",
        "\n",
        "\n",
        "    # Error graph 출력하기 위한 부분\n",
        "    errors.append(error)\n",
        "\n",
        "print(\"w : {} / b : {} / error : {}\".format(w, b, error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZE2CsZWDj7W"
      },
      "source": [
        "- plot 함수를 이용해서 mini-batch를 사용한 stochastic gradient descent의 경우, error가 어떻게 줄어드는지 확인해봅시다.\n",
        "- 3.1의 full-batch gradient descent를 사용한 경우와 plot을 통해 비교를 해보면 차이를 좀 더 명확히 확인할 수 있습니다.\n",
        "    - full-batch의 경우, 매 epoch마다 전체 dataset을 모두 사용하여 GD를 하기 때문에 그래프가 매끄럽지만, SGD에 비하여 초기 수렴속도가 느린 편입니다.\n",
        "    - mini-batch의 경우, 매 epoch마다 mini-batch를 sampling해서 GD를 하기 때문에 그래프가 매끄럽지 않지만, 그만큼 초기에 빠르게 minimum으로 수렴하는 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCpqNbBsDidu"
      },
      "source": [
        "plot(errors)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}