{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jyx133-vzzEf"
   },
   "source": [
    "소프트맥스 연산 \n",
    "\n",
    "- 소프트맥스(softmax) 함수는 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산입니다.\n",
    "- 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GzGLRJqlh29v"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J96r4fj_tTDV"
   },
   "outputs": [],
   "source": [
    "def softmax(vec):\n",
    "  denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))\n",
    "  numerator = np.sum(denumerator, axis=-1, keepdias=True)\n",
    "  val = denumerator/numerator\n",
    "  return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE-HVilq1zJt"
   },
   "source": [
    "추론을 할 때는 원-핫(one-hot)백터로 최대값을 가진 주소만 1로 출력하는 연산을 사용해서 softmax를 사용하진 않는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-eiX_B8vqZq"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bjDUJdxa17rf"
   },
   "outputs": [],
   "source": [
    "def one_hot(val, dim):\n",
    "  return [np.eye(dim)[_] for _ in val]\n",
    "\n",
    "def one_hot_encoding(vec):\n",
    "  vec_dim = vec.shape[1]\n",
    "  vec_argmax = np.argmax(vec, axis= -1)\n",
    "  return one_hot(vec_argmax, vec_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ME0US1pLvr-T"
   },
   "source": [
    "> Activation Function(활성함수)\n",
    "\n",
    "- 활성함수는 $R$ 위에 정의된 비선형함수로서 딥러닝에서 매우 중요한 개념이다.\n",
    "- 활성할수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없다.\n",
    "- sigmoid 함수나 tanh 함수가 전통적으로 많이 쓰이던 활성 항수지만 딥러닝에서는  **ReLU 함수**를 많이 쓰고 있다.\n",
    "\n",
    "***sigmoid 함수***  : $\\sigma(x) = \\frac{1}{1+e^{-x}}$</br>  \n",
    "***tanh 함수*** : $\\tanh(x)=\\frac{e^x - e^{-x}}{e^x + e^{-x}}$</br></br>\n",
    "\n",
    "***ReLU 함수*** : $ReLU(x)=max\\{0,x\\}$</br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goPyeIVewdtC"
   },
   "source": [
    "> backporgation(역전파) 알고리즘 \n",
    "\n",
    "- 딥러닝은 역전파 알고리즘을 이용하여 각 층에 사용된 파라미터를 학습한다. (Chain Rule + Gradient Descent)\n",
    "- 윗층 Layer부터 역순으로 계산한다.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "딥러닝학습방법이해하기.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
