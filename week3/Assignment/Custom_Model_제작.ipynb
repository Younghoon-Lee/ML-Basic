{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Custom Model 제작.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [
        "nzf_SMkwQIcp",
        "A9pZE3mpaIdG",
        "Oknx2fv3Ncvn",
        "VodIGNr0gI8U",
        "95QrHvgqadqg",
        "VfkQ0jmGbSmX",
        "JQsWM-vgqV17",
        "33PuuiWGZhz5",
        "M1EGwkfCxDr7",
        "D9JlxC5WyO7X",
        "qXkF-1SC1Q0q",
        "oJ0ZuWycwsff",
        "5qM693v7A31v",
        "zfJJRMVx4uZU",
        "f3MnmB0zE-TK",
        "dgoOCMtbJHEL",
        "W-eAr6c7GEHl",
        "m7w_YfK6HXQ9",
        "LZ0zV31lX6zN",
        "TwdiREr_YLEa",
        "SHpq-zKFdQ8j",
        "VfmnV6cRuYf5",
        "Ybvq8tz4769-",
        "5d8rq3BlRjmH",
        "jpXu3zuOcjzO",
        "yBmtQly_nqR7",
        "jPvOek5YxfRg",
        "VBC_HD024zT7",
        "zLFA5jeb7dcB",
        "IJNyfmll_h_o",
        "qrEsHFT2FRDj",
        "8r1QkKGGJobo",
        "GGZDh_Tw4tpS",
        "HXUIf4hw8R_A",
        "b7RtvqiWNGen",
        "aowwiHAz3I2c",
        "q2IFscSgPqj0",
        "C5EzHAPj3LW8",
        "rSyB0wutN_Oi",
        "pSIGoFyWRoJ2",
        "u7iF0gyR3dZa",
        "Ixzc0txuqmDb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tFl6DHSdUHf"
      },
      "source": [
        "<img src=\"https://i.imgur.com/uDGD221.png\" width=100%>\n",
        "\n",
        "```\n",
        "💡 Colab Dark 모드 사용을 권장합니다\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYStbm6ZsH9a"
      },
      "source": [
        "# Custom Model 제작\n",
        "> PyTorch의 첫 번째 필수 과제에 오신 것을 환영합니다! 이번 Colab에서는 PyTorch 강의를 수강하시면서 배운 다양한 지식들 및 그 외 유용한 지식들을 실습을 통해서 활용해볼 시간을 가질 것입니다!\n",
        "- 🌍 Custom 모델 제작을 위한 Documentation 활용\n",
        "- ⭐ Custom 모델 제작을 위한 nn.Module 클래스\n",
        "- 🚀 <font color='yellow'><b>[ Optional ]</b></font> Custom 모델 제작을 위한 Github 참조해보기\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqIkyC27Z5X"
      },
      "source": [
        "## 🦆 과제와 함께할 친구 부스트캠프 오리 - 부덕이\n",
        "> 부덕이는 이제 갓 PyTorch를 시작해서 궁금한게 매우 많고 열정이 넘쳐요! \n",
        "\n",
        "```python\n",
        "🦆\n",
        "반가워요! PyTorch를 함께 공부하게 된 부덕이라고 해요!\n",
        "PyTorch가 무엇인지 늘 궁금했었는데 이번에 공부하게 되어 너무 기뻐요!\n",
        "```\n",
        "```pyton\n",
        "😊\n",
        "물론이죠! 함께 열심히 공부해봐요!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzf_SMkwQIcp"
      },
      "source": [
        "## 😉 PyTorch 라이센스\n",
        "\n",
        "PyTorch는 BSD 라이센스를 사용합니다! 자유롭게 수정하고 배포가 가능하며 심지어 상업적으로 이용이 가능합니다!\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/pytorch%20official%20-%20bsd%20license.png?raw=true)\n",
        "\n",
        "- [Adding a Contributor License Agreement for PyTorch - PyTorch](https://pytorch.org/blog/a-contributor-license-agreement-for-pytorch/#what-is-not-changing)\n",
        "- [BSD License - Wikipedia](https://en.wikipedia.org/wiki/BSD_licenses)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE5_pyW7zvbT"
      },
      "source": [
        "## 🌈 과제 종류\n",
        "> 앞으로 여러분들이 마주하게 될 과제는 다음과 같이 4개의 종류로 구성되어 있습니다!\n",
        "- 🔎 탐색 : 정보를 찾아 모험을 떠나볼까요?\n",
        "- 👨‍💻 코딩 : 자신의 손으로 코드를 작성해보세요!\n",
        "- 📖 읽기 : 자료를 눈으로 찬찬히 훑어보세요!\n",
        "- ❓ 퀴즈 : 정답을 맞혀보세요!\n",
        "\n",
        "**기타**\n",
        "- 🎁 힌트 : 과제 해결에 도움을 주는 글입니다\n",
        "- ✨ 유용한 자료 : 읽으면 좋지만 필수는 아닙니다\n",
        "- 🔥 활활 : 과제 난이도가 높습니다\n",
        "- 🔥🔥🔥 활활활화르르 : 과제 난이도가 매우 높습니다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh7DIeNXnfZN"
      },
      "source": [
        "## 🌍 Custom 모델 제작을 위한 Documentation 활용\n",
        "\n",
        "```\n",
        "💡 PyTorch 라이브러리의 Documentation을 함께 탐사하며 Custom 모델\n",
        "   제작을 위해 Documentation을 활용하는 방법을 배워볼 것입니다!\n",
        "```\n",
        "\n",
        "Documentation은 개발자들이 자신들의 라이브러리에 대한 설명을 친절히 담아놓은 문서입니다. 우리가 라이브러리를 사용하며 경험하는 많은 문제들의 해법은 Stack overflow나 google에서 찾는 medium과 같은 블로그에 대부분 잘 정리되어있지만 세부적인 내용을 알아야 하는 순간이 있습니다. 우리가 찾는 내용이 documentation을 제외한 그 어느 곳에도 존재하지 않는 경우도 있지요.\n",
        "\n",
        "Documentation은 라이브러리의 모든 세부 정보가 담겨있는 곳으로 여기에 나오는 다양한 내용들을 많이 알수록 Custom 모델을 만들때 사용할 수 있는 도구의 범위가 넓어지게 됩니다. 더 멋진 세련된 자신만의 Custom 모델을 만들기 위해 가장 중요한 초석이라고 할 수 있습니다!\n",
        "\n",
        "그럼 🦆 부덕이와 함께 떠나볼까요!\n",
        "\n",
        "\n",
        "- 🌓 Documentation과 친해지기\n",
        "- 🌓 Documentation에서 정보 탐색하기\n",
        "- 🌓 Documentation에서 찾은 기능 이해 및 활용하기\n",
        "- 🌓 Documentation 읽기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9pZE3mpaIdG"
      },
      "source": [
        "###  🌓 Documentation과 친해지기\n",
        "> 처음 접하는 Documentation을 보게 되면 그 방대한 양에 기가 질리게 마련입니다! 특히나 생소한 분야의 Documentation이라면 그 어색함은 더더욱 말할 것도 없겠죠! 내용을 이해할 필요는 없습니다! 자유롭게 Documentation을 보면서 이것저것 눌러보고 탐험해보세요! \n",
        "\n",
        "- 🔎 <font color='orange'><b>[ 탐색 ]</b></font> Documentation 둘러보기\n",
        "- ❓ <font color='red'><b>[ 퀴즈 ]</b></font> PyTorch Release Status (배포 상태)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oknx2fv3Ncvn"
      },
      "source": [
        "#### 🔎 <font color='orange'><b>[ 탐색 ]</b></font> Documentation 둘러보기\n",
        "> 💡 제시되는 항목을 따라가주시면 됩니다! 퀴즈가 아닙니다!\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "PyTorch를 알기 위해서 공식 documention을 들어가보았어요!\n",
        "PyTorch 답게 빨간색으로 포인트를 준 홈페이지가 보이네요!\n",
        "저희 함께 Documentation을 둘러보아요!\n",
        "```\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VodIGNr0gI8U"
      },
      "source": [
        "##### ✔️ 버젼 (Version)\n",
        "``` python\n",
        "🦆\n",
        "저희가 보는 이 PyTorch Documentation의 버젼을 살펴보아요!\n",
        "```\n",
        "\n",
        "- ✅ PyTorch 버젼 확인해보기\n",
        "    - `1.9.0`\n",
        "- ✅ PyTorch 버젼 클릭해서 다른 버젼 살펴보기\n",
        "    - ![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/pytorch%20version.png?raw=true)\n",
        "    - `v0.1.12`에서 시작해서 현재 `v1.9.0` 버젼\n",
        "- ✅ master (unstable) 버젼 Documentation 열어보기\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95QrHvgqadqg"
      },
      "source": [
        "##### ✔️ 목차 (Index)\n",
        "``` python\n",
        "🦆\n",
        "PyTorch에는 정말 많은 내용이 담겨져 있기 때문에\n",
        "사람이 쉽게 내용을 찾기 위해서 목차로 정리가 잘 되어있을 거에요!\n",
        "Documentation의 좌측에 목차가 위치하고 있는 것 같아요! 같이 살펴봐요!\n",
        "```\n",
        "\n",
        "- ✅ `python API` 옆의 `[ - ]` 눌러보기\n",
        "    - ![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/+%20-%20button.png?raw=true)\n",
        "- ✅ 목차 살펴보기\n",
        "    - Notes\n",
        "    - Language Bindings\n",
        "    - Python API\n",
        "    - Libraries\n",
        "    - Community\n",
        "- ✅ python API의 하위 목차 훑어보기\n",
        "- ✅ python API의 하위 목차 아무거나 하나 가볍게 읽어보기\n",
        "    - ex) `torch.nn`\n",
        "- ✅ python API - torch 문서 내부 목차 훑어보기\n",
        "    - [TORCH - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html)\n",
        "    - ![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/table%20of%20content%20-%20torch.png?raw=true)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSwYkTH_lgsz"
      },
      "source": [
        "##### ✔️ 검색 (Search)\n",
        "``` python\n",
        "🦆\n",
        "목차가 있어 원하는 내용을 찾기가 수월해보이지만\n",
        "익숙한 사람이 아니라면 원하는 내용을 찾기 위해 한참을 헤맬 것 같아요!\n",
        "우리가 원하는 내용을 바로 찾기 위해 search bar를 사용하면 될 것 같아요!\n",
        "```\n",
        "\n",
        "- ✅ search bar 위치 확인해보기\n",
        "    - ![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/pytorch%20search%20bar.png?raw=true)\n",
        "- ✅ 원하는 키워드 아무것이나 검색해보기\n",
        "    - ex) duck\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfkQ0jmGbSmX"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> PyTorch Release Status (배포 상태)\n",
        "``` python\n",
        "🦆\n",
        "PyTorch에 정말 내용이 많네요! 둘러보는 것을 멈추고\n",
        "메인 페이지로 돌아왔는데 release status에 대한 설명이 쓰여있네요!\n",
        "Stable, Beta, Prototype 3가지가 적혀져 있는데 Beta가 무엇인지 알려주세요!\n",
        "```\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X_RYE6szDXr"
      },
      "source": [
        "```python\n",
        "😮\n",
        "# TODO : 맞고 틀리고가 없는 문제입니다. 문서를 읽고 답을 자유로이 적어주세요\n",
        "\n",
        "\n",
        "Beta란 아직 온전하게 완성된 버전이 아니며, 추가적으로 사용자의 피드백으로 버젼이 바뀔 가능성이 존재한다. beta 버젼은 새로운 feature가 안정되게 작동되는 것을 목표로 하면 하위 호완되는 것을 지양한다. \n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQsWM-vgqV17"
      },
      "source": [
        "### 🌓 Documentation에서 정보 탐색하기\n",
        "> 대부분의 해법은 구글링을 통해서 바로 정답을 찾을 수 있고 지금 이 글을 읽는 모든 분들은 이미 잘 활용하실 겁니다. 하지만 Documentation의 모든 정보가 항상 구글링을 통해 나오는 것은 아니기 때문에 구글링 없이 Documentation만을 가지고 정보를 찾는 방법을 실습해볼 것입니다!\n",
        "\n",
        "- ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Documentation이 제공하는 search 기능 활용\n",
        "- 🔎 <font color='orange'><b>[ 탐색 ]</b></font> 목차를 보고 원하는 정보를 찾기 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33PuuiWGZhz5"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Documentation이 제공하는 search 기능 활용\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "PyTorch가 제공해주는 검색 기능을 이용하여서 궁금한 것을 찾아보고 싶어요!\n",
        "```\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1EGwkfCxDr7"
      },
      "source": [
        "##### 💯 PyTorch의 Linear Algebra 기능 여부\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "PyTorch는 NumPy처럼 선형대수학(Linear Algebra) 관련 기능이 있나요?\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmxHQsDxZUoK"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 있는지 없는지 자유롭게 답해주세요!\n",
        "있다 !\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9JlxC5WyO7X"
      },
      "source": [
        "##### 💯 Matrix Multiplication\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "pytorch에서 Matrix Multiplication을 하고 싶을 때 사용하는 함수는 무엇일까요?\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHpU8WUZyO7f"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정해진 형식이 없습니다! 자유롭게 답해주세요!\n",
        "torch.matmul, torch.mm 등이 있다.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXkF-1SC1Q0q"
      },
      "source": [
        "##### 💯 Norm\n",
        "> 여기서 말하는 norm은 normalization이 아니라 벡터나 행렬의 크기의 개념인 norm임에 주의해주세요!\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "저는 모델을 만들 때 벡터나 행렬의 norm을 많이 이용할 것 같아요! \n",
        "이럴때 어떤 함수를 사용하면 좋을까요?\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orfjxQuq1Q0u"
      },
      "source": [
        "```python\n",
        "😃\n",
        "# TODO : 정해진 형식이 없습니다! 자유롭게 답해주세요!\n",
        "torch.linalg.nomr 함수를 사용하면 된다\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ0ZuWycwsff"
      },
      "source": [
        "#### 🔎 <font color='orange'><b>[ 탐색 ]</b></font> 목차를 보고 원하는 정보를 찾기\n",
        "> 💡 제시되는 항목을 따라가주시면 됩니다! 퀴즈가 아닙니다!\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "나중에 PyTorch에 익숙해지면 저도 라이브러리에 제 코드를 기여하고 싶어요!\n",
        "대부분의 라이브러리가 그렇듯 PyTorch도 분명히 기여 프로세스에 대해\n",
        "작성해놓은 문서가 있을거예요! \n",
        "```\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUmgABsu8dSn"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "역시 목차에 커뮤니티 (Community)가 있네요!\n",
        "여기에 코드에 기여하고 싶은 사람들을 위한 가이드 문서가 있어요! \n",
        "```\n",
        "\n",
        "- ✅ Community 하위 목차 살펴보기\n",
        "- ✅ PyTorch Contribution Guide 문서 열어서 훑어보기\n",
        "    - [PyTorch Contribution Guide - PyTorch 공식 문서](https://pytorch.org/docs/stable/community/contribution_guide.html)\n",
        "- ✅ PyTorch Contribution Guide 문서 내부 목차 확인\n",
        "    - ![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/table%20of%20content%20-%20contribution.png?raw=true)\n",
        "- ✅ 내부 목차 - Common Mistakes to Avoid 가볍게 읽어보기\n",
        "    - [PyTorch Contribution Guide - Common Mistakes To Avoid - PyTorch 공식 문서](https://pytorch.org/docs/stable/community/contribution_guide.html#common-mistakes-to-avoid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qM693v7A31v"
      },
      "source": [
        "### 🌓 Documentation에서 찾은 기능 이해 및 활용하기\n",
        "> 이제 PyTorch documentation에서 원하는 정보를 찾는 방법을 알게 되었습니다! 하지만 단지 찾기만 하면 소용이 없겠죠! Documentation에 적힌 기능의 설명을 읽고 직접 활용할 줄 알아야 진짜 의미가 있습니다. 우리는 간단한 예제를 통해 PyTorch를 직접 사용해볼 것입니다!\n",
        "\n",
        "- ❓ <font color='red'><b>[ 퀴즈 ]</b></font> PyTorch의 기본 구성 요소 Tensor\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 사칙연산 계산하기\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 인덱싱 (Indexing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJJRMVx4uZU"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> PyTorch의 기본 구성 요소 Tensor\n",
        "``` python\n",
        "🦆\n",
        "PyTorch는 tensor가 모든 것의 기본 구성 요소라고 해서\n",
        "사용하려고 찾아보았는데 \"torch.tensor\"와 \"torch.Tensor\"\n",
        "2가지가 있더라구요! 무슨 차이가 있는 거죠?\n",
        "```\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXqdRH8h4uZc"
      },
      "source": [
        "```python\n",
        "🙃\n",
        "# TODO : 맞고 틀리고가 없는 문제입니다. 문서를 읽고 답을 자유로이 적어주세요\n",
        "근본적으로 torch.Tensor는*class이고 torch.tensor은 함수이다.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3MnmB0zE-TK"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 사칙연산 계산하기\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "저희도 이제 드디어 PyTorch를 이용해 코딩을 시작하네요!\n",
        "간단한 사칙연산으로 같이 시작해봐요!\n",
        "```\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgoOCMtbJHEL"
      },
      "source": [
        "##### 💡 더하기 (add)\n",
        "> 🦆 부덕이가 코드를 작성해주었어요!\n",
        "\n",
        "```python\n",
        "🦆\n",
        "5 + 7 계산을 먼저 제가 해볼게요!\n",
        "```\n",
        "\n",
        "- [torch.add - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.add.html?highlight=add#torch.add)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5-9T-fjE3a_",
        "outputId": "5f9cdbbe-8078-4aba-b57a-7ecb62589009"
      },
      "source": [
        "import torch\n",
        "\n",
        "A = torch.Tensor([5])\n",
        "B = torch.Tensor([7])\n",
        "\n",
        "torch.add(A, B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U9Wd_2oCLVR",
        "outputId": "64dc6dc4-770b-49ad-fea6-4bace9d7c722"
      },
      "source": [
        "# 🦆 torch.add를 + 연산자를 통해서 사용 가능해요!\n",
        "A + B"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq9h3ucECtoe",
        "outputId": "b49419e7-3310-42be-f3e6-cd51a74172da"
      },
      "source": [
        "# 🦆 계산할때 피연산자(operand)중 하나라도 tensor가 입력되면 결과는 tensor로 나와요!\n",
        "A + 7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-eAr6c7GEHl"
      },
      "source": [
        "##### 💡 복잡한 사칙연산 계산\n",
        "```python\n",
        "🦆\n",
        "(3 + 7) * 2 - 5 / 10 은 무엇일까요?\n",
        "연산자(+, -, *, /) 없이 함수를 사용해 계산해주세요!\n",
        "```\n",
        "\n",
        "각 사칙연산은 torch에서 어떤 이름을 사용할까요?\n",
        "- `+` : [torch.add](https://pytorch.org/docs/stable/generated/torch.add.html?highlight=add#torch.add)\n",
        "- `-` : torch.sub\n",
        "- `*` : torch.mul\n",
        "- `/` : torch.div"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-1cyVz1CiCK",
        "outputId": "e26d2497-74ea-4d6b-b4b6-edf4979f0b51"
      },
      "source": [
        "import torch\n",
        "\n",
        "A = torch.Tensor([3])\n",
        "B = torch.Tensor([7])\n",
        "C = torch.Tensor([2])\n",
        "D = torch.Tensor([5])\n",
        "E = torch.Tensor([10])\n",
        "\n",
        "# TODO : torch 함수를 이용해서 (3 + 7) * 2 - 5 / 10 를 계산해보세요!\n",
        "\n",
        "# 정답 19.5가 나와야 합니다!\n",
        "\n",
        "output = torch.add(A, B) # 1줄에 torch 함수 하나씩만 사용하세요!\n",
        "output = torch.sub(torch.mul(output, C), torch.div(D,E))\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if output == 19.5:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7w_YfK6HXQ9"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 인덱싱 (Indexing)\n",
        "> <font color='yellow'><b>[ Optional ]</b></font> 문제의 경우 🔥 매우 어려울 수 있습니다 🔥 Documentation만으로 도저히 이해가 안가신다면 구글링을 허용합니다. 난이도가 높아 시간이 많이 소요될 수 있기 때문에 여유가 되시는 분에게만 추천드립니다.\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "tensor에서 원하는 값만 가져오고 싶은데 어떻게 해야 좋을지 모르겠어요!\n",
        "파이썬에서 list를 indexing(인덱싱)하는 것과 비슷할까요?\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ0zV31lX6zN"
      },
      "source": [
        "##### 💡 index_select\n",
        "\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "[[1  2]\n",
        " [3  4]] 2차원 텐서에서 [1  3]이라는 값을 가져오고 싶어요!\n",
        "```\n",
        "\n",
        "- [torch.index_select - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.index_select.html?highlight=index#torch.index_select)\n",
        "\n",
        "🎁 **힌트** 🎁\n",
        "- tensor를 원하는 모양으로 바꾸고 싶을때 [torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html?highlight=view#torch.Tensor.view)라는 함수를 사용하면 됩니다!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIDKrXzXZs07",
        "outputId": "be222af6-6404-41f7-c661-5a8fc885fa0d"
      },
      "source": [
        "import torch\n",
        "\n",
        "A = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "# TODO : [1, 3]을 만드세요!\n",
        "A = (A[:,:1]).view(-1,2)\n",
        "# torch.index_select 함수를 써서 해보세요!\n",
        "output = A\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "\n",
        "if torch.all(output == torch.Tensor([1, 3])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co8YFuVtaVZr",
        "outputId": "f51b5b1b-4395-40b9-8091-7ac1a857acff"
      },
      "source": [
        "# 파이썬 리스트 인덱싱과 비슷한 방법으로 해보세요!\n",
        "output = A\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if torch.all(output == torch.Tensor([1, 3])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwdiREr_YLEa"
      },
      "source": [
        "##### 💡 2D tensor에서 대각선 요소 가져오기 - 2D gather\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "[[1  2]\n",
        " [3  4]] 2차원 텐서에서 대각선 요소만 가져와서 [1  4] 1차원 텐서를 만들고 싶어요!\n",
        "\n",
        "indexing을 통해 해보려니까 왠지 모르게 안되더라구요!\n",
        "친구가 \"torch.gather\"라는 함수를 사용하라고 귀뜸해줬어요!\n",
        "```\n",
        "- [torch.gather - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwx3LEqKbU9M",
        "outputId": "69ac48d9-09ae-4226-fae7-6adc95575e6d"
      },
      "source": [
        "import torch\n",
        "\n",
        "A = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "# torch.gather 함수를 써서 해보세요!\n",
        "output = torch.gather(A,0,torch.tensor([[0,1]]))\n",
        "print(output)\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if torch.all(output == torch.Tensor([1, 4])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 4.]])\n",
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHpq-zKFdQ8j"
      },
      "source": [
        "##### 💡 🔥 <font color='yellow'><b>[ Optional ]</b></font> 3D tensor에서 대각선 요소 가져오기 - 3D gather 🔥\n",
        "> PyTorch 숙련자에게도 약간 난이도가 있는 문제입니다. PyTorch에서 다뤄지는 대부분의 tensor는 3D 이상인 만큼 시간이 될 때 한번 즈음 연습해보는 것을 추천드립니다.\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "2D에서 성공적으로 대각선 요소를 모아서 너무 기뻐요!\n",
        "3D에도 적용시켜보고 싶은데 가능할까요?\n",
        "\n",
        "[[[1  2]\n",
        "  [3  4]]\n",
        " [[5  6]                                   [[1  4]\n",
        "  [7  8]]] 3차원 텐서에서 대각선 요소만 가져와서 [5  8]] 이라는 2차원 텐서를 만들고 싶어요!\n",
        "```\n",
        "- [torch.gather - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7_WCGcpdQ81",
        "outputId": "cfed1bbd-8281-4c94-bb48-986fcc689efc"
      },
      "source": [
        "import torch\n",
        "\n",
        "A = torch.Tensor([[[1, 2],\n",
        "                   [3, 4]],\n",
        "                  [[5, 6],\n",
        "                   [7, 8]]])\n",
        "\n",
        "# torch.gather 함수를 써서 해보세요!\n",
        "output = torch.gather(A,3)\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if torch.all(output == torch.Tensor([[1, 4], [5, 8]])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🦆 다시 도전해봐요!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfmnV6cRuYf5"
      },
      "source": [
        "##### 💡 🔥🔥🔥 <font color='yellow'><b>[ Optional ]</b></font> 임의의 크기의 3D tensor에서 대각선 요소 모으기 - 3D gather 🔥🔥🔥\n",
        "> PyTorch 숙련자에게도 어려운 문제입니다. 다양한 크기의 입력값에도 강건하게 대응할 수 있는 모델을 만드는 과정에서 부딪히게 되는 문제이기도 합니다. gather의 동작 원리를 정확히 숙지해야 하며 확장성 있는 유연한 코드를 작성해야 합니다.\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "저희는 지금까지 정해진 크기의 입력에서 대각선 요소를 가져왔는데\n",
        "임의의 크기의 3D 텐서에서도 마찬가지로 대각선 요소를 가져와서 2D 텐서를 만들 수 있을까요?\n",
        "```\n",
        "- [torch.gather - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather)\n",
        "\n",
        "🎁 **힌트** 🎁\n",
        "- 원하는 크기의 tensor를 만들기 위해 [torch.Tensor.expand](https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html?highlight=expand)를 사용할 수 있습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg_byIzJxoPC"
      },
      "source": [
        "import torch\n",
        "\n",
        "# TODO : 임의의 크기의 3D tensor에서 대각선 요소 가져와 2D로 반환하는 함수를 만드세요! \n",
        "def get_diag_element_3D(A):\n",
        "\n",
        "    output = A\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q9gz0d5rYiG",
        "outputId": "2c06d9c9-60fb-4ffc-9f4f-0a9edd6ab299"
      },
      "source": [
        "# TODO : 원하는 크기의 3D tensor를 만들어보면서 제대로 동작하는지 테스트해보세요!\n",
        "C = 1\n",
        "H = 2\n",
        "W = 3\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "A = torch.tensor([i for i in range(1, C*H*W + 1)])\n",
        "A = A.view(C, H, W)\n",
        "\n",
        "print(f\"원본 3D 행렬\\n{A}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(f\"대각선 요소를 모은 2D 행렬\")\n",
        "get_diag_element_3D(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "원본 3D 행렬\n",
            "tensor([[[1, 2, 3],\n",
            "         [4, 5, 6]]])\n",
            "--------------------------------------------------\n",
            "대각선 요소를 모은 2D 행렬\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 2, 3],\n",
              "         [4, 5, 6]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORBb-HPcJnCg",
        "outputId": "bc0cf766-f4d9-4a1a-acda-4b866a912d6c"
      },
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "A = torch.tensor([[[1]]])\n",
        "\n",
        "if torch.all(get_diag_element_3D(A) == torch.Tensor([[1]])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSbNIk3tyj8U",
        "outputId": "564db1e5-25e4-447d-f5e8-0b5e2ea65e67"
      },
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "A = torch.Tensor([[[1, 2],\n",
        "                   [3, 4]],\n",
        "                  [[5, 6],\n",
        "                   [7, 8]]])\n",
        "\n",
        "if torch.all(get_diag_element_3D(A) == torch.Tensor([[1, 4],\n",
        "                                                     [5, 8]])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🦆 다시 도전해봐요!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "H3ewV_sBJ-nm",
        "outputId": "ff6f187c-74dc-4951-ca60-60958cb81a62"
      },
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "A = torch.Tensor([[[1, 2, 3],\n",
        "                   [4, 5, 6]]])\n",
        "\n",
        "if torch.all(get_diag_element_3D(A) == torch.Tensor([[1, 5]])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5690f10ee105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                    [4, 5, 6]]])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_diag_element_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🎉🎉🎉 성공!!! 🎉🎉🎉\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fueU_tOgIzDC"
      },
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "A = torch.tensor([[[ 1,  2,  3,  4,  5],\n",
        "                   [ 6,  7,  8,  9, 10],\n",
        "                   [11, 12, 13, 14, 15]],\n",
        "          \n",
        "                  [[16, 17, 18, 19, 20],\n",
        "                   [21, 22, 23, 24, 25],\n",
        "                   [26, 27, 28, 29, 30]]])\n",
        "\n",
        "if torch.all(get_diag_element_3D(A) == torch.Tensor([[ 1,  7, 13],\n",
        "                                                     [16, 22, 28]])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MKSgAWOJWvb"
      },
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "A = torch.tensor([[[ 1,  2,  3],\n",
        "                   [ 4,  5,  6],\n",
        "                   [ 7,  8,  9],\n",
        "                   [10, 11, 12],\n",
        "                   [13, 14, 15]],\n",
        "        \n",
        "                  [[16, 17, 18],\n",
        "                   [19, 20, 21],\n",
        "                   [22, 23, 24],\n",
        "                   [25, 26, 27],\n",
        "                   [28, 29, 30]],\n",
        "        \n",
        "                  [[31, 32, 33],\n",
        "                   [34, 35, 36],\n",
        "                   [37, 38, 39],\n",
        "                   [40, 41, 42],\n",
        "                   [43, 44, 45]]])\n",
        "\n",
        "if torch.all(get_diag_element_3D(A) == torch.Tensor([[ 1,  5,  9],\n",
        "                                                     [16, 20, 24],\n",
        "                                                     [31, 35, 39]])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h_5Q9Xejktd"
      },
      "source": [
        "### 🌓 Documentation 읽기\n",
        "> PyTorch documentation에서 원하는 정보를 찾는 것은 물론 활용도 문제없이 하게 되었습니다! 하지만 아직 PyTorch가 제공하는 다양한 기능에 어떤 것들이 있는지 모릅니다. Custom 모델을 제작함에 있어 PyTorch가 제공하는 기능을 많이 알면 많이 알수록 더 다양하고 멋진 모델을 만들어낼 수 있게 됩니다! 그렇기에 이번 섹션에서는 Documentation을 차분히 읽어보는 시간을 가질 것입니다!\n",
        "\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> torch 문서 읽기\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> torch.linalg 문서 읽기\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> torch.nn 문서 읽기\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> torch.nn `Linear Layers`\n",
        "- ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Linear vs LazyLinear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybvq8tz4769-"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> torch 문서 읽기\n",
        "``` python\n",
        "🦆\n",
        "Documentation을 잠깐 둘러보았는데 Python API 목차의 torch 문서에 \n",
        "중요하고 유용한 내용들이 가득 담겨있는 것 같아요!\n",
        "\n",
        "개발자들이 정성들여 만들어놓은 다양한 기능들이 있는데\n",
        "몰라서 활용되지 않는다면 정말 아쉬울 것 같아요!\n",
        "\n",
        "우리 같이 읽어봐요!\n",
        "```\n",
        "\n",
        "- [torch 문서 - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d8rq3BlRjmH"
      },
      "source": [
        "##### 💌 Tensors\n",
        "> 🦆 부덕이가 Tensor부분을 정리 해주었어요!\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "Tensors 부분을 제가 먼저 읽어서 정리를 해두었어요!\n",
        "제가 어떻게 해당 부분을 읽어나갔는지 공유해드릴게요!\n",
        "```\n",
        "\n",
        "- [Tensors - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html#tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOvQP1sYUfci"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "\"Tensors\"라는 이름을 보며 추측해보았는데 tensor 자료구조를 생성하거나\n",
        "관련된 특성을 추출할 수 있는 함수들이 모여있을 것 같아요!\n",
        "\n",
        "먼저 Tensors 부분을 읽어나가면서 어떤 함수들이 있는지 보았어요!\n",
        "\"is_tensor\"나 \"is_storage\"와 같은 함수명을 보아서 자료구조를\n",
        "체크하는 함수인 것 같아요!\n",
        "```\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/torch%20-%20tensors.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EYZfH4CX7tT"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "\"is_tensor\"라는 함수를 더 알고 싶어서 링크를 타고 들어갔어요!\n",
        "파란 박스 부분에 설명을 읽고 나니까 제가 생각했던 대로\n",
        "tensor 자료구조인지 체크하는 것이 맞네요!\n",
        "\n",
        "아래 빨간 박스 부분에 예제가 나와있더라구요!\n",
        "코드를 직접 쳐보면 더 이해가 잘될 것 같아서\n",
        "날개로 직접 예제를 구현해보았어요!\n",
        "```\n",
        "- [torch.is_tensor - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor)\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/torch%20is%20tensor.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-up9L8HlYzmm",
        "outputId": "31c05015-036e-43ef-c831-578e6a35457e"
      },
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1,2,3])\n",
        "y = [1, 2, 3]\n",
        "\n",
        "# 🦆 멋지게 동작하네요! 예상했던 대로에요!\n",
        "torch.is_tensor(x), torch.is_tensor(y) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-pYoFx5bYcj",
        "outputId": "8a7bbfb2-140c-48c5-e4d1-616bd83e0207"
      },
      "source": [
        "# 🦆 numel이라는 함수도 한 번 써보았어요! 잘 동작하네요!\n",
        "torch.numel(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wP5qfTMUYxq"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "스크롤을 내리다보니까 Tensor의 속하는 함수들인데\n",
        "다음의 세부 항목으로 분류했더라구요!\n",
        "- \"Creation Ops\"\n",
        "- \"Indexing, Slicing, Joining, Mutating Ops\"\n",
        "\n",
        "그래서 여기에 같이 정리하지 않고 새로운 섹션을 만들어서 정리하려고 해요!\n",
        "```\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/torch%20-%20tensors%20-%20ops.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpXu3zuOcjzO"
      },
      "source": [
        "##### 💌 Tensors - Creation Ops\n",
        "> 🦆 부덕이가 정리를 이어나가고 있어요!\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "Tensors 부분에 속하지면 따로 정리하고 싶어서\n",
        "이렇게 별도 섹션으로 나누어놓았어요!\n",
        "```\n",
        "- [Tensors - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html#tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJZJuicGdHwm"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "이름을 보면 \"Tensors\"라는 자료구조를 만드는 함수들이 모여있겠죠?\n",
        "저는 먼저 아래 그림에서 빨간 박스 친 부분을 유심히 읽어보았어요!\n",
        "\"함수명\"과 우측에 \"함수에 대한 요약 설명\"을 읽어나갔죠!\n",
        "```\n",
        "\n",
        "- ✅ 함수와 요약 설명을 가볍게 훑어보세요!\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/torch%20-%20tensors%20-%20creation%20ops.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b6lTFnCjQwc"
      },
      "source": [
        "```python\n",
        "🦆\n",
        "뭐든지 직접 손으로 쳐본게 더 이해가 잘가고 기억에 오래 남더라구요!\n",
        "그래서 함수들을 쭉 둘러보고 3가지 함수의 예제를 구현해보았어요!\n",
        "```\n",
        "\n",
        "- ✅ <font color='yellow'><b>[ Optional ]</b></font> 원하는 3개 함수를 골라서 예제 코드를 돌려보세요!\n",
        "    - from_numpy\n",
        "    - zeros\n",
        "    - zeros_like\n",
        "\n",
        "\n",
        "- [torch.from_numpy - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy)\n",
        "- [torch.zeros - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros)\n",
        "- [torch.zeros_like - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhp7vsgE8FY4",
        "outputId": "4d627900-40b0-45f7-b90f-b6f8afc291a6"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 🦆 torch.from_numpy\n",
        "a = np.array([1,2,3])\n",
        "t = torch.from_numpy(a)\n",
        "t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mzqAJDGm9Hd",
        "outputId": "94661fa9-baff-4dcd-b52f-f7477290ed3f"
      },
      "source": [
        "# 🦆 torch.zeros\n",
        "torch.zeros(2, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA2pv3pFiu3M",
        "outputId": "0aae08fe-2d6c-4332-e925-d2b4dcf7326b"
      },
      "source": [
        "# 🦆 torch.zeros_like\n",
        "torch.zeros_like(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBmtQly_nqR7"
      },
      "source": [
        "##### 💌 Tensors - Indexing, Slicing, Joining, Mutating Ops\n",
        "> 🦆 부덕이가 마저 정리해놓았어요!\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "Tensors 항목의 마지막 세부 항목이에요!\n",
        "Indexing, Slicing 등 보기만 해도 중요한 함수들이 모여있을 것 같아요!\n",
        "```\n",
        "\n",
        "- [Tensors - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html#tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4iVxzyHnqR-"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "읽다보니까 가끔은 예제 코드가 없는 함수가 종종 섞여있어요!\n",
        "이럴때는 함수 설명을 더 자세히 읽고 구현해봐야 할 거 같아요!\n",
        "```\n",
        "\n",
        "- ✅ 함수와 요약 설명을 가볍게 훑어보세요!\n",
        "- ✅ <font color='yellow'><b>[ Optional ]</b></font> 원하는 3개 함수를 골라서 예제 코드를 돌려보세요!\n",
        "    - chunk\n",
        "    - swapdims\n",
        "    - zeros_like\n",
        "\n",
        "\n",
        "- [torch.chunk - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk)\n",
        "- [torch.swapdims - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims)\n",
        "- [torch.Tensor.scatter_ - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq9zOgoznqR_",
        "outputId": "89ff7c58-99df-4c44-816d-c1b9f4c8f799"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 🦆 torch.chunk\n",
        "t = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "# 🦆 예제는 없지만 documentation을 읽어서 구현해보았어요!\n",
        "print(torch.chunk(t, 2, 0))\n",
        "print(\" \")\n",
        "print(torch.chunk(t, 3, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[1, 2, 3]]), tensor([[4, 5, 6]]))\n",
            " \n",
            "(tensor([[1],\n",
            "        [4]]), tensor([[2],\n",
            "        [5]]), tensor([[3],\n",
            "        [6]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Qy8zEPnqSA",
        "outputId": "3a33a4e1-590e-4948-8f4b-cebeade075e0"
      },
      "source": [
        "# 🦆 torch.swapdims\n",
        "x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n",
        "x\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o6mHeHeutTh",
        "outputId": "59ccb80a-3afb-40e6-a721-bdfa3f41ace0"
      },
      "source": [
        "torch.swapdims(x, 0, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0, 1],\n",
              "         [4, 5]],\n",
              "\n",
              "        [[2, 3],\n",
              "         [6, 7]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TBo37wDu4xW",
        "outputId": "5b05ec9f-16f0-4055-8946-ab25cb700573"
      },
      "source": [
        "torch.swapdims(x, 0, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0, 4],\n",
              "         [2, 6]],\n",
              "\n",
              "        [[1, 5],\n",
              "         [3, 7]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M08dZW2fnqSB",
        "outputId": "a996df29-90a2-4fc4-f77a-1208f90e9c52"
      },
      "source": [
        "# 🦆 torch.Tensor.scatter_\n",
        "src = torch.arange(1, 11).reshape((2, 5))\n",
        "src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  3,  4,  5],\n",
              "        [ 6,  7,  8,  9, 10]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6WstSp-wLU4",
        "outputId": "917c1b8c-e892-41c9-ab4d-f9f1198a920e"
      },
      "source": [
        "# 🦆 우리가 함께 공부했던 gather와 비슷한 느낌이 나요!\n",
        "index = torch.tensor([[0, 1, 2, 0]])\n",
        "torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 4, 0],\n",
              "        [0, 2, 0, 0, 0],\n",
              "        [0, 0, 3, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0yIEAw-xAdV",
        "outputId": "ec91babc-9a61-4e1c-fbd9-9046fc7f907b"
      },
      "source": [
        "index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
        "torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3, 0, 0],\n",
              "        [6, 7, 0, 0, 8],\n",
              "        [0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPvOek5YxfRg"
      },
      "source": [
        "##### 💌 Random sampling\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "랜덤 함수들은 초기화, 샘플링 등 다양한 곳에 활용할 수 있을 것 같아요!\n",
        "유용한만큼 눈여겨서 봐두면 나중에 잘 쓰일 것 같아요!\n",
        "```\n",
        "\n",
        "- [Random sampling - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html#random-sampling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAcT0HckxfR7"
      },
      "source": [
        "- ✅ 함수와 요약 설명을 가볍게 훑어보세요!\n",
        "- ✅ <font color='yellow'><b>[ Optional ]</b></font> 원하는 3개 함수를 골라서 예제 코드를 돌려보세요!\n",
        "    - seed\n",
        "    - manual_seed\n",
        "    - initial_seed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TCXjpanaRdF",
        "outputId": "e140a608-d9dc-4a21-cbf5-1effcd117510"
      },
      "source": [
        "import torch\n",
        "torch.seed()\n",
        "\n",
        "torch.initial_seed()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14873066720050693234"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBC_HD024zT7"
      },
      "source": [
        "##### 💌 Math operations - Pointwise Ops\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "연산과 관련된 다양한 함수들이 모여있네요!\n",
        "pointwise로 연산처리를 하는 함수들인가봐요!\n",
        "```\n",
        "\n",
        "- [Math operations - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html#math-operations)\n",
        "- [Pointwise - Wikipedia](https://en.wikipedia.org/wiki/Pointwise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQDWkg-S4zT9"
      },
      "source": [
        "- ✅ 함수와 요약 설명을 가볍게 훑어보세요!\n",
        "- ✅ <font color='yellow'><b>[ Optional ]</b></font> 원하는 3개 함수를 골라서 예제 코드를 돌려보세요!\n",
        "    - torch.abs()\n",
        "    - torch.acos()\n",
        "    - torch.asin()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLFA5jeb7dcB"
      },
      "source": [
        "##### 💌 Math operations - Reduction Ops\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "함수들이 조건에 따라 tensor에서 특정 값만을 가져오거나\n",
        "연산을 통해서 크기를 줄이는 등 주어진 tensor의 크기를 줄여서\n",
        "출력하기 때문에 reduction의 이름이 붙은 것 같아요!\n",
        "```\n",
        "\n",
        "- [Math operations - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html#math-operations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poy8CCOC7dcC"
      },
      "source": [
        "- ✅ 함수와 요약 설명을 가볍게 훑어보세요!\n",
        "- ✅ <font color='yellow'><b>[ Optional ]</b></font> 원하는 3개 함수를 골라서 예제 코드를 돌려보세요!\n",
        "    - torch.argmax()\n",
        "    - torch.argmin()\n",
        "    - torch.amax()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S598A0RjNsc",
        "outputId": "e86ba06e-d40a-4f4b-ac66-68bb41ba5247"
      },
      "source": [
        "import torch\n",
        "\n",
        "a = torch.rand(4,4)\n",
        "print(a)\n",
        "torch.argmax(a,dim=1)\n",
        "torch.amax(a,1)\n",
        "torch.max(a,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3322, 0.2981, 0.2168, 0.8911],\n",
            "        [0.6079, 0.7255, 0.9586, 0.7056],\n",
            "        [0.4480, 0.6878, 0.9487, 0.9473],\n",
            "        [0.3409, 0.9462, 0.8056, 0.5572]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([0.8911, 0.9586, 0.9487, 0.9462]), indices=tensor([3, 2, 2, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJNyfmll_h_o"
      },
      "source": [
        "##### 💌 Math operations - Comparison Ops\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "작은지 큰지 이런 비교와 관련된 기능을 포함하는 함수들처럼 보여요!\n",
        "if else 문만큼 유용할 것 같아요!\n",
        "```\n",
        "\n",
        "- [Math operations - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html#math-operations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOStUYzm_h_p"
      },
      "source": [
        "- ✅ 함수와 요약 설명을 가볍게 훑어보세요!\n",
        "- ✅ <font color='yellow'><b>[ Optional ]</b></font> 원하는 3개 함수를 골라서 예제 코드를 돌려보세요!\n",
        "    - torch.argsort()\n",
        "    - \n",
        "    - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0YipGtkkssS",
        "outputId": "73befdac-f645-4e0a-e4da-9fe18aa25386"
      },
      "source": [
        "import torch\n",
        "# torch.seed()\n",
        "# a = torch.randn(4,4)\n",
        "print(a)\n",
        "torch.argsort(a,dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2120, -1.7395, -0.1270, -1.6704],\n",
            "        [-0.1977,  0.2853, -0.2305, -1.0576],\n",
            "        [-0.6735, -0.9760,  0.4633, -0.3533],\n",
            "        [-0.5711, -0.3795,  0.0891, -0.3534]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 3, 0, 2],\n",
              "        [3, 2, 0, 1],\n",
              "        [1, 0, 3, 2],\n",
              "        [0, 1, 3, 2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrEsHFT2FRDj"
      },
      "source": [
        "##### 💌 Math operations - Other Operations\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "특정한 세부 항목으로 분류하기 힘든 함수들을 모아놨나봐요!\n",
        "유용해보이는 함수 몇몇 눈에 띄네요!\n",
        "특히 \"einsum\"은 배치 단위의 텐서 계산에 유용해보여서\n",
        "왠지 기억해두면 custom 모델을 만들때 사용할 일이 있을 것 같아요!\n",
        "```\n",
        "\n",
        "- [Math operations - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html#math-operations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9dwBl4SFRDk"
      },
      "source": [
        "- ✅ 함수와 요약 설명을 가볍게 훑어보세요!\n",
        "- ✅ <font color='yellow'><b>[ Optional ]</b></font> 원하는 3개 함수를 골라서 예제 코드를 돌려보세요!\n",
        "    - \n",
        "    - torch.einsum()\n",
        "    - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQtxvUhcnotR",
        "outputId": "c528471a-3f5a-4d93-bbea-08d2e3b871ea"
      },
      "source": [
        "import torch\n",
        "torch.einsum('ii',torch.rand(4,4))\n",
        "\n",
        "x = torch.arange(9).reshape(3,3)\n",
        "# print(x)\n",
        "torch.einsum('ii',x)\n",
        "\n",
        "#outer product\n",
        "x = torch.arange(5)\n",
        "print(x)\n",
        "y = torch.arange(4)\n",
        "print(y)\n",
        "torch.einsum('i,j->ij',x,y)\n",
        "\n",
        "# batch matrix multiplication\n",
        "As = torch.arange(30).reshape(3,2,5)\n",
        "print(As)\n",
        "Bs = torch.arange(60).reshape(3,5,4)\n",
        "print(Bs)\n",
        "torch.einsum('bij,bjk->bik',As,Bs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4])\n",
            "tensor([0, 1, 2, 3])\n",
            "tensor([[[ 0,  1,  2,  3,  4],\n",
            "         [ 5,  6,  7,  8,  9]],\n",
            "\n",
            "        [[10, 11, 12, 13, 14],\n",
            "         [15, 16, 17, 18, 19]],\n",
            "\n",
            "        [[20, 21, 22, 23, 24],\n",
            "         [25, 26, 27, 28, 29]]])\n",
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11],\n",
            "         [12, 13, 14, 15],\n",
            "         [16, 17, 18, 19]],\n",
            "\n",
            "        [[20, 21, 22, 23],\n",
            "         [24, 25, 26, 27],\n",
            "         [28, 29, 30, 31],\n",
            "         [32, 33, 34, 35],\n",
            "         [36, 37, 38, 39]],\n",
            "\n",
            "        [[40, 41, 42, 43],\n",
            "         [44, 45, 46, 47],\n",
            "         [48, 49, 50, 51],\n",
            "         [52, 53, 54, 55],\n",
            "         [56, 57, 58, 59]]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 120,  130,  140,  150],\n",
              "         [ 320,  355,  390,  425]],\n",
              "\n",
              "        [[1720, 1780, 1840, 1900],\n",
              "         [2420, 2505, 2590, 2675]],\n",
              "\n",
              "        [[5320, 5430, 5540, 5650],\n",
              "         [6520, 6655, 6790, 6925]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r1QkKGGJobo"
      },
      "source": [
        "##### 💌 Math operations - BLAS and LAPACK Operations\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "위협적으로 보이는 이름에 놀라셨죠? 저도 놀랐어요!\n",
        "\n",
        "- \"BLAS\" - Basic Linear Algebra Subprograms\n",
        "- \"LAPACK\" - Linear Algebra PACKage\n",
        "\n",
        "찾아보았는데 이는 모두 선형대수학(Linear Algebra)와 연관된 이름이에요!\n",
        "함수를 살펴보니까 친숙함이 느껴져요!\n",
        "```\n",
        "\n",
        "- [Math operations - PyTorch 공식 문서](https://pytorch.org/docs/stable/torch.html#math-operations)\n",
        "- [BLAS - netlib](http://www.netlib.org/blas/)\n",
        "- [LAPACK - netlib](http://www.netlib.org/lapack/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZVGkAp5Jobp"
      },
      "source": [
        "- ✅ 함수와 요약 설명을 가볍게 훑어보세요!\n",
        "- ✅ <font color='yellow'><b>[ Optional ]</b></font> 원하는 3개 함수를 골라서 예제 코드를 돌려보세요!\n",
        "    - \n",
        "    - \n",
        "    - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbzHU6WTJobq",
        "outputId": "b120dcf4-4f10-4005-9d29-65e80972961d"
      },
      "source": [
        "import torch\n",
        "\n",
        "M = torch.randn(2, 3)\n",
        "mat1 = torch.randn(2, 3)\n",
        "mat2 = torch.randn(3, 3)\n",
        "torch.addmm(M, mat1, mat2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2715, -0.1265, -1.1123],\n",
              "        [ 1.0081,  0.3057,  0.7136]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRk9bcnqJobr",
        "outputId": "eb6b9287-c99a-4a7b-d719-d17de377f644"
      },
      "source": [
        "a = torch.eye(10)\n",
        "torch.matrix_rank(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: torch.matrix_rank is deprecated in favor of torch.linalg.matrix_rankand will be removed in a future PyTorch release. The parameter 'symmetric' was renamed in torch.linalg.matrix_rank to 'hermitian'. (Triggered internally at  /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:438.)\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5GegxahJobr",
        "outputId": "39756d58-4f11-402f-8a5c-2b40e05c52d0"
      },
      "source": [
        "b = torch.eye(10)\n",
        "b[0, 0] = 0\n",
        "torch.matrix_rank(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_b9Xu6qJobs",
        "outputId": "ecf9704e-73ea-4d45-901a-e0f06b0513e3"
      },
      "source": [
        "a = torch.tensor([[12., -51, 4],\n",
        "                  [6, 167, -68],\n",
        "                  [-4, 24, -41]])\n",
        "q, r = torch.qr(a)\n",
        "q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8571,  0.3943,  0.3314],\n",
              "        [-0.4286, -0.9029, -0.0343],\n",
              "        [ 0.2857, -0.1714,  0.9429]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5mBhSmmJobs",
        "outputId": "ddbd5f95-6503-45d4-ccbb-d2523d9c4f7a"
      },
      "source": [
        "r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -14.0000,  -21.0000,   14.0000],\n",
              "        [   0.0000, -175.0000,   70.0000],\n",
              "        [   0.0000,    0.0000,  -35.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9SbUGKmJobs",
        "outputId": "593ac454-8315-497b-d5b5-f770c0bf1389"
      },
      "source": [
        "torch.mm(q, r).round()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 12., -51.,   4.],\n",
              "        [  6., 167., -68.],\n",
              "        [ -4.,  24., -41.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_2T-Q3EJobt",
        "outputId": "248583c8-600a-4c8d-f67e-b3f05b46d210"
      },
      "source": [
        "torch.allclose(torch.matmul(q, r), a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGZDh_Tw4tpS"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> torch.linalg 문서 읽기\n",
        "``` python\n",
        "🦆\n",
        "torch 문서를 읽으면서 보았던 선형대수학(linear algebra) 관련 함수들이\n",
        "\"torch.linalg\"로 새롭게 정리되면서 기존 \"torch\"에 속해있던\n",
        "선형대수학 기능들이 deprecated 된 것 같아요!\n",
        "\n",
        "디렉토리가 이동했을 뿐 함수명은 이미 친숙하게 느껴지네요! 그렇지 않나요?\n",
        "```\n",
        "\n",
        "- [torch.linalg 문서 - PyTorch 공식 문서](https://pytorch.org/docs/stable/linalg.html#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOg5zeq46J0A"
      },
      "source": [
        "- ✅ torch.linalg 하위 목차를 한번 훑어보세요!\n",
        "    - Matrix Properties\n",
        "    - Decompositions\n",
        "    - Solvers\n",
        "    - Inverses\n",
        "    - Matrix Products\n",
        "    - Tensor Operations\n",
        "    - Experimental Functions\n",
        "- ✅ 각 하위 목차별로 어떤 함수가 속해있는지 훑어보세요!\n",
        "    - ex) Matrix Properties에는 `norm`, `vector_norm` 등이 있다\n",
        "    - ex) Decompositions에는 `cholesky`, `qr`, `svd` 등이 있다\n",
        "    - ex) Inverses에는 `inv`, `pinv` 2개의 함수가 있다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXUIf4hw8R_A"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> torch.nn 문서 읽기\n",
        "``` python\n",
        "🦆\n",
        "설명에 그래프를 만들기 위한 \"basic building block\"이라고 쓰여있어요!\n",
        "\n",
        "지금까지 저희가 확인한 다양한 함수들을 잘 활용하면\n",
        "여기서 말하는 \"basic building block\"을 만들 수 있겠지만 시간이 걸리니까\n",
        "PyTorch에서 미리 만들어두고 이를 \"torch.nn\"으로 묶어놓은 것 같아요! \n",
        "\n",
        "여기서 제공해주는 블럭들을 잘 이용하면 그래프라는 딥러닝 모델을 만들 수 있을 것 같아요!\n",
        "\n",
        "매우 중요해보이지만 내용이 정말 많네요!\n",
        "이 많은 내용을 다 볼 수는 없으니까 간단하게 훑어보기만 해요!\n",
        "지금은 생소하고 어려워보여도 차츰 익숙해질게 틀림없어요!\n",
        "```\n",
        "\n",
        "- [torch.nn 문서 - PyTorch 공식 문서](https://pytorch.org/docs/stable/nn.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQjpdQWB8R_S"
      },
      "source": [
        "- ✅ torch.nn의 하위 목차를 한번 훑어보세요!\n",
        "- ✅ 각 하위 목차별로 어떤 레이어(Layer)혹은 함수(Function)가 속해있는지 훑어보세요!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R68OFoe8BzbF"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> torch.nn `Linear Layers`\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "가볍게 읽기만 하려고 했는데 그래도 한 두개는 직접 예제를 따라해보고 싶어요!\n",
        "딥러닝을 공부할 때 자주 나오던 y = WX + b 라는 공식이 기억나시나요?\n",
        "이 linear transformation을 구현해놓은 \"nn.Linear\"가 속해잇는\n",
        "\"Linear Layers\" 항목을 잠깐만 같이 살펴봐요!\n",
        "```\n",
        "\n",
        "- [torch.nn Linear Layers - PyTorch 공식 문서](https://pytorch.org/docs/stable/nn.html#linear-layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZmymVenGzXd"
      },
      "source": [
        "##### 💡 nn.Linear\n",
        "\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "앞으로 왠지 이 \"nn.Linear\"는 정말 자주 마주칠 것만 같은 예감이 들어요!\n",
        "```\n",
        "\n",
        "- [torch.nn.Linear - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "\n",
        "🎁 **힌트** 🎁\n",
        "- PyTorch에는 tensor 크기(or 모양)를 반환하는 함수가 있어요! 영어로 크기가 무엇일까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjHTIZdrGzXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b66e3e-eaf4-4a9c-f82c-f4a36d9202e8"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "X = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "X.size()\n",
        "# TODO : tensor X의 크기는 (2, 2)입니다\n",
        "#        nn.Linear를 사용하여서 (2, 5)로 크기를 바꾸고 이 크기를 출력하세요!\n",
        "m = nn.Linear(2,5)\n",
        "output = m(X)\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU_nhb1vKTi-"
      },
      "source": [
        "##### 💡 nn.Identity\n",
        "> 믿기지 않겠지만 이 layer도 유용하게 사용됩니다. 다만 딥러닝을 막 배우는 단계에서 이 layer를 사용할 일은 거의 없기 때문에 사용처를 아실 필요는 없습니다. 다만 궁금해하실 분들을 위해 링크를 남겨놓습니다.\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "??? 이 \"nn.Identity\"는 도대체.. 뭐죠???\n",
        "입력과 출력이 동일하게 나오는데 도대체 왜 만들어놓은 걸까요?\n",
        "그래도.. 한번 사용해봐요!\n",
        "```\n",
        "\n",
        "- [torch.nn.Identity - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Identity.html#torch.nn.Identity)\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [What is the use of nn.Identity? - PyTorch Forum](https://discuss.pytorch.org/t/what-is-the-use-of-nn-identity/51781)\n",
        "- [What is the idea behind using nn.Identity for residual learning? - Stack Overflow](https://stackoverflow.com/questions/64229717/what-is-the-idea-behind-using-nn-identity-for-residual-learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om5zRqyvKTjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9824c256-0b7d-4a9b-b5bc-dc59b19e43dd"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "X = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "# TODO : nn.Identity를 생성해 X를 입력시킨 후 나온 출력값이 X와 동일한지 확인해보세요!\n",
        "m = nn.Identity()\n",
        "output = m(X)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRWBmAYkOYgy"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Linear vs LazyLinear\n",
        "``` python\n",
        "🦆\n",
        "그냥 지나치려고 했는데 신경이 쓰여서요!\n",
        "Linear와 LazyLinear 차이가 뭐죠?!\n",
        "```\n",
        "\n",
        "- [torch.nn.Linear - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "- [torch.nn.LazyLinear - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIrfhKtCOYg6"
      },
      "source": [
        "```python\n",
        "😂\n",
        "# TODO : 맞고 틀리고가 없는 문제입니다. 문서를 읽고 답을 자유로이 적어주세요\n",
        "LazyLinear function은 weight와 bias가 torch.nn.UninitializedParameter class 의 것이다. LazyLinear은 한번 forward를 거친뒤 Linear function으로 변한다. 정확하지는 않지만 더 빠른 학습을 위해, weight와 bias를 미리 랜덤하게 설정하는 대신 data에 입출력에 맞게 근사하게 설정하기위한 용도인것 같다. \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNriTnyfdI0-"
      },
      "source": [
        "### 🎉🎉🎉 Documentation 완료! 🎉🎉🎉\n",
        "\n",
        "```python\n",
        "🦆\n",
        "우리가 함께 여기까지 오다니 너무 기뻐요!\n",
        "같이 좀 더 힘내서 나아가보자구요!\n",
        "```\n",
        "\n",
        "Documentation 장이 생각보다 많이 힘들었죠?<br>\n",
        "적지 않은 분량이었음에도 무사히 마무리 지으신 것을 정말 축하드립니다! 🎉<br> 결코 쉽지 않은 일입니다.\n",
        "\n",
        "여러분은 이제 Documentation을 활용할 준비가 되었습니다.<br>\n",
        "이제 우리의 본래 목적인 모델 제작과 관련된 내용을 다룰 차례입니다.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYHm2O2t6Usd"
      },
      "source": [
        "## ⭐ Custom 모델 제작을 위한 nn.Module 클래스\n",
        "\n",
        "```\n",
        "💡 PyTorch 라이브러리가 제공해주는 다양한 기능들과\n",
        "   nn.Module를 활용하여 모델 제작 및 분석을 진행해볼 것입니다!\n",
        "```\n",
        "\n",
        "Documentation에 나온 다양한 기능들을 찾고 활용하는 방법을 배웠으니 이제 PyTorch가 제공해주는 기능들을 조합하여서 멋진 모델을 만들 차례입니다. 모델을 만들기 위해서 기능들을 단순히 나열해놓기만 한다면 지저분하겠죠? 그래서 PyTorch는 이런 일련의 기능들을 한 곳에 모아 하나의 모델로 추상화할 수 있게끔 클래스를 제공합니다.\n",
        "\n",
        "```\n",
        "💡 nn.Module\n",
        "```\n",
        "- [torch.nn.Module - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
        "\n",
        "`nn.Module` 클래스는 여러 기능들을 한 곳에 모아놓는 상자 역할을 합니다.<br>\n",
        "`nn.Module`이라는 상자는 다른 `nn.Module` 상자를 포함할 수도 있습니다!<br>\n",
        "어떻게 사용햐느냐에 따라 `nn.Module` 상자는 다른 의미를 가집니다.\n",
        "\n",
        "- `nn.Module`이라는 상자에 `기능`들을 가득 모아놓은 경우 `basic building block`\n",
        "- `nn.Module`이라는 상자에 `basic building block`인 `nn.Module`들을 가득 모아놓은 경우 `딥러닝 모델`\n",
        "- `nn.Module`이라는 상자에 `딥러닝 모델`인 `nn.Module`들을 가득 모아놓은 경우 `더욱 큰 딥러닝 모델`\n",
        "\n",
        "`nn.Module`은 빈 상자일 뿐 이를 어떻게 사용할지는 온전히 설계자의 몫입니다!<br>\n",
        "`기능`과 `basic building block`과 `딥러닝 모델`을 혼재해서 마구잡이로 담을 수도 있고<br>\n",
        "`기능`은 `기능`끼리 `block`은 `block`끼리 계층적으로 담을 수도 있습니다!\n",
        "\n",
        "우리는 여기서 `nn.Module`를 이용해 모델을 제작해보고 제작한 모델이 어떻게 구성되어있는지 분석해볼 것입니다!<br>\n",
        "추가적으로 custom 모델 제작에 유용할 수 있는 `nn.Module`의 기능들도 살펴볼 것입니다!\n",
        "\n",
        "🦆 부덕이가 솟구치는 배움의 욕구를 주체하기 어려워하고 있어요!\n",
        "\n",
        "- ☄️ nn.Module 모델 제작\n",
        "- ☄️ nn.Module 분석하기\n",
        "- ☄️ nn.Module 알쓸신잡\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXCog9jT_MJt"
      },
      "source": [
        "### ☄️ nn.Module 모델 제작\n",
        "> nn.Module이 무엇인지 이해한 후 이를 이용하여서 모델을 제작하는 다양한 방법에 대해서 실습해보고 custom 모델을 제작함에 있어 중요한 개념들에 대해 이해하는 시간을 가질 것입니다\n",
        "\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> torch.nn.Module 문서 읽기\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 1 + 2\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> Container\n",
        "- ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Python List vs PyTorch ModuleList\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 조건문\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> Module들의 흐름 느껴보기\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> Parameter\n",
        "- ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Tensor vs Parameter\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> Buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgJ38Rsiy2Tc"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> torch.nn.Module 문서 읽기\n",
        "``` python\n",
        "🦆\n",
        "\"nn.Module\"은 앞으로 저희와 계속 함께하게될 운명이 틀림없어요!\n",
        "잠깐이지만 \"nn.Module\"에 대해서 알아보는 시간을 가져봐요! \n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoDjvw97y2Tr"
      },
      "source": [
        "- ✅ Documentation에서 nn.Module을 검색해서 찾으세요!\n",
        "- ✅ nn.Module 문서의 설명을 읽어보세요!\n",
        "    - ![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/nn.Module.png?raw=true)\n",
        "- ✅ nn.Module 내부의 method들의 이름과 설명을 가볍게 훑어보세요!\n",
        "```\n",
        "🔔 외우거나 이해하는 시간이 아닙니다!\n",
        "     설명이 눈에 잘 안들어오고 어색하다면 매우 정상입니다\n",
        "     억지로 읽으실 필요 없습니다! 눈에 안들어오면 그냥 지나가세요!\n",
        "     앞으로 공부를 해가시면서 점차 익숙해지실거니 지금은\n",
        "     가벼운 마음으로 훑어보면서 이런게 있구나~ 정도로만 보세요!\n",
        "```\n",
        "    - add_module\n",
        "    - apply\n",
        "    - bfloat16\n",
        "    - buffers\n",
        "    - children\n",
        "    - cpu\n",
        "    - cuda\n",
        "    - double\n",
        "    - dump_patches\n",
        "    - eval\n",
        "    - extra_repr\n",
        "    - float\n",
        "    - forward\n",
        "    - get_buffer\n",
        "    - get_parameter\n",
        "    - get_submodule\n",
        "    - half\n",
        "    - load_state_dict\n",
        "    - modules\n",
        "    - named_buffers\n",
        "    - named_children\n",
        "    - named_modules\n",
        "    - named_parameters\n",
        "    - parameters\n",
        "    - register_backward_hook\n",
        "    - register_buffer\n",
        "    - register_forward_hook\n",
        "    - register_forward_pre_hook\n",
        "    - register_full_backward_hook\n",
        "    - register_parameter\n",
        "    - requires_grad_\n",
        "    - share_memory\n",
        "    - state_dict\n",
        "    - to\n",
        "    - to_empty\n",
        "    - train\n",
        "    - type\n",
        "    - xpu\n",
        "    - zero_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OhWyjstxvUt"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 1 + 2\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "Documentation을 읽어나가다가 가볍게 \"torch.add\"를 이용해서 \n",
        "사칙연산을 계산한 것을 기억하세요?\n",
        "\n",
        "이번에는 \"nn.Module\"를 이용해서 더하기 연산을 하는 모델을 만들어보아요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [torch.add - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.add.html?highlight=add#torch.add)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF5CCXcgxjWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c7da7c-c97a-4739-ecea-179e1e34f9ef"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : Add 모델을 완성하세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self):\n",
        "        # TODO : init 과정에서 반드시 들어가야 하는 super 관련 코드가 있습니다\n",
        "        super().__init__()\n",
        "        \n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \n",
        "        return torch.add(x1,x2)\n",
        "        # TODO : torch.add 함수를 사용해서 더하기 연산을 해주세요!\n",
        "        \n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x1 = torch.tensor([1])\n",
        "x2 = torch.tensor([2])\n",
        "\n",
        "add = Add()\n",
        "output = add(x1, x2)\n",
        "\n",
        "if output == 3:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfhwgR6kYy6i"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "super를 통해서 init을 하는 것은 왜 그런걸까요?\n",
        "\n",
        "아래 링크의 글을 읽으니까 의문이 좀 풀리는 것 같아요! \n",
        "```\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [Why is the super constructor necessary in PyTorch custom modules? - Stack Overflow](https://stackoverflow.com/questions/63058355/why-is-the-super-constructor-necessary-in-pytorch-custom-modules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpJ51jOZL1pm"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> Container\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "저희가 원하는 모듈(Module)을 성공적으로 만들었어요!\n",
        "시작이 절반! 인지는 모르겠지만 모델을 만든 것은 틀림없죠!\n",
        "\n",
        "이렇게 만든 모듈(Module)들을 묶어서 사용하고 싶은데 어떻게 하는걸까요?\n",
        "파이썬의 리스트에 모듈들을 보관하면 되는걸까요?\n",
        "\n",
        "앗! 찾았어요! \n",
        "Documentation을 열심히 찾아보니까 관련된 함수들이\n",
        "\"torch.nn\"의 Container 항목에 포함되어있네요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [Containers  - PyTorch 공식 문서](https://pytorch.org/docs/stable/nn.html#containers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a_OSRXMgc8j"
      },
      "source": [
        "##### 💡 torch.nn.Sequential\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "모듈(Module)들을 하나로 묶어 순차적으로 실행시키고 싶을때\n",
        "torch.nn.Sequential를 사용한다고 하네요! 같이 만들어봐요!\n",
        "```\n",
        "\n",
        "- [torch.nn.Sequential - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAzm120dL1p4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f04daae-f3a6-4275-8bad-58e79339f008"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : 다음의 모듈(Module)을 읽고 이해해보세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "    \n",
        "\n",
        "# TODO : 위에 모듈(Module)과 nn.Sequential를 이용해서\n",
        "#        입력값 x가 주어지면 다음의 연산을 처리하는 모델을 만들어보세요!\n",
        "#        y = x + 3 + 2 + 5\n",
        "calculator = nn.Sequential(\n",
        "    Add(3),\n",
        "    Add(2),\n",
        "    Add(5)\n",
        ")\n",
        "488\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([1])\n",
        "\n",
        "output = calculator(x)\n",
        "\n",
        "if output == 11:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Zs6pnh1925"
      },
      "source": [
        "##### 💡 torch.nn.ModuleList\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "torch.nn.Sequential은 묶어놓은 모듈들을 차례대로 수행하기 때문에\n",
        "실행 순서가 정해져있는 기능들을 하나로 묶어두기 좋아보여요!\n",
        "\n",
        "하지만 파이썬의 list처럼 모아두기만 하고 그때그때 원하는 것만\n",
        "인덱싱(indexing)을 통해 쓰고 싶으면 torch.nn.ModuleList을 쓰면 되지 않을까요?\n",
        "```\n",
        "\n",
        "- [torch.nn.ModuleList - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D3cK6pS193J",
        "outputId": "28f53e86-707c-4ed0-f403-e16fa4841d33"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : 다음의 모듈(Module)을 읽고 이해해보세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "\n",
        "# TODO : Calculator 모델을 완성하세요!\n",
        "class Calculator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.add_list = nn.ModuleList([Add(2), Add(3), Add(5)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO : self.add_list에 담긴 모듈들을 이용하여서\n",
        "        #        y = ((x + 3) + 2) + 5 의 연산을 구현하세요!\n",
        "        add_list = self.add_list\n",
        "        x = add_list[2](add_list[0](add_list[1](x)))\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([1])\n",
        "\n",
        "calculator = Calculator()\n",
        "output = calculator(x)\n",
        "print(output)\n",
        "if output == 11:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([11])\n",
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl8bTU-FWLWP"
      },
      "source": [
        " **이 forward() 함수는 model 객체를 데이터와 함께 호출하면 자동으로 실행이됩니다. 예를 들어 model이란 이름의 객체를 생성 후, model(입력 데이터)와 같은 형식으로 객체를 호출하면 자동으로 forward 연산이 수행됩니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLOhs8I56dYA"
      },
      "source": [
        "##### 💡 torch.nn.ModuleDict\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "torch.nn.ModuleLists 정말 편리하네요!!\n",
        "하지만 만약 리스트에 담긴 모듈의 크기가 정말 커진다면\n",
        "나중에는 인덱싱(indexing)으로 원하는 모듈을 찾기가 정말 힘들어질 것 같아요!\n",
        "\n",
        "파이썬의 dict처럼 특정 모듈을 key값을 이용해 보관해놓는다면\n",
        "나중에 원하는 모듈을 가져올때 훨씬 수월하지 않을까요?\n",
        "마침 PyTorch에 torch.nn.ModuleDict이 있네요! 같이 써봐요!\n",
        "```\n",
        "\n",
        "- [torch.nn.ModuleDict - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suYi2pjj6dYK",
        "outputId": "128183e2-864c-4ceb-d385-892a6a0e8f7e"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : 다음의 모듈(Module)을 읽고 이해해보세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "\n",
        "# TODO : Calculator 모델을 완성하세요!\n",
        "class Calculator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.add_dict = nn.ModuleDict({'add2': Add(2),\n",
        "                                       'add3': Add(3),\n",
        "                                       'add5': Add(5)})\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO : self.add_dict에 담긴 모듈들을 이용하여서\n",
        "        #        y = ((x + 3) + 2) + 5 의 연산을 구현하세요!\n",
        "        add_dict = self.add_dict\n",
        "        x = add_dict[\"add5\"](add_dict[\"add2\"](add_dict[\"add3\"](x)))\n",
        "       \n",
        "        return x\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([1])\n",
        "\n",
        "calculator = Calculator()\n",
        "output = calculator(x)\n",
        "\n",
        "if output == 11:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgV9Ntk38ieo"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Python List vs PyTorch ModuleList\n",
        "``` python\n",
        "🦆\n",
        "그런데 가만히 생각해보니까 파이썬에도 List가 있는데 왜 굳이\n",
        "PyTorch에서는 ModuleList를 별도로 만들어두었을까요?\n",
        "\n",
        "그 이유가 궁금해요!\n",
        "```\n",
        "\n",
        "- [torch.nn.ModuleList - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList)\n",
        "\n",
        "🎁 **힌트** 🎁\n",
        "- 아래에 작성된 코드를 실행시키시면 힌트를 얻을 수 있습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0X4Bi7E9bjz"
      },
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "\n",
        "class PythonList(nn.Module):\n",
        "    \"\"\"Python List\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Python List\n",
        "        self.add_list = [Add(2), Add(3), Add(5)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.add_list[1](x)\n",
        "        x = self.add_list[0](x)\n",
        "        x = self.add_list[2](x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class PyTorchList(nn.Module):\n",
        "    \"\"\"PyTorch List\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Pytorch ModuleList\n",
        "        self.add_list = nn.ModuleList([Add(2), Add(3), Add(5)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.add_list[1](x)\n",
        "        x = self.add_list[0](x)\n",
        "        x = self.add_list[2](x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcZcR67F_MbE",
        "outputId": "97fdf526-9a20-455d-e36a-22980e156632"
      },
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([1])\n",
        "\n",
        "python_list = PythonList()\n",
        "pytorch_list = PyTorchList()\n",
        "\n",
        "# 기능 동작은 동일합니다!\n",
        "print(python_list(x), pytorch_list(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([11]) tensor([11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8qgY2EK_lMb",
        "outputId": "f1828a79-37df-4e63-e0d7-a63b26c12292"
      },
      "source": [
        "# Python List로 모아놓은 모듈들이 감쪽같이 사라졌습니다!\n",
        "python_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonList()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-eFwdj7ABMH",
        "outputId": "531e9fde-17c8-4695-bd0f-6dc774a8f6ba"
      },
      "source": [
        "# 하지만 PyTorch의 ModuleList로 모아놓은 모듈들은 짠! 하고 나타나네요!\n",
        "pytorch_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PyTorchList(\n",
              "  (add_list): ModuleList(\n",
              "    (0): Add()\n",
              "    (1): Add()\n",
              "    (2): Add()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EZ7R-Js8ie5"
      },
      "source": [
        "```python\n",
        "😌\n",
        "# TODO : 맞고 틀리고가 없는 문제입니다. 문서를 읽고 답을 자유로이 적어주세요\n",
        " but modules it contains are properly registered, and will be visible by all Module methods.\n",
        " ModuleList로 구현하게 되면\n",
        " 모듈을 호출했을 때 ModuleList가 같이 출력된다. \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ARH3kXTPKEq"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 조건문\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "정말 많은 것을 한번에 배우니까 생각보다 힘드네요 꽉꽉!💦\n",
        "죄송해요! 보통 인간어를 사용하는데 힘들면 가끔 오리어가 나오네요!\n",
        "\n",
        "모델을 만들때 PyTorch는 동적 계산 그래프를 사용하기 때문에\n",
        "if / else 와 같은 조건문을 쉽게 사용할 수 있는 장점이 있다고 들었어요!\n",
        "\n",
        "정말 멋진 것 같아요! 같이 어서 써봐요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [Can someone explain the use of a dynamic graph? - Reddit](https://www.reddit.com/r/pytorch/comments/8kpsjy/can_someone_explain_the_use_of_a_dynamic_graph/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDBSmecjPKE7",
        "outputId": "1dcd919c-4c25-42e9-8362-eb38e364f420"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : 다음의 모듈(Module)을 읽고 이해해보세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "class Sub(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x - self.value\n",
        "\n",
        "\n",
        "# TODO : Calculator 모델을 완성하세요!\n",
        "class Calculator(nn.Module):\n",
        "    def __init__(self, cal_type):\n",
        "        super().__init__()\n",
        "        self.cal_type = cal_type\n",
        "        self.add = Add(3)\n",
        "        self.sub = Sub(3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO : cal_type에 \"add\"가 입력되면 더하기 모델 y = x + 3\n",
        "        #                   \"sub\"가 입력되면 빼기 모델 y = x - 3\n",
        "        #                   \"add\", \"sub\"가 아닌 다른 문자열이 입력되면 ValueError을 일으키세요!\n",
        "        #        if/elif/else 조건문을 사용하세요! \n",
        "        if self.cal_type == \"add\":\n",
        "            x = self.add(x)\n",
        "        elif self.cal_type == \"sub\":\n",
        "            x = self.sub(x)\n",
        "        else:\n",
        "            raise ValueError\n",
        "        \n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([5])\n",
        "\n",
        "try:\n",
        "    calculator = Calculator(\"none\")\n",
        "    output = calculator(x)\n",
        "\n",
        "    print(\"🦆 잘못된 문자열 입력에는 에러를 발생시키세요!!\")\n",
        "except ValueError:\n",
        "    calculator = Calculator(\"add\")\n",
        "    add_output = calculator(x)\n",
        "\n",
        "    calculator = Calculator(\"sub\")\n",
        "    sub_output = calculator(x)\n",
        "    \n",
        "    if add_output == 8 and sub_output == 2:\n",
        "        print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "    else:\n",
        "        print(\"🦆 다시 도전해봐요!\")\n",
        "except:\n",
        "    print(\"🦆 ValueError를 발생시키세요!!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfdy2siYp4ua"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> Module들의 흐름 느껴보기\n",
        "> 🦆 부덕이가 코드를 작성해주었어요\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "Module은 Module을 포함할 수 있다는 사실이 멋져요!\n",
        "\n",
        "- 최소의 기능 단위인 function\n",
        "- function들로 이루어진 layer\n",
        "- layer로 이루어진 model\n",
        "\n",
        "작은 부분부터 블럭을 하나씩 쌓고 또 쌓다보면 어느 순간\n",
        "우리는 거대한 딥러닝 모델이라는 멋진 탑을 볼 수 있게 되니까요!\n",
        "\n",
        "Module과 Module의 연결이 만들어내는 흐름을 느껴보세요!\n",
        "각 Module의 초기화는 어떤 순서로 되는지,\n",
        "언제 시작하고 끝나는지 천천히 생각해보세요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGPSjEytp4um",
        "outputId": "6aa27a32-4b03-47a4-f4c1-565381c80c46"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# Function\n",
        "class Function_A(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        print(f\"        Function A Initialized\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"        Function A started\")\n",
        "        print(f\"        Function A done\")\n",
        "\n",
        "class Function_B(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        print(f\"        Function B Initialized\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"        Function B started\")\n",
        "        print(f\"        Function B done\")\n",
        "\n",
        "class Function_C(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        print(f\"        Function C Initialized\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"        Function C started\")\n",
        "        print(f\"        Function C done\")\n",
        "\n",
        "class Function_D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        print(f\"        Function D Initialized\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"        Function D started\")\n",
        "        print(f\"        Function D done\")\n",
        "\n",
        "\n",
        "# Layer\n",
        "class Layer_AB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.a = Function_A()\n",
        "        self.b = Function_B()\n",
        "\n",
        "        print(f\"    Layer AB Initialized\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"    Layer AB started\")\n",
        "        self.a(x)\n",
        "        self.b(x)\n",
        "        print(f\"    Layer AB done\")\n",
        "\n",
        "class Layer_CD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = Function_C()\n",
        "        self.d = Function_D()\n",
        "\n",
        "        print(f\"    Layer CD Initialized\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"    Layer CD started\")\n",
        "        self.c(x)\n",
        "        self.d(x)\n",
        "        print(f\"    Layer CD done\")\n",
        "\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ab = Layer_AB()\n",
        "        self.cd = Layer_CD()\n",
        "\n",
        "        print(f\"Model ABCD Initialized\\n\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"Model ABCD started\")\n",
        "        self.ab(x)\n",
        "        self.cd(x)\n",
        "        print(f\"Model ABCD done\\n\")\n",
        "\n",
        "\n",
        "x = torch.tensor([7])\n",
        "\n",
        "model = Model()\n",
        "model(x)\n",
        "\n",
        "print(\"🎉🎉🎉 모든 딥러닝 모델은 이처럼 Module들이 쌓이고 쌓여서 만들어집니다! 🎉🎉🎉\")\n",
        "print(\"🎉🎉🎉 흐름을 느껴보시고 이 흐름이 이해가 되신 분은 다음으로 가시면 됩니다! 🎉🎉\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Function A Initialized\n",
            "        Function B Initialized\n",
            "    Layer AB Initialized\n",
            "        Function C Initialized\n",
            "        Function D Initialized\n",
            "    Layer CD Initialized\n",
            "Model ABCD Initialized\n",
            "\n",
            "Model ABCD started\n",
            "    Layer AB started\n",
            "        Function A started\n",
            "        Function A done\n",
            "        Function B started\n",
            "        Function B done\n",
            "    Layer AB done\n",
            "    Layer CD started\n",
            "        Function C started\n",
            "        Function C done\n",
            "        Function D started\n",
            "        Function D done\n",
            "    Layer CD done\n",
            "Model ABCD done\n",
            "\n",
            "🎉🎉🎉 모든 딥러닝 모델은 이처럼 Module들이 쌓이고 쌓여서 만들어집니다! 🎉🎉🎉\n",
            "🎉🎉🎉 흐름을 느껴보시고 이 흐름이 이해가 되신 분은 다음으로 가시면 됩니다! 🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYE0lhVK2Vow"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> Parameter\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "linear transformation인 Y = XW + b 에 대해서 생각하고 있었어요!\n",
        "X는 저희가 torch.Tensor로 만들어서 제공하는데 W, b는 어디서 만들죠?\n",
        "\n",
        "언뜻 친구한테 들었는데 nn.Module안에 미리 만들어진 tensor들을\n",
        "보관할 수 있다고 들은 것 같아요! 뭐랬지, 아마 Parameter라고 한 것 같아요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [torch.nn.parameter.Parameter - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html?highlight=parameter)\n",
        "\n",
        "🎁 **힌트** 🎁\n",
        "- [PyTorch linear.py L81 - L85 - PyTorch 공식 Github](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L81-L85)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQoHSQcA2Vo3",
        "outputId": "2a1a9cc9-f118-41b9-d980-b16ecf3e7641"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# TODO : Linear 모델을 완성하세요!\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO : W, b parameter를 생성하세요! 모두 1로 초기화해주세요!\n",
        "        self.W = Parameter(torch.ones(out_features, in_features))\n",
        "        self.b = Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.addmm(self.b, x, self.W.T)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "linear = Linear(2, 3)\n",
        "output = linear(x)\n",
        "\n",
        "if torch.all(output == torch.Tensor([[4, 4, 4],\n",
        "                                     [8, 8, 8]])):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8u8jNL8H_dN"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Tensor vs Parameter\n",
        "``` python\n",
        "🦆\n",
        "생각해보면 W, b도 tensor를 이용하면 되는 것 아닌가요?\n",
        "왜 굳이 Parameter라는 별개의 클래스를 사용하는 거죠?\n",
        "```\n",
        "🎁 **힌트** 🎁\n",
        "- 아래에 작성된 코드를 실행시키시면 힌트를 얻을 수 있습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULF5STEQH_dO",
        "outputId": "a0c8ccf1-8615-47ab-f582-0390f8fb8683"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class Linear_Parameter(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # torch.nn.parameter.Parameter\n",
        "        self.W = Parameter(torch.ones((out_features, in_features)))\n",
        "        self.b = Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.addmm(self.b, x, self.W.T)\n",
        "\n",
        "        return output\n",
        "\n",
        "class Linear_Tensor(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # torch.Tensor\n",
        "        self.W = torch.ones((out_features, in_features))\n",
        "        self.b = torch.ones(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.addmm(self.b, x, self.W.T)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "x = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "linear_parameter = Linear_Parameter(2, 3)\n",
        "linear_tensor = Linear_Tensor(2, 3)\n",
        "\n",
        "output_parameter = linear_parameter(x)\n",
        "output_tensor = linear_tensor(x)\n",
        "\n",
        "# 값은 동일하게 계산되는 것을 볼 수 있습니다!\n",
        "# 하지만 출력을 자세히 보시면, Parameter를 이용해서 W, b를 만들 경우에만\n",
        "# output tensor에 gradient를 계산하는 함수인 grad_fn가 생성됩니다\n",
        "print(output_parameter)\n",
        "print(output_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 4., 4.],\n",
            "        [8., 8., 8.]], grad_fn=<AddmmBackward>)\n",
            "tensor([[4., 4., 4.],\n",
            "        [8., 8., 8.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCmHCEEWH_dP",
        "outputId": "e2de0e48-8fd6-4bf7-98ed-d619a36b3add"
      },
      "source": [
        "# Parameter로 만든 W, b는 저장할 tensor로 지정되어있습니다\n",
        "linear_parameter.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('W', tensor([[1., 1.],\n",
              "                      [1., 1.],\n",
              "                      [1., 1.]])), ('b', tensor([1., 1., 1.]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5obZKlPzH_dP",
        "outputId": "09dce683-170e-426b-bee4-846a3b92776c"
      },
      "source": [
        "# torch.Tensor로 만든 W, b는 저장되지 않습니다\n",
        "linear_tensor.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH49yfoYH_dR"
      },
      "source": [
        "```python\n",
        "😏\n",
        "# TODO : 맞고 틀리고가 없는 문제입니다. 문서를 읽고 답을 자유로이 적어주세요\n",
        "Parameter class 를 이용하면 auto_grad를 사용할 수 있다. \n",
        "또 상속받은 부모함수에 연동되어 더 효과적으로 사용할 수 있다. \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WfRph4AYrb"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> Buffer\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "친구가 말하길 Custom 모델을 만들때 대부분 torch.nn에 구현된 layer들을\n",
        "가져가다 사용하기 때문에 Parameter를 직접 다뤄볼 일은 매우 드물 것이라고 하네요!\n",
        "\n",
        "저희가 직접 새로운 layer를 작성할게 아니라면\n",
        "Parameter를 사용할 일이 거의 없다고 해요!\n",
        "\n",
        "하지만 Parameter를 사용할줄 아는 것은 중요하다면서 칭찬해줬어요!\n",
        "추가적으로 buffer라는 것도 있다면서 가르켜주었죠!\n",
        "\n",
        "일반적인 Tensor는 Parameter와 다르게 gradient를 계산하지 않아\n",
        "값도 업데이트 되지 않고, 모델을 저장할 때 무시되잖아요?\n",
        "\n",
        "하지만 Parameter로 지정하지 않아서 값이 업데이트 되지 않는다 해도\n",
        "저장하고싶은 tensor가 있을 수도 있잖아요?\n",
        "\n",
        "그럴때는 buffer에 tensor를 등록해주면 되요!\n",
        "모델을 저장할때 Parameter뿐만 아니라 buffer로 등록된 tensor들도 같이 저장되요!\n",
        "\n",
        "정리하면 다음과 같아요!\n",
        "\n",
        "- \"Tensor\"\n",
        "    - ❌ gradient 계산\n",
        "    - ❌ 값 업데이트\n",
        "    - ❌ 모델 저장시 값 저장\n",
        "- \"Parameter\"\n",
        "    - ✅ gradient 계산\n",
        "    - ✅ 값 업데이트\n",
        "    - ✅ 모델 저장시 값 저장\n",
        "- \"Buffer\"\n",
        "    - ❌ gradient 계산\n",
        "    - ❌ 값 업데이트\n",
        "    - ✅ 모델 저장시 값 저장\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [register_buffer - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_buffer#torch.nn.Module.register_buffer)\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [What is the difference between `register_buffer` and `register_parameter` of `nn.Module` - PyTorch Forum](https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU_FYpsgAYri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5565caa8-8c23-4c3d-8c00-ab4c9e293d24"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# TODO : Model 모델을 완성하세요!\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.parameter = Parameter(torch.Tensor([7]))\n",
        "        self.tensor = torch.Tensor([7])\n",
        "        self.register_buffer('buffer', self.tensor)\n",
        "\n",
        "        # TODO : torch.Tensor([7])를 buffer이라는 이름으로 buffer에 등록해보세요!\n",
        "\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "model = Model()\n",
        "\n",
        "try:\n",
        "    buffer = model.get_buffer('buffer')\n",
        "    if buffer == 7:\n",
        "        print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\\n\")\n",
        "        print(\"🎉 이제 buffer에 등록된 tensor는 모델이 저장될 때 같이 저장될거예요! 🎉\")\n",
        "        print(model.state_dict())\n",
        "    else:\n",
        "        print(\"🦆 다시 도전해봐요!\")\n",
        "except:\n",
        "    print(\"🦆 다시 도전해봐요!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n",
            "\n",
            "🎉 이제 buffer에 등록된 tensor는 모델이 저장될 때 같이 저장될거예요! 🎉\n",
            "OrderedDict([('parameter', tensor([7.])), ('buffer', tensor([7.]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY5I6ZQbScV2"
      },
      "source": [
        "``` python\n",
        "😩\n",
        "아 너무 어려워요... 그래서 이걸 어디다 쓰는데요..?\n",
        "```\n",
        "``` python\n",
        "🦆\n",
        "그 말 할 줄 알았어요 준비해두었죠!\n",
        "한가지 좋은 예시로 BatchNorm에서 사용되요!\n",
        "아래 링크를 첨부해놓았으니 더 알고 싶으면 읽어보세요!\n",
        "\n",
        "이 buffer도 Parameter와 마찬가지로 사용할 일은 드물 것 같아요!\n",
        "하지만 알아두면 언젠가 요긴하게 쓸 날이 오겠죠?\n",
        "```\n",
        "\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [torch.nn.BatchNorm1d - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html?highlight=buffer)\n",
        "- [PyTorch batchnorm.py L51 - L52 - PyTorch 공식 Github](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py#L51-L52)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr6BwccdiH5s"
      },
      "source": [
        "### ☄️ nn.Module 분석하기\n",
        "> 모델을 제작함에 있어 중요한 기본 개념들을 이해하였습니다. 이제 저희는 원하는 모델을 제작할 수 있는 힘을 얻게 되었습니다. 하지만 custom 모델을 만들고 난 후 이 모델 내부가 어떻게 구성되었는지 어떻게 알 수 있을까요? 자신이 만든 custom 모델이라면 그럭저럭 기억이 나기 때문에 괜찮을 수 있습니다. 그렇다면 다른 사람의 모델을 참조하기 위해서 가져왔는데 이런 경우 어떻게 그 모델 내부를 분석할 수 있을까요? 우리는 그 방법에 대해서 가볍게 알아보는 시간을 가질 것입니다. \n",
        "\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 부덕이 모델 분석해보기\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 부덕이 모델 수정하기 - module 참조 제거\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 부덕이 모델 수정하기 - module 출력 내용 변경하기\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 부덕이 모델 수정하기 - Docstring 작성\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> BatchNorm1d 분석해보기\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0rBcgFOdVTL"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 부덕이 모델 분석해보기\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "PyTorch에 대해서 아무것도 몰랐었는데\n",
        "직접 모델을 만들어낼 줄 알게 되다니! 정말 꿈만 같아요!\n",
        "\n",
        "Module들의 흐름을 느껴보기 위해 제가 작성했던 코드를 보고있으니\n",
        "마음 속 깊은 곳에서 뿌듯함이 솟구쳐 올라와 날개 춤을 멈출 수가 없네요!\n",
        "\n",
        "그런데 모델을 만들고 난 후 어떤 module과 parameter를 썼는지\n",
        "어떻게 알 수 있는 거죠?\n",
        "\n",
        "느낌상 \"nn.Module\" Documentation에 그 방법이 나와있을 것 같아요!\n",
        "\n",
        "제가 전에 작성해놓았던 코드를 가져왔어요! 그때 보셨을 때와 조금 다르죠?\n",
        "배운 것을 적용해보느라고 조금씩 수정을 해보았어요!\n",
        "제가 만든 이 모델을 함께 분석해봐요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [torch.nn.Module - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wIMxj_7cxSH",
        "outputId": "211836aa-de6a-4dc5-a011-6eea656dd073"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "# 하지만 아래 과제를 진행하기 전에 아래 코드를 보면서 최대한 이해해보세요!\n",
        "\n",
        "# Function\n",
        "class Function_A(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * 2\n",
        "        return x\n",
        "\n",
        "class Function_B(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W1 = Parameter(torch.Tensor([10]))\n",
        "        self.W2 = Parameter(torch.Tensor([2]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x / self.W1\n",
        "        x = x / self.W2\n",
        "\n",
        "        return x\n",
        "\n",
        "class Function_C(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.register_buffer('duck', torch.Tensor([7]), persistent=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * self.duck\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Function_D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W1 = Parameter(torch.Tensor([3]))\n",
        "        self.W2 = Parameter(torch.Tensor([5]))\n",
        "        self.c = Function_C()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.W1\n",
        "        x = self.c(x)\n",
        "        x = x / self.W2\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Layer\n",
        "class Layer_AB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.a = Function_A('duck')\n",
        "        self.b = Function_B()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.a(x) / 5\n",
        "        x = self.b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Layer_CD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = Function_C()\n",
        "        self.d = Function_D()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c(x)\n",
        "        x = self.d(x) + 1\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    docstirng\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ab = Layer_AB()\n",
        "        self.cd = Layer_CD()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ab(x)\n",
        "        x = self.cd(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "x = torch.tensor([7])\n",
        "\n",
        "model = Model()\n",
        "model.state_dict()\n",
        "# model(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('ab.b.W1', tensor([10.])),\n",
              "             ('ab.b.W2', tensor([2.])),\n",
              "             ('cd.c.duck', tensor([7.])),\n",
              "             ('cd.d.W1', tensor([3.])),\n",
              "             ('cd.d.W2', tensor([5.])),\n",
              "             ('cd.d.c.duck', tensor([7.]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvtNG5hkeb2G"
      },
      "source": [
        "##### 💡 named_children vs named_modules\n",
        "> 🦆 부덕이가 코드를 작성해주었어요\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "제가 만든 모델에서 어떤 module들이 있었는지 기억이 나질 않아요!\n",
        "그래서 모델 내부의 module들 목록을 보고싶어요!\n",
        "\n",
        "Documentation을 찾아보니까 children이나 module이라는 이름을 가진\n",
        "함수가 바로 제가 원하는 기능을 가진 것 같아요!\n",
        "\n",
        "하지만 이 둘은 무슨 차이일까요?\n",
        "역시 직접 코드를 통해서 확인해봐야겠어요!\n",
        "```\n",
        "\n",
        "- [named_children - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=child#torch.nn.Module.named_children)\n",
        "- [named_modules - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=named#torch.nn.Module.named_modules)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSZh9S6SjCSQ",
        "outputId": "87e74f80-d799-40f8-ecc7-26e47348dd6d"
      },
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(f\"[ Name ] : {name}\\n[ Module ]\\n{module}\")\n",
        "    print(\"-\" * 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Name ] : \n",
            "[ Module ]\n",
            "Model(\n",
            "  (ab): Layer_AB(\n",
            "    (a): Function_A()\n",
            "    (b): Function_B()\n",
            "  )\n",
            "  (cd): Layer_CD(\n",
            "    (c): Function_C()\n",
            "    (d): Function_D(\n",
            "      (c): Function_C()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : ab\n",
            "[ Module ]\n",
            "Layer_AB(\n",
            "  (a): Function_A()\n",
            "  (b): Function_B()\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : ab.a\n",
            "[ Module ]\n",
            "Function_A()\n",
            "------------------------------\n",
            "[ Name ] : ab.b\n",
            "[ Module ]\n",
            "Function_B()\n",
            "------------------------------\n",
            "[ Name ] : cd\n",
            "[ Module ]\n",
            "Layer_CD(\n",
            "  (c): Function_C()\n",
            "  (d): Function_D(\n",
            "    (c): Function_C()\n",
            "  )\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : cd.c\n",
            "[ Module ]\n",
            "Function_C()\n",
            "------------------------------\n",
            "[ Name ] : cd.d\n",
            "[ Module ]\n",
            "Function_D(\n",
            "  (c): Function_C()\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : cd.d.c\n",
            "[ Module ]\n",
            "Function_C()\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKfekNTn3D3n",
        "outputId": "ae7c198f-cd66-4c63-deba-ad807d36ed70"
      },
      "source": [
        "for name, child in model.named_children():\n",
        "    print(f\"[ Name ] : {name}\\n[ Children ]\\n{child}\")\n",
        "    print(\"-\" * 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Name ] : ab\n",
            "[ Children ]\n",
            "Layer_AB(\n",
            "  (a): Function_A()\n",
            "  (b): Function_B()\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : cd\n",
            "[ Children ]\n",
            "Layer_CD(\n",
            "  (c): Function_C()\n",
            "  (d): Function_D(\n",
            "    (c): Function_C()\n",
            "  )\n",
            ")\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc5uZnP929pS"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "아하! 이제 알겠어요!\n",
        "\n",
        "\"children\"은 한 단계 아래의 submodule까지만 표시하는 것이고\n",
        "\"modules\"는 자신에게 속하는 모든 submodule들을 표시해주는 것이군요!\n",
        "\n",
        "\"named_modules\", \"named_children\"은 module의 이름도 돌려주는데\n",
        "그냥 module만 필요한 경우는 \"modules\", \"children\"를 사용하면 되겠네요!\n",
        "\n",
        "아니 그런데 왜 \"Function_D\"가 \"Function_C\"를 참조하는 중이죠?\n",
        "같은 기본 단위는 서로 독립적이어야 하는데 이러면 문제가 있을 것 같아요!\n",
        "분석이 끝나면 수정을 해야겠어요!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC4XBN5k5g53"
      },
      "source": [
        "##### 💡 get_submodule\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "제가 만든 모델 내부에 어떤 module들이 있는지 잘 알겠어요!\n",
        "이제 제가 원하는 특정 module만을 가져오고 싶어요!\n",
        "\n",
        "Function_A를 가져다 줄 수 있을까요?\n",
        "```\n",
        "\n",
        "- [get_submodule - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=get_submodule#torch.nn.Module.get_submodule)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSYN3x5hl4Wf",
        "outputId": "9614cd32-bca2-48cc-e381-5cfa7f1c0a20"
      },
      "source": [
        "# TODO : Function_A 를 가져오세요!\n",
        "submodule = model.get_submodule('ab.a')\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if submodule.__class__.__name__  == 'Function_A':\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87fdrrUW_Eyg"
      },
      "source": [
        "##### 💡 Parameter\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "이제 module에 대한 자신감이 생겼어요!\n",
        "그런데 문득 생각해보니까 Parameter에 대해서 공부하면서\n",
        "제가 어떤 모듈에 Parameter를 생성해놨던 것 같아요!\n",
        "\n",
        "저희가 module에서 module 목록을 보고 특정 module을\n",
        "찾기도 했던 것처럼 Parameter에서도 똑같이 해봐요!\n",
        "```\n",
        "\n",
        "- [parameters - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=parameters#torch.nn.Module.parameters)\n",
        "- [named_parameters - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=named#torch.nn.Module.named_parameters)\n",
        "- [get_parameter - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=get#torch.nn.Module.get_parameter)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HZk5-vlly_P",
        "outputId": "49074ca7-40df-463a-9f44-8379b89356f7"
      },
      "source": [
        "# 🦆 제가 4개의 Parameter를 만들었었군요!\n",
        "print(model.state_dict())\n",
        "for name, parameter in model.named_parameters():\n",
        "    print(f\"[ Name ] : {name}\\n[ Parameter ]\\n{parameter}\")\n",
        "    print(\"-\" * 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('ab.b.W1', tensor([10.])), ('ab.b.W2', tensor([2.])), ('cd.c.duck', tensor([7.])), ('cd.d.W1', tensor([3.])), ('cd.d.W2', tensor([5.])), ('cd.d.c.duck', tensor([7.]))])\n",
            "[ Name ] : ab.b.W1\n",
            "[ Parameter ]\n",
            "Parameter containing:\n",
            "tensor([10.], requires_grad=True)\n",
            "------------------------------\n",
            "[ Name ] : ab.b.W2\n",
            "[ Parameter ]\n",
            "Parameter containing:\n",
            "tensor([2.], requires_grad=True)\n",
            "------------------------------\n",
            "[ Name ] : cd.d.W1\n",
            "[ Parameter ]\n",
            "Parameter containing:\n",
            "tensor([3.], requires_grad=True)\n",
            "------------------------------\n",
            "[ Name ] : cd.d.W2\n",
            "[ Parameter ]\n",
            "Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWwtgIOBD5-0"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "아하! 제가 4개의 Parameter를 만들었었네요!\n",
        "이름을 표시하니까 Parameter가 어디에 속하는지 알 수 있어 너무 좋아요!\n",
        "\n",
        "- \"Function_B\"에 W1, W2 Parameter 2개\n",
        "- \"Function_D\"에 W1, W2 Parameter 2개\n",
        "\n",
        "\"parameters\"를 사용해서 목록을 확인해도 되지만\n",
        "이름이 표시 안되니까 어떤 module에 속한\n",
        "Parameter인지 알기가 너무 힘들 것 같아요!\n",
        "\n",
        "Function_B에 속해있는 Parameter W1을 사용하고 싶은데\n",
        "이를 가져다 주실수 있나요?\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIhreLUX_Eyj",
        "outputId": "cfc41dcd-9b10-4c10-c379-b79d45a3f253"
      },
      "source": [
        "# TODO : Function_B에 속하는 Parameter W1을 가져오세요!\n",
        "parameter = model.get_parameter('ab.b.W1')\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if parameter  == 10:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZC6RK20H8iC"
      },
      "source": [
        "##### 💡 Buffer\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "생각을 해보니까 buffer도 하나 추가해뒀던 것 같아요!\n",
        "buffer도 같이 분석해봐요!\n",
        "```\n",
        "\n",
        "- [buffers - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=buffers#torch.nn.Module.buffers)\n",
        "- [named_buffers - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=named_buffers#torch.nn.Module.named_buffers)\n",
        "- [get_buffer - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=get_buffer#torch.nn.Module.get_buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AxUn_jgH8iE",
        "outputId": "834dcb18-8a6c-4d0c-902d-d34b50e56d0f"
      },
      "source": [
        "# TODO : named_buffers를 사용해서 model에 속하는 buffer 전체 목록을 가져오세요!\n",
        "for name, buffer in model.named_buffers():\n",
        "    print(f\"[ Name ] : {name}\\n[ Buffer ] : {buffer}\")\n",
        "    print(\"-\" * 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Name ] : cd.c.duck\n",
            "[ Buffer ] : tensor([7.])\n",
            "------------------------------\n",
            "[ Name ] : cd.d.c.duck\n",
            "[ Buffer ] : tensor([7.])\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWjaJqgWI5r-",
        "outputId": "e7989384-95f9-49b0-a497-f0f7187da285"
      },
      "source": [
        "# TODO : buffers를 사용해서 model에 속하는 buffer 전체 목록을 가져오세요!\n",
        "for buffer in model.buffers():\n",
        "    print(f\"[ Buffer ] : {buffer}\")\n",
        "    print(\"-\" * 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Buffer ] : tensor([7.])\n",
            "------------------------------\n",
            "[ Buffer ] : tensor([7.])\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2ePuoOWH8iF",
        "outputId": "3ace4e4f-0f62-4227-f6ea-3e4c889755db"
      },
      "source": [
        "# TODO : Function_C에 속하는 Buffer를 가져오세요!\n",
        "buffer = model.get_buffer('cd.c.duck')\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if buffer == 7:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfrjK8Q5tNH6"
      },
      "source": [
        "##### 💡 Docstring\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "아차! 깜빡 잊었네요!\n",
        "\n",
        "PyTorch에서 제공하는 함수나 클래스들은 모두 Docstring이 작성되어있어요!\n",
        "하지만 Documentation을 보는 것이 더 편리하기 때문에 굳이 이용하지는 않죠!\n",
        "\n",
        "custom 모델을 만드는 중이고 이 custom 모델을 사용할\n",
        "다른 개발자들과 미래의 자신을 위해서 Docstring 작성은 필수에요!\n",
        "나중에 Documentation을 만들때도 이 Docstring이 있다면 만들기가 수월하겠죠?\n",
        "\n",
        "기초적인 실수를 했네요!\n",
        "나중에 Docstring을 추가해야겠어요!\n",
        "```\n",
        "- [Docstring - Wikipedia](https://en.wikipedia.org/wiki/Docstring)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCkBRdsItfTQ",
        "outputId": "5340aeb6-49f0-4cf8-be18-1b7031ce23ac"
      },
      "source": [
        "print(model.__doc__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    docstirng\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeIqAIk1NTUh"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 부덕이 모델 수정하기 - module 참조 제거\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "제 모델을 분석하면서 알게 되었는데 제가 가장 기본 단위로 만들어놓은\n",
        "\"Function_D\"가 \"Function_C\"를 포함하고 있더라구요!\n",
        "저는 기본 단위인 module끼리 서로 참조하는 걸 원하지 않아요!\n",
        "\n",
        "그리고 \"Function_C\"를 \"Function_D\"에서 제거해도\n",
        "전체 연산의 결과는 동일하게 하고 싶어요!\n",
        "\n",
        "같이 수정 해봐요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [torch.nn.Module - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A95FtG-1Px7k",
        "outputId": "9f3fe836-3092-4b4c-f802-a1c169ea227c"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# Function\n",
        "class Function_A(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * 2\n",
        "        return x\n",
        "\n",
        "class Function_B(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W1 = Parameter(torch.Tensor([10]))\n",
        "        self.W2 = Parameter(torch.Tensor([2]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x / self.W1\n",
        "        x = x / self.W2\n",
        "\n",
        "        return x\n",
        "\n",
        "class Function_C(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.register_buffer('duck', torch.Tensor([7]), persistent=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * self.duck\n",
        "        \n",
        "        return x\n",
        "\n",
        "# TODO : Function_C에 대한 참조를 없애고, 대체할 수 있는 연산을 추가하세요!\n",
        "#        전체 모델의 연산 결과는 바뀌면 안됩니다!\n",
        "class Function_D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W1 = Parameter(torch.Tensor([3]))\n",
        "        self.W2 = Parameter(torch.Tensor([5]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.W1\n",
        "        x = x * torch.Tensor([7])\n",
        "        x = x / self.W2\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Layer\n",
        "class Layer_AB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.a = Function_A('duck')\n",
        "        self.b = Function_B()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.a(x) / 5\n",
        "        x = self.b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Layer_CD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = Function_C()\n",
        "        self.d = Function_D()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c(x)\n",
        "        x = self.d(x) + 1\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ab = Layer_AB()\n",
        "        self.cd = Layer_CD()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ab(x)\n",
        "        x = self.cd(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([7])\n",
        "\n",
        "model = Model()\n",
        "output = model(x)\n",
        "\n",
        "fixed = 1\n",
        "# test = Function_C()\n",
        "# output = test(torch.Tensor([1]))\n",
        "# print(output)\n",
        "for name, _ in model.named_modules():\n",
        "    if 'cd.d.c' == name:\n",
        "        print(\"🦆 아직 Function_D가 Function_C를 참조하고 있네요!\")\n",
        "        fixed = 0\n",
        "\n",
        "if fixed:\n",
        "    if output == 6.5720:\n",
        "        print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "    else:\n",
        "        print(output)\n",
        "        print(\"🦆 module간 참조는 성공적으로 수정되었는데 모델 연산이 달라졌어요!\")\n",
        "        print(\"🦆 Function_C를 제거하시면서 해당 연산을 대체하는 연산을 Function_D에 추가하세요!\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij7zgUmsUwD6"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 부덕이 모델 수정하기 - module 출력 내용 변경하기\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "Function 단위 module인 \"Function_A\"의 경우에\n",
        "instance를 생성하려면 이름을 입력하도록 했어요!\n",
        "그래서 다음처럼 \"Function_A\"를 생성하였죠!\n",
        "\n",
        "# Function_A('duck')\n",
        "\n",
        "저는 완성된 모델을 출력하면 다음처럼 나오길 원했어요!\n",
        "\n",
        "✅ 부덕이가 원하는 이상적인 출력\n",
        "Model(\n",
        "  (ab): Layer_AB(\n",
        "    (a): Function_A(name=duck)\n",
        "    (b): Function_B()\n",
        "  )\n",
        "  (cd): Layer_CD(\n",
        "    (c): Function_C()\n",
        "    (d): Function_D()\n",
        "  )\n",
        ")\n",
        "\n",
        "하지만 실제로 출력해보니 다음과 같이 나오더라구요.\n",
        "\n",
        "❌ 실제 출력 결과\n",
        "Model(\n",
        "  (ab): Layer_AB(\n",
        "    (a): Function_A()\n",
        "    (b): Function_B()\n",
        "  )\n",
        "  (cd): Layer_CD(\n",
        "    (c): Function_C()\n",
        "    (d): Function_D()\n",
        "  )\n",
        ")\n",
        "\n",
        "이를 어쩌면 좋을까요? 꽉꽉!💦\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [extra_repr - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=repr#torch.nn.Module.extra_repr)\n",
        "\n",
        "🎁 **힌트** 🎁\n",
        "- nn.Module 클래스에서 repr관련 method를 찾아보세요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enxjXUKNUwD9",
        "outputId": "478a6639-eca9-4b0f-8bf0-36d479ad8c5b"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# Function\n",
        "\n",
        "# TODO : Function_A를 수정해서 부덕이가 원하는 출력이 나오도록 해주세요!\n",
        "class Function_A(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * 2\n",
        "        return x\n",
        "      \n",
        "    # def __repr__(self):\n",
        "    #     return \"Function_A(name=duck)\"\n",
        "    def extra_repr(self):\n",
        "          return \"name={}\".format(self.name)\n",
        "    \n",
        "\n",
        "class Function_B(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W1 = Parameter(torch.Tensor([10]))\n",
        "        self.W2 = Parameter(torch.Tensor([2]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x / self.W1\n",
        "        x = x / self.W2\n",
        "\n",
        "        return x\n",
        "\n",
        "class Function_C(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.register_buffer('duck', torch.Tensor([7]), persistent=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * self.duck\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Function_D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W1 = Parameter(torch.Tensor([3]))\n",
        "        self.W2 = Parameter(torch.Tensor([5]))\n",
        "        self.register_buffer('duck', torch.Tensor([7]), persistent=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.W1\n",
        "        x = x * self.duck\n",
        "        x = x / self.W2\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Layer\n",
        "class Layer_AB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.a = Function_A('duck')\n",
        "        self.b = Function_B()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.a(x) / 5\n",
        "        x = self.b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Layer_CD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = Function_C()\n",
        "        self.d = Function_D()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c(x)\n",
        "        x = self.d(x) + 1\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ab = Layer_AB()\n",
        "        self.cd = Layer_CD()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ab(x)\n",
        "        x = self.cd(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "model = Model()\n",
        "\n",
        "model_repr = repr(model)\n",
        "\n",
        "print(\"모델 출력 결과\")\n",
        "print(\"-\" * 30)\n",
        "print(model_repr)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "answer = \"Model(\\n  (ab): Layer_AB(\\n    (a): Function_A(name=duck)\\n    (b): Function_B()\\n  )\\n  (cd): Layer_CD(\\n    (c): Function_C()\\n    (d): Function_D()\\n  )\\n)\"\n",
        "\n",
        "if model_repr == answer:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "    print(\"🦆 너무 고마워요 꽉꽉!\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "모델 출력 결과\n",
            "------------------------------\n",
            "Model(\n",
            "  (ab): Layer_AB(\n",
            "    (a): Function_A(name=duck)\n",
            "    (b): Function_B()\n",
            "  )\n",
            "  (cd): Layer_CD(\n",
            "    (c): Function_C()\n",
            "    (d): Function_D()\n",
            "  )\n",
            ")\n",
            "------------------------------\n",
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n",
            "🦆 너무 고마워요 꽉꽉!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6lHaqZqvnb5"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 부덕이 모델 수정하기 - Docstring 작성\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "가볍게 Docstring을 작성해봐요!\n",
        "Numpy, Pydoc, Google 등 다양한 스타일이 있으니까 궁금하면\n",
        "아래 링크에서 읽어보면서 공부해보도록 해요!\n",
        "\n",
        "지금은 스타일 상관없이 Docstring이라는 것을 추가하기만 해보죠!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [Docstrings in Python - Data Camp](https://www.datacamp.com/community/tutorials/docstrings-python)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8_KUU4AvncC",
        "outputId": "034bace3-3e90-43cf-ce58-6fff6c0f3a1a"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "# TODO : Docstring을 추가해보세요!\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    docstring\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "model = Model()\n",
        "\n",
        "if model.__doc__:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")\n",
        "help(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n",
            "Help on Model in module __main__ object:\n",
            "\n",
            "class Model(torch.nn.modules.module.Module)\n",
            " |  docstring\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Model\n",
            " |      torch.nn.modules.module.Module\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self)\n",
            " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __call__ = _call_impl(self, *input, **kwargs)\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __dir__(self)\n",
            " |      Default dir() implementation.\n",
            " |  \n",
            " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
            " |      Adds a child module to the current module.\n",
            " |      \n",
            " |      The module can be accessed as an attribute using the given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (string): name of the child module. The child module can be\n",
            " |              accessed from this module using the given name\n",
            " |          module (Module): child module to be added to the module.\n",
            " |  \n",
            " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
            " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
            " |      as well as self. Typical use includes initializing the parameters of a model\n",
            " |      (see also :ref:`nn-init-doc`).\n",
            " |      \n",
            " |      Args:\n",
            " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> @torch.no_grad()\n",
            " |          >>> def init_weights(m):\n",
            " |          >>>     print(m)\n",
            " |          >>>     if type(m) == nn.Linear:\n",
            " |          >>>         m.weight.fill_(1.0)\n",
            " |          >>>         print(m.weight)\n",
            " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
            " |          >>> net.apply(init_weights)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 1.,  1.],\n",
            " |                  [ 1.,  1.]])\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 1.,  1.],\n",
            " |                  [ 1.,  1.]])\n",
            " |          Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |          Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |  \n",
            " |  bfloat16(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
            " |      Returns an iterator over module buffers.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          torch.Tensor: module buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for buf in model.buffers():\n",
            " |          >>>     print(type(buf), buf.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  children(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Returns an iterator over immediate children modules.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a child module\n",
            " |  \n",
            " |  cpu(self: ~T) -> ~T\n",
            " |      Moves all model parameters and buffers to the CPU.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Moves all model parameters and buffers to the GPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on GPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  double(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  eval(self: ~T) -> ~T\n",
            " |      Sets the module in evaluation mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  extra_repr(self) -> str\n",
            " |      Set the extra representation of the module\n",
            " |      \n",
            " |      To print customized extra information, you should re-implement\n",
            " |      this method in your own modules. Both single-line and multi-line\n",
            " |      strings are acceptable.\n",
            " |  \n",
            " |  float(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  forward = _forward_unimplemented(self, *input: Any) -> None\n",
            " |      Defines the computation performed at every call.\n",
            " |      \n",
            " |      Should be overridden by all subclasses.\n",
            " |      \n",
            " |      .. note::\n",
            " |          Although the recipe for forward pass needs to be defined within\n",
            " |          this function, one should call the :class:`Module` instance afterwards\n",
            " |          instead of this since the former takes care of running the\n",
            " |          registered hooks while the latter silently ignores them.\n",
            " |  \n",
            " |  get_buffer(self, target: str) -> 'Tensor'\n",
            " |      Returns the buffer given by ``target`` if it exists,\n",
            " |      otherwise throws an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the buffer\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.Tensor: The buffer referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not a\n",
            " |              buffer\n",
            " |  \n",
            " |  get_parameter(self, target: str) -> 'Parameter'\n",
            " |      Returns the parameter given by ``target`` if it exists,\n",
            " |      otherwise throws an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the Parameter\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Parameter``\n",
            " |  \n",
            " |  get_submodule(self, target: str) -> 'Module'\n",
            " |      Returns the submodule given by ``target`` if it exists,\n",
            " |      otherwise throws an error.\n",
            " |      \n",
            " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
            " |      looks like this:\n",
            " |      \n",
            " |      .. code-block::text\n",
            " |      \n",
            " |          A(\n",
            " |              (net_b): Module(\n",
            " |                  (net_c): Module(\n",
            " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
            " |                  )\n",
            " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
            " |              )\n",
            " |          )\n",
            " |      \n",
            " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
            " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
            " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
            " |      \n",
            " |      To check whether or not we have the ``linear`` submodule, we\n",
            " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
            " |      we have the ``conv`` submodule, we would call\n",
            " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
            " |      \n",
            " |      The runtime of ``get_submodule`` is bounded by the degree\n",
            " |      of module nesting in ``target``. A query against\n",
            " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
            " |      the number of transitive modules. So, for a simple check to see\n",
            " |      if some submodule exists, ``get_submodule`` should always be\n",
            " |      used.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the submodule\n",
            " |              to look for. (See above example for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Module: The submodule referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Module``\n",
            " |  \n",
            " |  half(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
            " |      Copies parameters and buffers from :attr:`state_dict` into\n",
            " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
            " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
            " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
            " |      \n",
            " |      Args:\n",
            " |          state_dict (dict): a dict containing parameters and\n",
            " |              persistent buffers.\n",
            " |          strict (bool, optional): whether to strictly enforce that the keys\n",
            " |              in :attr:`state_dict` match the keys returned by this module's\n",
            " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
            " |      \n",
            " |      Returns:\n",
            " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
            " |              * **missing_keys** is a list of str containing the missing keys\n",
            " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
            " |  \n",
            " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Returns an iterator over all modules in the network.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a module in the network\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.modules()):\n",
            " |                  print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
            " |  \n",
            " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
            " |      Returns an iterator over module buffers, yielding both the\n",
            " |      name of the buffer as well as the buffer itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all buffer names.\n",
            " |          recurse (bool): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for name, buf in self.named_buffers():\n",
            " |          >>>    if name in ['running_var']:\n",
            " |          >>>        print(buf.size())\n",
            " |  \n",
            " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
            " |      Returns an iterator over immediate children modules, yielding both\n",
            " |      the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, Module): Tuple containing a name and child module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for name, module in model.named_children():\n",
            " |          >>>     if name in ['conv4', 'conv5']:\n",
            " |          >>>         print(module)\n",
            " |  \n",
            " |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
            " |      Returns an iterator over all modules in the network, yielding\n",
            " |      both the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          memo: a memo to store the set of modules already added to the result\n",
            " |          prefix: a prefix that will be added to the name of the module\n",
            " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
            " |          or not\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, Module): Tuple of name and module\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.named_modules()):\n",
            " |                  print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> ('', Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          ))\n",
            " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
            " |  \n",
            " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
            " |      Returns an iterator over module parameters, yielding both the\n",
            " |      name of the parameter as well as the parameter itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all parameter names.\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, Parameter): Tuple containing the name and parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for name, param in self.named_parameters():\n",
            " |          >>>    if name in ['bias']:\n",
            " |          >>>        print(param.size())\n",
            " |  \n",
            " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
            " |      Returns an iterator over module parameters.\n",
            " |      \n",
            " |      This is typically passed to an optimizer.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Parameter: module parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for param in model.parameters():\n",
            " |          >>>     print(type(param), param.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Registers a backward hook on the module.\n",
            " |      \n",
            " |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
            " |      the behavior of this function will change in future versions.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
            " |      Adds a buffer to the module.\n",
            " |      \n",
            " |      This is typically used to register a buffer that should not to be\n",
            " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
            " |      is not a parameter, but is part of the module's state. Buffers, by\n",
            " |      default, are persistent and will be saved alongside parameters. This\n",
            " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
            " |      only difference between a persistent buffer and a non-persistent buffer\n",
            " |      is that the latter will not be a part of this module's\n",
            " |      :attr:`state_dict`.\n",
            " |      \n",
            " |      Buffers can be accessed as attributes using given names.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (string): name of the buffer. The buffer can be accessed\n",
            " |              from this module using the given name\n",
            " |          tensor (Tensor): buffer to be registered.\n",
            " |          persistent (bool): whether the buffer is part of this module's\n",
            " |              :attr:`state_dict`.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
            " |  \n",
            " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Registers a forward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time after :func:`forward` has computed an output.\n",
            " |      It should have the following signature::\n",
            " |      \n",
            " |          hook(module, input, output) -> None or modified output\n",
            " |      \n",
            " |      The input contains only the positional arguments given to the module.\n",
            " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
            " |      The hook can modify the output. It can modify the input inplace but\n",
            " |      it will not have effect on forward since this is called after\n",
            " |      :func:`forward` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Registers a forward pre-hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time before :func:`forward` is invoked.\n",
            " |      It should have the following signature::\n",
            " |      \n",
            " |          hook(module, input) -> None or modified input\n",
            " |      \n",
            " |      The input contains only the positional arguments given to the module.\n",
            " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
            " |      The hook can modify the input. User can either return a tuple or a\n",
            " |      single modified value in the hook. We will wrap the value into a tuple\n",
            " |      if a single value is returned(unless that value is already a tuple).\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Registers a backward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time the gradients with respect to module\n",
            " |      inputs are computed. The hook should have the following signature::\n",
            " |      \n",
            " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
            " |      \n",
            " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
            " |      with respect to the inputs and outputs respectively. The hook should\n",
            " |      not modify its arguments, but it can optionally return a new gradient with\n",
            " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
            " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
            " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
            " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
            " |      arguments.\n",
            " |      \n",
            " |      .. warning ::\n",
            " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
            " |          will raise an error.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
            " |      Adds a parameter to the module.\n",
            " |      \n",
            " |      The parameter can be accessed as an attribute using given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (string): name of the parameter. The parameter can be accessed\n",
            " |              from this module using the given name\n",
            " |          param (Parameter): parameter to be added to the module.\n",
            " |  \n",
            " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
            " |      Change if autograd should record operations on parameters in this\n",
            " |      module.\n",
            " |      \n",
            " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
            " |      in-place.\n",
            " |      \n",
            " |      This method is helpful for freezing part of the module for finetuning\n",
            " |      or training parts of a model individually (e.g., GAN training).\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Args:\n",
            " |          requires_grad (bool): whether autograd should record operations on\n",
            " |                                parameters in this module. Default: ``True``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  share_memory(self: ~T) -> ~T\n",
            " |      See :meth:`torch.Tensor.share_memory_`\n",
            " |  \n",
            " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
            " |      Returns a dictionary containing a whole state of the module.\n",
            " |      \n",
            " |      Both parameters and persistent buffers (e.g. running averages) are\n",
            " |      included. Keys are corresponding parameter and buffer names.\n",
            " |      \n",
            " |      Returns:\n",
            " |          dict:\n",
            " |              a dictionary containing a whole state of the module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> module.state_dict().keys()\n",
            " |          ['bias', 'weight']\n",
            " |  \n",
            " |  to(self, *args, **kwargs)\n",
            " |      Moves and/or casts the parameters and buffers.\n",
            " |      \n",
            " |      This can be called as\n",
            " |      \n",
            " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
            " |      \n",
            " |      .. function:: to(dtype, non_blocking=False)\n",
            " |      \n",
            " |      .. function:: to(tensor, non_blocking=False)\n",
            " |      \n",
            " |      .. function:: to(memory_format=torch.channels_last)\n",
            " |      \n",
            " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
            " |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
            " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
            " |      (if given). The integral parameters and buffers will be moved\n",
            " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
            " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
            " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
            " |      pinned memory to CUDA devices.\n",
            " |      \n",
            " |      See below for examples.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): the desired device of the parameters\n",
            " |              and buffers in this module\n",
            " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
            " |              the parameters and buffers in this module\n",
            " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
            " |              dtype and device for all parameters and buffers in this module\n",
            " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
            " |              format for 4D parameters and buffers in this module (keyword\n",
            " |              only argument)\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> linear = nn.Linear(2, 2)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]])\n",
            " |          >>> linear.to(torch.double)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
            " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
            " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
            " |          >>> cpu = torch.device(\"cpu\")\n",
            " |          >>> linear.to(cpu)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
            " |      \n",
            " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
            " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
            " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
            " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
            " |  \n",
            " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
            " |      Moves the parameters and buffers to the specified device without copying storage.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): The desired device of the parameters\n",
            " |              and buffers in this module.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  train(self: ~T, mode: bool = True) -> ~T\n",
            " |      Sets the module in training mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      Args:\n",
            " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
            " |                       mode (``False``). Default: ``True``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
            " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          dst_type (type or string): the desired type\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Moves all model parameters and buffers to the XPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on XPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  zero_grad(self, set_to_none: bool = False) -> None\n",
            " |      Sets gradients of all model parameters to zero. See similar function\n",
            " |      under :class:`torch.optim.Optimizer` for more context.\n",
            " |      \n",
            " |      Args:\n",
            " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
            " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  T_destination = ~T_destination\n",
            " |  \n",
            " |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
            " |  \n",
            " |  dump_patches = False\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tka7VMeekxd7"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> BatchNorm1d 분석해보기\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "저 이제 자신감이 생긴 것 같아요!\n",
        "\n",
        "PyTorch가 미리 만들어둔 module중 하나를 분석해서\n",
        "제가 어느정도 성장했는지 알아보고 싶어요!\n",
        "\n",
        "음.. 뭐가 좋지? BatchNorm1d으로 해봐요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [torch.nn.BatchNorm1d - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUshguoBlTNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125075c6-dd75-4e5c-e21f-21581d71dfcc"
      },
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "\n",
        "module = nn.BatchNorm1d(10)\n",
        "\n",
        "# for parameter in module.buffers():\n",
        "#     print(parameter)\n",
        "\n",
        "# TODO : nn.BatchNorm1d의 parameter와 buffer 갯수를 알아내세요!\n",
        "parameter_n = 2\n",
        "buffer_n = 3\n",
        "\n",
        "# TODO : nn.BatchNorm1d의 buffer 이름을 알아내세요!\n",
        "#        [이름, 이름, 이름] 형태로 저장해주세요!\n",
        "\n",
        "buffer_list =[]\n",
        "for name, buffer in module.named_buffers():\n",
        "    buffer_list.append(name)\n",
        "\n",
        "buffer_names = buffer_list\n",
        "print(buffer_list)\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "answer = set(['running_mean', 'running_var', 'num_batches_tracked'])\n",
        "\n",
        "if parameter_n == 2 and buffer_n == 3 and answer == set(buffer_names):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['running_mean', 'running_var', 'num_batches_tracked']\n",
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFLQWT3w0m4v"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "Docstring을 보는 것보다 PyTorch 공식 문서를 보는게 훨씬 편해서\n",
        "BatchNorm1d는 별도로 Docstring을 보지 않았어요!\n",
        "\n",
        "하지만 Documentation이 없는 모델을 사용중이라면\n",
        "Docstring을 Documentation처럼 여기고 꼼꼼히 보아야 해요!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2clV-zzGiH-v"
      },
      "source": [
        "### ☄️ nn.Module 알쓸신잡\n",
        "> 위에서 다루지 않았지만 nn.Module이 제공하는 기능 중에서 알아두면 유용한 기능에 대해 살펴볼 것입니다!\n",
        "\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> hook\n",
        "- 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> apply\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tJVT5Mp9I4S"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> hook\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "hook라는 생소한 용어를 처음 들어보았어요!\n",
        "이게 무엇일까요? 왠지 모르게 겁이 나네요!\n",
        "\n",
        "인터넷으로 찾아보니까 패키지화된 코드에서 다른 프로그래머가\n",
        "custom 코드를 중간에 실행시킬 수 있도록 만들어놓은 인터페이스라고 하네요!\n",
        "\n",
        "- 프로그램의 실행 로직을 분석하거나\n",
        "- 프로그램에 추가적인 기능을 제공하고 싶을 때\n",
        "\n",
        "hook이 사용된다고 해요!\n",
        "\n",
        "알듯말듯 하네요! 친구가 도움을 준다고 하니 같이 배워봐요! \n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [hook - WhatIs](https://whatis.techtarget.com/definition/hook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HujUYII-wih"
      },
      "source": [
        "##### 💡 hook의 원리\n",
        "> 🦆 부덕이 친구가 코드를 작성해주었어요\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "hook이 무엇인지 정말 모르겠다고 하니까 친구가 코드를 작성해주었어요!\n",
        "이 코드를 보고나면 한결 이해가 수월해질 거라고 하더라구요!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nok5e_w1PjwS",
        "outputId": "f8c6af1f-3b81-4923-94c0-04cd157c0a47"
      },
      "source": [
        "def program_A(x):\n",
        "    print('program A processing!')\n",
        "    return x + 3\n",
        "\n",
        "def program_B(x):\n",
        "    print('program B processing!')\n",
        "    return x - 3\n",
        "\n",
        "def hook(x):\n",
        "    print(\"hook 작동\")\n",
        "    return x + 1\n",
        "class Package(object):\n",
        "    \"\"\"프로그램 A와 B를 묶어놓은 패키지 코드\"\"\"\n",
        "    def __init__(self):\n",
        "        self.programs = [program_A, program_B]\n",
        "        self.hooks = []\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for program in self.programs:\n",
        "            x = program(x)\n",
        "\n",
        "            # Package를 사용하는 사람이 자신만의 custom program을\n",
        "            # 등록할 수 있도록 미리 만들어놓은 인터페이스 hook\n",
        "            if self.hooks:\n",
        "                for hook in self.hooks:\n",
        "                    output = hook(x)\n",
        "\n",
        "                    # return 값이 있는 hook의 경우에만 x를 업데이트 한다\n",
        "                    if output:\n",
        "                        x = output\n",
        "\n",
        "        return x\n",
        "\n",
        "# 패키지 생성\n",
        "package = Package()\n",
        "package.hooks.append(hook)\n",
        "\n",
        "# 패키지 실행\n",
        "input = 3\n",
        "output = package(input)\n",
        "\n",
        "# 패키지 결과\n",
        "print(f\"Package Process Result! [ input {input} ] [ output {output} ]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program A processing!\n",
            "hook 작동\n",
            "program B processing!\n",
            "hook 작동\n",
            "Package Process Result! [ input 3 ] [ output 5 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KIn1D59SxDf"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "코드를 보고나서 조금 감이 오기 시작한 것 같아요!\n",
        "\n",
        "Package은 원래부터 self.hooks 라는 변수를 가지고 있었어요!\n",
        "그래서 Package를 실행하면 패키지에 포함된 프로그램을 하나씩 \n",
        "실행하는 중간 중간 self.hooks 에 등록된 함수가 있는지 체크하게 되는거죠!\n",
        "\n",
        "- self.hooks 에 등록된 함수가 있으면 실행 ✅\n",
        "- self.hooks 에 등록된 함수가 없으면 무시 ❌\n",
        "\n",
        "Package를 사용하는 개발자들이 자신의 custom 코드를 Package 중간중간에\n",
        "실행할 수 있도록 Package 제작한 개발자들이 미리 만들어놓은 인터페이스인 거예요!\n",
        "```\n",
        "```python\n",
        "😯\n",
        "세상에! 그럼 저희가 그동안 써오던 여러 패키지들도\n",
        "저희가 custom 코드를 패키지 내부에서 실행시킬 수 있도록\n",
        "\"hook\"이라고 불리는 인터페이스를 가지고 있었을 수도 있겠군요!\n",
        "```\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "역시 뭐든지 알아야 쓸 수 있는 것 같아요!\n",
        "\n",
        "친구가 준 hook의 사용 예시를 함께 봐요!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMmA-BQdeYfi",
        "outputId": "91acec0a-96aa-452d-e05b-9991beba152a"
      },
      "source": [
        "# Hook - 프로그램의 실행 로직 분석 사용 예시\n",
        "def hook_analysis(x):\n",
        "    print(f'hook for analysis, current value is {x}')\n",
        "\n",
        "# 생성된 패키지에 hook 추가\n",
        "package.hooks = []\n",
        "package.hooks.append(hook_analysis)\n",
        "\n",
        "# 패키지 실행\n",
        "input = 3\n",
        "output = package(input)\n",
        "\n",
        "# 패키지 결과\n",
        "print(f\"Package Process Result! [ input {input} ] [ output {output} ]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program A processing!\n",
            "hook for analysis, current value is 6\n",
            "program B processing!\n",
            "hook for analysis, current value is 3\n",
            "Package Process Result! [ input 3 ] [ output 3 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijgpx3s6fLAO",
        "outputId": "35df2dd0-36dc-448b-ce52-62f021e72d1b"
      },
      "source": [
        "# Hook - 프로그램에 기능 추가 예시\n",
        "def hook_multiply(x):\n",
        "    print('hook for multiply')\n",
        "    return x * 3\n",
        "\n",
        "# 생성된 패키지에 hook 추가\n",
        "package.hooks = []\n",
        "package.hooks.append(hook_multiply)\n",
        "\n",
        "# 패키지 실행\n",
        "input = 3\n",
        "output = package(input)\n",
        "\n",
        "# 패키지 결과\n",
        "print(f\"Package Process Result! [ input {input} ] [ output {output} ]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program A processing!\n",
            "hook for multiply\n",
            "program B processing!\n",
            "hook for multiply\n",
            "Package Process Result! [ input 3 ] [ output 45 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxWqcHgBg2PU",
        "outputId": "a4810bd7-eb17-496c-bebc-decc2de382f2"
      },
      "source": [
        "# 여러개의 hook을 넣을 수 있다\n",
        "package.hooks = []\n",
        "package.hooks.append(hook_multiply)\n",
        "package.hooks.append(hook_analysis)\n",
        "\n",
        "# 패키지 실행\n",
        "input = 3\n",
        "output = package(input)\n",
        "\n",
        "# 패키지 결과\n",
        "print(f\"Package Process Result! [ input {input} ] [ output {output} ]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program A processing!\n",
            "hook for multiply\n",
            "hook for analysis, current value is 18\n",
            "program B processing!\n",
            "hook for multiply\n",
            "hook for analysis, current value is 45\n",
            "Package Process Result! [ input 3 ] [ output 45 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVflo4JVhIHV"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "친구가 준 예시의 hook에서는 프로그램이 실행되고 난 이후에만\n",
        "hook 함수를 사용할 수가 있어요!\n",
        "\n",
        "만약 Package를 설계할 때 프로그램 실행 앞 뒤로 hook를 넣어둔다면\n",
        "프로그램의 실행 전과 후 모두 저희의 custom 함수를 실행할 수 있겠죠!\n",
        "\n",
        "그래서 프로그램 실행 전에 hook을 사용할 수 있도록\n",
        "pre_hook 이라는 인터페이스를 만들었보았어요!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1iz3uPh9I4b"
      },
      "source": [
        "def program_A(x):\n",
        "    print('program A processing!')\n",
        "    return x + 3\n",
        "\n",
        "def program_B(x):\n",
        "    print('program B processing!')\n",
        "    return x - 3\n",
        "\n",
        "class Package(object):\n",
        "    \"\"\"프로그램 A와 B를 묶어놓은 패키지 코드\"\"\"\n",
        "    def __init__(self):\n",
        "        self.programs = [program_A, program_B]\n",
        "\n",
        "        # hooks\n",
        "        self.pre_hooks = []\n",
        "        self.hooks = []\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for program in self.programs:\n",
        "            \n",
        "            # pre_hook\n",
        "            if self.pre_hooks:\n",
        "                for hook in self.pre_hooks:\n",
        "                    output = hook(x)\n",
        "                    if output:\n",
        "                        x = output\n",
        "\n",
        "            x = program(x)\n",
        "\n",
        "            # hook\n",
        "            if self.hooks:\n",
        "                for hook in self.hooks:\n",
        "                    output = hook(x)\n",
        "                    if output:\n",
        "                        x = output\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvZi6GDLh9iJ"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "친구한테 위의 코드를 보내주었더니 칭찬해주었어요!\n",
        "\n",
        "hook을 어디에다가 심어놓을 것인지는 설계자의 마음이라고 하였죠!\n",
        "보내준 예시에서는 Package쪽에다가 hook 인터페이스를 만들었지만\n",
        "program 내부에서 hook 인터페이스를 만들어둘 수도 있다고 하였어요!\n",
        "\n",
        "만약 이런 경우에는 프로그램 별로 다 다른 hook을 사용할 수 있기 때문에\n",
        "더욱 custom하기 좋다고 하였죠!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70LM7F_OjMqB"
      },
      "source": [
        "##### 💡 PyTorch의 hook\n",
        "> 🦆 부덕이가 코드를 작성해주었어요\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "hook을 이해하고 나서 PyTorch에는 어떤 hook이 있는지 살펴보았어요!\n",
        "크게 2가지로 나뉘더라구요!\n",
        "\n",
        "- Tensor에 적용하는 hook\n",
        "- Module에 적용하는 hook\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR6xLrKogJoy"
      },
      "source": [
        "```python\n",
        "🦆\n",
        "Tensor에 등록하는 hook의 경우에는 \"_backward_hooks\"에서 확인할 수 있어요!\n",
        "\n",
        "module과 다르게 tensor에는 backward hook만 있어요!\n",
        "forward hook이 있는지 찾아보았는데 없더라구요!\n",
        "```\n",
        "- [register_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N75CZzOwie3a",
        "outputId": "c76f1000-5205-45a9-b336-9d29bf18ede7"
      },
      "source": [
        "import torch\n",
        "\n",
        "tensor = torch.rand(1, requires_grad=True)\n",
        "\n",
        "def tensor_hook(grad):\n",
        "    pass\n",
        "\n",
        "tensor.register_hook(tensor_hook)\n",
        "\n",
        "# 🦆 tensor는 backward hook만 있어요!\n",
        "tensor._backward_hooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([(1, <function __main__.tensor_hook>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ_ll23vibx-"
      },
      "source": [
        "```python\n",
        "🦆\n",
        "nn.Module에 등록하는 모든 hook은 \"__dict__\"을 이용하면 한번에 확인이 가능해요!\n",
        "```\n",
        "\n",
        "- [register_forward_pre_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=hook#torch.nn.Module.register_forward_pre_hook)\n",
        "- [register_forward_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=hook#torch.nn.Module.register_forward_hook)\n",
        "- [register_backward_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=hook#torch.nn.Module.register_backward_hook)\n",
        "- [register_full_backward_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_full#torch.nn.Module.register_full_backward_hook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f522pSdWXgGA",
        "outputId": "3695945d-b0a1-4254-965d-39772b15dd11"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "def module_hook(grad):\n",
        "    pass\n",
        "\n",
        "model = Model()\n",
        "model.register_forward_pre_hook(module_hook)\n",
        "model.register_forward_hook(module_hook)\n",
        "model.register_full_backward_hook(module_hook)\n",
        "\n",
        "# 🦆 __dict__에는 module의 모든 변수와 parameter, hook 등의 중요한 정보가 담겨있어요!\n",
        "#    module이 정보의 저장소로 이용하는 공간인만큼 자세한 잊지 말고 나중에 필요할 때 사용해보세요!\n",
        "model.__dict__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_backward_hooks': OrderedDict([(4, <function __main__.module_hook>)]),\n",
              " '_buffers': OrderedDict(),\n",
              " '_forward_hooks': OrderedDict([(3, <function __main__.module_hook>)]),\n",
              " '_forward_pre_hooks': OrderedDict([(2, <function __main__.module_hook>)]),\n",
              " '_is_full_backward_hook': True,\n",
              " '_load_state_dict_pre_hooks': OrderedDict(),\n",
              " '_modules': OrderedDict(),\n",
              " '_non_persistent_buffers_set': set(),\n",
              " '_parameters': OrderedDict(),\n",
              " '_state_dict_hooks': OrderedDict(),\n",
              " 'training': True}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxX-trOlrvjh"
      },
      "source": [
        "```python\n",
        "🦆\n",
        "\"__dict__\" 을 확인하니까 다음의 5가지가 나오더라구요!\n",
        "\n",
        "- forward_pre_hooks\n",
        "- forward_hooks\n",
        "- backward_hooks # deprecated\n",
        "- full_backward_hooks\n",
        "- state_dict_hooks # used internally\n",
        "\n",
        "왠지 이름을 보면 언제 사용하는지 알 것 같아요!\n",
        "forward와 backward시에 각각 hook이 호출되는 거겠죠!\n",
        "\n",
        "- forward 시에는 pre_hook과 hook이 있고\n",
        "- backward는 시에는 hook만 존재하네요!\n",
        "- state_dict의 경우도 hook이 있는데 저희가 사용하는게 아니라\n",
        "  \"load_state_dict\" 함수가 내부적으로 사용한다고 하네요!\n",
        "  아래 링크 첨부한 곳에 내용이 쓰여있어요!\n",
        "\n",
        "nn.Module에서 이렇게 hook이라는 공간을 만들어 둔 줄 몰랐네요!\n",
        "저희 이제 잘 알게 되었으니까 기회가 되면 꼭 써봐요!\n",
        "\n",
        "매번 module를 실행할때마다 module은 등록한 hook이 있는지 없는지 체크하는데\n",
        "매번 등록된 hook이 없으면 module도 섭섭하지 않을까요?\n",
        "\n",
        "너무 슬플 것 같아요..\n",
        "```\n",
        "- [Invoking Time of nn.Module _register_state_dict_hook() - PyTorch Forum](https://discuss.pytorch.org/t/invoking-time-of-nn-module-register-state-dict-hook/108163)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r2HJhPzCqZM"
      },
      "source": [
        "##### 💡 forward hook\n",
        "``` python\n",
        "🦆\n",
        "뭐든지 직접 사용해봐야 이해가 더 잘되겠죠?\n",
        "\n",
        "module에만 적용할 수 있는 forward hook! 같이 사용해봐요!\n",
        "```\n",
        "\n",
        "**Module**\n",
        "- [register_forward_pre_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=hook#torch.nn.Module.register_forward_pre_hook)\n",
        "- [register_forward_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=hook#torch.nn.Module.register_forward_hook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfanMq-Et5gc"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "Add 모델에 어떤 값이 전파되는지 알아봐요!\n",
        "미리 만들어둔 list와 forward hook이면 분명 알아낼 수 있을 거예요!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMmXBTDhtOIc",
        "outputId": "f13881a5-495e-42f3-d2b5-a352e4ecd643"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# Add 모델을 수정하지 마세요! \n",
        "class Add(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() \n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        output = torch.add(x1, x2)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 모델 생성\n",
        "add = Add()\n",
        "\n",
        "# TODO: 답을 x1, x2, output 순서로 list에 차례차례 넣으세요! \n",
        "answer = []\n",
        "\n",
        "\n",
        "# TODO : pre_hook를 이용해서 x1, x2 값을 알아내 answer에 저장하세요\n",
        "def pre_hook(module, input):\n",
        "    answer.append(input[0])\n",
        "    answer.append(input[1])\n",
        "    module.input = input\n",
        "    pass\n",
        "\n",
        "# TODO : hook를 이용해서 output 값을 알아내 answer에 저장하세요\n",
        "def hook(module, input, output):\n",
        "    answer.append(output)\n",
        "    pass\n",
        "\n",
        "add.register_forward_pre_hook(pre_hook)\n",
        "add.register_forward_hook(hook)\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x1 = torch.rand(1)\n",
        "x2 = torch.rand(1)\n",
        "\n",
        "output = add(x1, x2)\n",
        "print(answer)\n",
        "print(add.input)\n",
        "\n",
        "if answer == [x1, x2, output]:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([0.6458]), tensor([0.6417]), tensor([1.2876])]\n",
            "(tensor([0.6458]), tensor([0.6417]))\n",
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyygsrsQD_4E"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "휴 생각보다 어려웠던 것 같아요!\n",
        "아직 익숙하지 않아서 그런가봐요!\n",
        "\n",
        "저희가 방금 hook를 이용해서 모델을 통해 전파되는 값들을 저장했는데\n",
        "이뿐만 아니라 전파되는 값을 수정도 가능하다고 들었어요!\n",
        "\n",
        "정말 가능한지 한번 해봐요!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj7hpOeCEv5-",
        "outputId": "a6f39224-2569-4838-dfd3-277fc808f60c"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Add 모델을 수정하지 마세요! \n",
        "class Add(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() \n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        output = torch.add(x1, x2)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 모델 생성\n",
        "add = Add()\n",
        "\n",
        "\n",
        "# TODO : hook를 이용해서 전파되는 output 값에 5를 더해보세요!\n",
        "def hook(module, input, output):\n",
        "    output += torch.Tensor([5.])\n",
        "    \n",
        "    return output\n",
        "\n",
        "add.register_forward_hook(hook)\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x1 = torch.rand(1)\n",
        "x2 = torch.rand(1)\n",
        "\n",
        "output = add(x1, x2)\n",
        "\n",
        "if output == x1 + x2 + 5:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdSszPOlFg2C"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "해냈어요! 저희가 전파되는 값을 수정했어요!\n",
        "\n",
        "hook 정말 강력하네요!\n",
        "잘 사용한다면 정말 유용할 것 같아요!\n",
        "\n",
        "또 어디에 사용해볼 수 있을까요?\n",
        "궁금해서 아래 글을 읽어보았는데 좋은 사례들이 있는 것 같아요!\n",
        "\n",
        "backward hook 관련 내용도 나오는데\n",
        "이 부분은 backward hook을 연습하고 마저 읽어야겠어요!\n",
        "```\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [How to Use PyTorch Hooks - Medium](https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904)\n",
        "- [PyTorch 101, Part 5: Understanding Hooks - Paperspace blog](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA_-_-XdtJwT"
      },
      "source": [
        "##### 💡 backward hook\n",
        "``` python\n",
        "🦆\n",
        "forward hook은 module에만 적용할 수 있지만\n",
        "backward hook은 tensor와 module 2가지에 적용할 수 있더라구요!\n",
        "\n",
        "같이 사용해봐요!\n",
        "```\n",
        "\n",
        "**Tensor**\n",
        "- [register_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook)\n",
        "\n",
        "**Module**\n",
        "- [register_backward_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=hook#torch.nn.Module.register_backward_hook)\n",
        "- [register_full_backward_hook - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_full#torch.nn.Module.register_full_backward_hook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qIlAHSCy19v"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "이제 gradient까지 다루기 시작하다니! 너무 흥분되요!\n",
        "모델에서 backpropagation할 때 뒤로 전파되는 gradient값을 같이 알아봐요!\n",
        "\n",
        "forward에서 했듯 list와 backward hook이면 분명 가능할거에요! \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT96uu7WYURO",
        "outputId": "7e89a609-8aef-4806-91c0-be6c6fc664a9"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# Model 모델을 수정하지 마세요! \n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W = Parameter(torch.Tensor([5]))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        output = x1 * x2\n",
        "        output = output * self.W\n",
        "\n",
        "        return output\n",
        "\n",
        "# 모델 생성\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# TODO: 답을 x1.grad, x2.grad, output.grad 순서로 list에 차례차례 넣으세요! \n",
        "answer = []\n",
        "\n",
        "# TODO : hook를 이용해서 x1.grad, x2.grad, output.grad 값을 알아내 answer에 저장하세요\n",
        "def module_hook(module, grad_input, grad_output):\n",
        "    answer.append(grad_input[0])\n",
        "    answer.append(grad_input[1])\n",
        "    answer.append(grad_output[0])\n",
        "    print(grad_input,grad_output)\n",
        "    pass\n",
        "\n",
        "model.register_full_backward_hook(module_hook)\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x1 = torch.rand(1, requires_grad=True)\n",
        "x2 = torch.rand(1, requires_grad=True)\n",
        "\n",
        "output = model(x1, x2)\n",
        "output.retain_grad()\n",
        "output.backward()\n",
        "\n",
        "if answer == [x1.grad, x2.grad, output.grad]:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([0.4207]), tensor([4.3987])) (tensor([1.]),)\n",
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHkgQGZm35q4"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "module 단위의 backward hook은 정말 좋지만\n",
        "module을 기준으로 input, output gradient 값만 가져와서\n",
        "module 내부의 tensor의 gradient값은 알아낼 수 없어요!\n",
        "\n",
        "그래서 Model의 Parameter W의 gradient값을 알고 싶지만\n",
        "module 단위 backward hook로는 알아낼 수가 없네요!\n",
        "\n",
        "tensor 단위의 backward hook를 사용해야겠어요!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG0_rueTaYj3",
        "outputId": "230a70e2-ba2a-471a-c050-e904c6e8ae5d"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W = Parameter(torch.Tensor([5]))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        output = x1 * x2\n",
        "        output = output * self.W\n",
        "\n",
        "        return output\n",
        "\n",
        "# 모델 생성\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# TODO: Model의 Parameter W의 gradient 값을 저장하세요!\n",
        "answer = []\n",
        "\n",
        "# TODO : hook를 이용해서 W의 gradient 값을 알아내 answer에 저장하세요\n",
        "def tensor_hook(grad):\n",
        "    answer.append(grad)\n",
        "    pass\n",
        "\n",
        "model.W.register_hook(tensor_hook)\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x1 = torch.rand(1, requires_grad=True)\n",
        "x2 = torch.rand(1, requires_grad=True)\n",
        "\n",
        "output = model(x1, x2)\n",
        "output.backward()\n",
        "\n",
        "if answer == [model.W.grad]:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQtbwyLO6Z7p"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "이제 저희는 모델의 어떤 tensor에서도\n",
        "원하는 gradient값을 알아낼 수 있게 되었어요!\n",
        "\n",
        "그런데 backward hook도 gradient값의 흐름에 영향을 미칠 수 있을까요?\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-C4I5fX6AE8",
        "outputId": "348953b4-85f3-4cdb-f28e-b486968ff7bd"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W = Parameter(torch.Tensor([5]))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        output = x1 * x2\n",
        "        output = output * self.W\n",
        "\n",
        "        return output\n",
        "\n",
        "# 모델 생성\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# TODO : hook를 이용해서 module의 gradient 출력의 합이 1이 되도록 하세요!\n",
        "#        ex) (1.5, 0.5) -> (0.75, 0.25)\n",
        "def module_hook(module, grad_input, grad_output):\n",
        "    grad_input = list(grad_input)\n",
        "    sum_input = sum(grad_input)\n",
        "    \n",
        "    grad_input[0] = grad_input[0]/sum_input\n",
        "    \n",
        "    grad_input[1] = grad_input[1]/sum_input\n",
        "    print(grad_input)\n",
        "    pass\n",
        "\n",
        "model.register_full_backward_hook(module_hook)\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x1 = torch.rand(1, requires_grad=True)\n",
        "x2 = torch.rand(1, requires_grad=True)\n",
        "\n",
        "output = model(x1, x2)\n",
        "output.backward()\n",
        "\n",
        "if x1.grad + x2.grad == 1:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([0.9188]), tensor([0.0812])]\n",
            "🦆 다시 도전해봐요!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOY2fP6aR1d8"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "forward hook, backward hook 모두 다루어보았네요!\n",
        "휴! 정말 힘들었어요!\n",
        "\n",
        "그래도 유용한 기능을 하나 알게 된 것 같아 매우 기뻐요!\n",
        "다음의 것들을 해볼 수 있을 것 같아요!\n",
        "\n",
        "- gradient값의 변화를 시각화\n",
        "- gradient값이 특정 임계값을 넘으면 gradient exploding 경고 알림\n",
        "- 특정 tensor의 gradient값이 너무 커지거나 작아지는 현상이 관측되면\n",
        "  해당 tensor 한정으로 gradient clipping\n",
        "\n",
        "아! 그러고보니 forward hook을 실습하고 읽다 만 문서가 생각나네요!\n",
        "이제 마저 backward hook 관련 내용을 읽어야겠네요!\n",
        "\n",
        "동영상도 하나 발견했는데 hook에 대해서 상세히 잘 설명해주는 것 같아요!\n",
        "hook의 동작 원리를 좀 더 자세히 알고싶을 때 한 번 봐야겠어요!\n",
        "```\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [How to Use PyTorch Hooks - Medium](https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904)\n",
        "- [PyTorch 101, Part 5: Understanding Hooks - Paperspace blog](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/)\n",
        "- [PyTorch Hooks Explained - In-depth Tutorial - YouTube](https://www.youtube.com/watch?v=syLFCVYua6Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz48RlN1_6t_"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> apply\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "이건 제가 설명드릴 수 있을 것 같아요! 공부를 열심히 했죠!\n",
        "\n",
        "우리는 PyTorch의 nn.Module은 상자라는 것을 같이 배웠어요!\n",
        "그래서 module은 module를 포함할 수 있고 다른 module 속에 들어갈 수도 있죠!\n",
        "\n",
        "하나의 module에 다른 모든 module들이 담기면\n",
        "우리는 이 거대한 module들의 집합을 모델이라고 부르죠!\n",
        "\n",
        "모델은 수많은 module과 module들이 서로 복잡하게 얽혀있는\n",
        "트리(Tree) 혹은 그래프(Graph)라고 볼 수 있어요!\n",
        "\n",
        "모델에 무언가를 적용하면 단지 맨 꼭대기의 module 하나가 아니라\n",
        "모델을 구성하는 전체 module에 모두 적용이 되어야 하고\n",
        "nn.Module의 method들은 대부분 내부적으로 이를 지원해요!\n",
        "\n",
        "예로 \".cpu()\"를 맨 위 module에 적용하면 우리는 신경쓰지 않아도\n",
        "module이 그 아래에 존재하는 모든 module에 \".cpu()\"를 적용해요!\n",
        "\n",
        "그러면 nn.Module에 이미 구현되어있는 method가 아닌\n",
        "저희만의 custom 함수를 모델에 적용하고 싶다면 어떻게 하면 좋을까요?\n",
        "모델에 속하는 모든 module에 일일이 함수를 적용해야할까요?\n",
        "\n",
        "이때 사용하는게 바로 \"apply\"에요!\n",
        "함수를 적용한다는 문구가 잘 와닿지 않으시죠? 함께 사용해봐요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [apply - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=apply#torch.nn.Module.apply)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kOCk33KLR8i"
      },
      "source": [
        "##### 💡 apply 예제\n",
        "> 🦆 부덕이가 코드를 작성해주었어요\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "처음 보는 기능은 Documentation의 예제를 사용해보면 이해가 되더라구요!\n",
        "apply 함수에 적혀있는 예제를 그대로 가져와봤어요!\n",
        "```\n",
        "- [apply - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=apply#torch.nn.Module.apply)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7tUi0q0CVap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8e8509-1f30-4aed-ba43-12ebe476f24e"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "@torch.no_grad()\n",
        "def init_weights(m):\n",
        "    print(type(m))\n",
        "    print(\"1\")\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.fill_(1.0)\n",
        "        print(m.weight)\n",
        "\n",
        "net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
        "net.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.nn.modules.linear.Linear'>\n",
            "1\n",
            "Parameter containing:\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "<class 'torch.nn.modules.linear.Linear'>\n",
            "1\n",
            "Parameter containing:\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "<class 'torch.nn.modules.container.Sequential'>\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdSFfplQPgg9"
      },
      "source": [
        "```python\n",
        "🦆\n",
        "아하! 그러니까 apply를 통해 적용하는 함수는 module을 입력으로 받는군요!\n",
        "모델의 모든 module들을 순차적으로 입력받아서 처리하는 것 같아요!\n",
        "\n",
        "apply 함수는 일반적으로 가중치 초기화(Weight Initialization)에 많이 사용된다고 해요!\n",
        "Parameter로 지정한 tensor의 값을 원하는 값으로 지정해주는 것을 의미하는 것 같아요!\n",
        "하지만 저도 아직은 생소해서 정확한지 모르겠네요!\n",
        "\"m.weight.fill_\"과 같은 코드를 보니까 갑자기 두통이..\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7RtvqiWNGen"
      },
      "source": [
        "##### 💡 부덕이 모델 apply - Module 출력해보기\n",
        "> 🦆 부덕이가 코드를 작성해주었어요\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "예제만으로 무언가 이해가 안된 찝찝한 기분이 들어요!\n",
        "예전에 만들어놓은 모델을 다시 가져와서 적용해봐야겠어요!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBfaXAWNN6-C"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "# 하지만 아래 과제를 진행하기 전에 아래 코드를 보면서 최대한 이해해보세요!\n",
        "\n",
        "# Function\n",
        "class Function_A(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.W\n",
        "\n",
        "class Function_B(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x - self.W\n",
        "\n",
        "class Function_C(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.W\n",
        "\n",
        "class Function_D(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x / self.W\n",
        "\n",
        "\n",
        "# Layer\n",
        "class Layer_AB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.a = Function_A('plus')\n",
        "        self.b = Function_B('substract')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.a(x)\n",
        "        x = self.b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Layer_CD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = Function_C('multiply')\n",
        "        self.d = Function_D('divide')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c(x)\n",
        "        x = self.d(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ab = Layer_AB()\n",
        "        self.cd = Layer_CD()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ab(x)\n",
        "        x = self.cd(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "model = Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-3cdt8TR3_v",
        "outputId": "7728ec33-5a4d-404b-a75a-314fcbee2725"
      },
      "source": [
        "def print_module(module):\n",
        "    print(module)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# 🦆 apply는 apply가 적용된 module을 return 해줘요!\n",
        "returned_module = model.apply(print_module)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function_A(name=plus)\n",
            "------------------------------\n",
            "Function_B(name=substract)\n",
            "------------------------------\n",
            "Layer_AB(\n",
            "  (a): Function_A(name=plus)\n",
            "  (b): Function_B(name=substract)\n",
            ")\n",
            "------------------------------\n",
            "Function_C(name=multiply)\n",
            "------------------------------\n",
            "Function_D(name=divide)\n",
            "------------------------------\n",
            "Layer_CD(\n",
            "  (c): Function_C(name=multiply)\n",
            "  (d): Function_D(name=divide)\n",
            ")\n",
            "------------------------------\n",
            "Model(\n",
            "  (ab): Layer_AB(\n",
            "    (a): Function_A(name=plus)\n",
            "    (b): Function_B(name=substract)\n",
            "  )\n",
            "  (cd): Layer_CD(\n",
            "    (c): Function_C(name=multiply)\n",
            "    (d): Function_D(name=divide)\n",
            "  )\n",
            ")\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oFU67qIS-KA"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "아하! apply는 Postorder Traversal 방식으로 module들에 함수를 적용하네요!\n",
        "이제 이해가 된 것 같아요!\n",
        "```\n",
        "\n",
        "- [4 Types of Tree Traversal Algorithms - Towards Data Science](https://towardsdatascience.com/4-types-of-tree-traversal-algorithms-d56328450846)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX9s5bAEZR81"
      },
      "source": [
        "##### 💡 부덕이 모델 apply - 가중치 초기화\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "가중치 초기화(Weight Initialization)이라는 용어가 낯설지만\n",
        "역시 Parameter의 값을 초기화하는게 맞았어요!\n",
        "\n",
        "제가 만든 모델에 총 4개의 Parameter가 있는데 모든 값을 1로 초기화해봐요!\n",
        "```\n",
        "\n",
        "🎁 **힌트** 🎁\n",
        "- [How to initialize weights in PyTorch? - Stack Overflow](https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJHv20B8ZR9E"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "# 실행만 시켜주시고 다음 셀로 넘어가주세요!\n",
        "\n",
        "# Function\n",
        "class Function_A(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.W\n",
        "\n",
        "class Function_B(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x - self.W\n",
        "\n",
        "class Function_C(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.W\n",
        "\n",
        "class Function_D(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x / self.W\n",
        "\n",
        "\n",
        "# Layer\n",
        "class Layer_AB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.a = Function_A('plus')\n",
        "        self.b = Function_B('substract')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.a(x)\n",
        "        x = self.b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Layer_CD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = Function_C('multiply')\n",
        "        self.d = Function_D('divide')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c(x)\n",
        "        x = self.d(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ab = Layer_AB()\n",
        "        self.cd = Layer_CD()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ab(x)\n",
        "        x = self.cd(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jifkSJbZR9F",
        "outputId": "d363683f-b2fe-4072-b4c2-0f878e0af99f"
      },
      "source": [
        "model = Model()\n",
        "\n",
        "# TODO : apply를 이용해 모든 Parameter 값을 1로 만들어보세요!\n",
        "@torch.no_grad()\n",
        "def weight_initialization(module):\n",
        "    module_name = module.__class__.__name__\n",
        "    search = 'Function_'\n",
        "    print(module_name)\n",
        "    if search in module_name:\n",
        "        module.W.fill_(1.)\n",
        "\n",
        "# 🦆 apply는 apply가 적용된 module을 return 해줘요!\n",
        "returned_module = model.apply(weight_initialization)\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.rand(1)\n",
        "\n",
        "output = model(x)\n",
        "\n",
        "if torch.isclose(output, x):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function_A\n",
            "Function_B\n",
            "Layer_AB\n",
            "Function_C\n",
            "Function_D\n",
            "Layer_CD\n",
            "Model\n",
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXUDj5guCimd"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "Pretrained 모델을 가져다가 사용할 때 원하는 Parameter에다가\n",
        "backward hook를 추가하는 것도 가능할 것 같아요!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Aa_VhGg5ds"
      },
      "source": [
        "##### 💡 <font color='yellow'><b>[ Optional ]</b></font> 🔥 부덕이 모델 apply - repr 수정하기 🔥\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "현재 모델을 출력해보면 다음과 같이 나와요!\n",
        "\n",
        "❌ 실제 출력 결과\n",
        "Model(\n",
        "  (ab): Layer_AB(\n",
        "    (a): Function_A()\n",
        "    (b): Function_B()\n",
        "  )\n",
        "  (cd): Layer_CD(\n",
        "    (c): Function_C()\n",
        "    (d): Function_D()\n",
        "  )\n",
        ")\n",
        "\n",
        "이걸 다음처럼 출력되게 만들고 싶어요!\n",
        "\n",
        "✅ 부덕이가 원하는 이상적인 출력\n",
        "Model(\n",
        "  (ab): Layer_AB(\n",
        "    (a): Function_A(name=plus)\n",
        "    (b): Function_B(name=substract)\n",
        "  )\n",
        "  (cd): Layer_CD(\n",
        "    (c): Function_C(name=multiply)\n",
        "    (d): Function_D(name=divide)\n",
        "  )\n",
        ")\n",
        "\n",
        "apply를 이용해서 repr 출력 메세지를 수정해봐요!\n",
        "```\n",
        "\n",
        "🎁 **힌트** 🎁\n",
        "- [Any elegant way to add a method to an existing object in python? - Stack Overflow](https://stackoverflow.com/questions/30294458/any-elegant-way-to-add-a-method-to-an-existing-object-in-python/30294947)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "UR1C9k7-g-my"
      },
      "source": [
        "#@title 부덕이 모델\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "# 실행만 시켜주시고 다음 셀로 넘어가주세요!\n",
        "\n",
        "# Function\n",
        "class Function_A(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.W\n",
        "\n",
        "class Function_B(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x - self.W\n",
        "\n",
        "class Function_C(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.W\n",
        "\n",
        "class Function_D(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x / self.W\n",
        "\n",
        "\n",
        "# Layer\n",
        "class Layer_AB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.a = Function_A('plus')\n",
        "        self.b = Function_B('substract')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.a(x)\n",
        "        x = self.b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Layer_CD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = Function_C('multiply')\n",
        "        self.d = Function_D('divide')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c(x)\n",
        "        x = self.d(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ab = Layer_AB()\n",
        "        self.cd = Layer_CD()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ab(x)\n",
        "        x = self.cd(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOq-Hv_ruX-y",
        "outputId": "141a63dc-52c4-4ec7-fbca-d7190651616b"
      },
      "source": [
        "model = Model()\n",
        "\n",
        "# TODO : apply를 이용해서 부덕이가 원하는대로 repr 출력을 수정해주세요!\n",
        "from functools import partial\n",
        "\n",
        "def function_repr(self):\n",
        "    return f'name={self.name}'\n",
        "\n",
        "def add_repr(module):\n",
        "    module_name = module.__class__.__name__\n",
        "    search = \"Function_\"\n",
        "    # print(module)\n",
        "    if search in module_name:\n",
        "        def new_repr(self):\n",
        "            return 'name={}'.format(self.name)\n",
        "        module.extra_repr = partial(new_repr,module)\n",
        "\n",
        "\n",
        "\n",
        "# 🦆 apply는 apply가 적용된 module을 return 해줘요!\n",
        "returned_module = model.apply(add_repr)\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "model_repr = repr(model)\n",
        "\n",
        "print(\"모델 출력 결과\")\n",
        "print(\"-\" * 30)\n",
        "print(model_repr)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "answer = \"Model(\\n  (ab): Layer_AB(\\n    (a): Function_A(name=plus)\\n    (b): Function_B(name=substract)\\n  )\\n  (cd): Layer_CD(\\n    (c): Function_C(name=multiply)\\n    (d): Function_D(name=divide)\\n  )\\n)\"\n",
        "\n",
        "if model_repr == answer:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "    print(\"🦆 너무 고마워요 꽉꽉!\")\n",
        "else:\n",
        "    print(\"🦆 다시 도전해봐요!\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "모델 출력 결과\n",
            "------------------------------\n",
            "Model(\n",
            "  (ab): Layer_AB(\n",
            "    (a): Function_A(name=plus)\n",
            "    (b): Function_B(name=substract)\n",
            "  )\n",
            "  (cd): Layer_CD(\n",
            "    (c): Function_C(name=multiply)\n",
            "    (d): Function_D(name=divide)\n",
            "  )\n",
            ")\n",
            "------------------------------\n",
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n",
            "🦆 너무 고마워요 꽉꽉!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYMJgAEQZR9G"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "언제든지 원하는 method를 모델에 원하는 module에 추가할 수 있다니!\n",
        "무언가 응용할 수 있는 곳이 많을 것 같아요!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dim6OGlSD4Hk"
      },
      "source": [
        "##### 💡 <font color='yellow'><b>[ Optional ]</b></font> 🔥🔥🔥 부덕이 모델 apply - Function 수정하기 (흑마법편) 🔥🔥🔥\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "친구가 제 모델의 코드를 보더니 문제를 하나 내줬어요!\n",
        "\n",
        "현재 4개의 Function A, B, C, D가 있어요!\n",
        "\n",
        "- A : x + W\n",
        "- B : x - W\n",
        "- C : x * W\n",
        "- D : x / W\n",
        "\n",
        "이걸 다음처럼 linear transformation처럼 동작하도록 바꿔보래요!\n",
        "\n",
        "- A : x @ W + b\n",
        "- B : x @ W + b\n",
        "- C : x @ W + b\n",
        "- D : x @ W + b\n",
        "\n",
        "W는 이미 각 Function에 생성된 Parameter이고\n",
        "b는 새롭게 만들어야 하는 Parameter에요!\n",
        "\n",
        "연산 수식이 동일할 필요는 없지만 계산 결과는 같아야 한다고 하더라구요!\n",
        "직접 \"nn.Linear\" 모델을 이용해서 제대로 만들었는지 검증한대요!\n",
        "\n",
        "결과 비교를 위해서 W과 b는 모두 1로 값을 초기화한다고 해요!\n",
        "\n",
        "아! 이제 tensor에 담긴 값은 scalar가 아니라\n",
        "2*2크기의 matrix라는 점에 주의하라고 하더라구요!\n",
        "저는 절대로 풀 수 없을거라고 약올리고 갔는데 왠지 모르게 분해요!💢\n",
        "```\n",
        "\n",
        "🎁 **힌트** 🎁\n",
        "- forward hook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POOWtst-zqip"
      },
      "source": [
        "#@title Test 코드\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "def tester(model, friend_model):\n",
        "    x = torch.rand(2, 2, requires_grad=True)\n",
        "\n",
        "    # 우리가 생성한 모델\n",
        "    output = model(x)\n",
        "    output = output.sum()\n",
        "    output.backward()\n",
        "\n",
        "    our_grad = x.grad.clone()\n",
        "    grads = [(name, param.grad) for name, param in model.named_parameters()]\n",
        "\n",
        "    x.grad = None\n",
        "\n",
        "    # 친구가 생성한 모델\n",
        "    friend_output = friend_model(x)\n",
        "    friend_output = friend_output.sum()\n",
        "    friend_output.backward()\n",
        "\n",
        "    friend_grad = x.grad.clone()\n",
        "    friend_grads = [(name, param.grad) for name, param in friend_model.named_parameters()]\n",
        "\n",
        "    # 총 결과\n",
        "    total_result = 0\n",
        "\n",
        "    # Parameter 갯수 비교\n",
        "    if len(grads) == len(friend_grads):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m 두 모델이 같은 Parameter 갯수를 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m 두 모델이 다른 Parameter 갯수를 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 Parameter 갯수 : {len(grads)} 🐦 친구 모델 Parameter 갯수 : {len(friend_grads)}\")\n",
        "        return\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Parameter 이름 체크\n",
        "    params = []\n",
        "    for grad in grads:\n",
        "        param = ''.join(grad[0].split('.')[1:])\n",
        "        params.append(param)\n",
        "\n",
        "    if 'ab' in params:\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_A에 Parameter b를 만드셨네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Function_A에 Parameter b가 없어요!\")\n",
        "        return\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if 'bb' in params:\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_B에 Parameter b를 만드셨네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Function_B에 Parameter b가 없어요!\")\n",
        "        return\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if 'cb' in params:\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_C에 Parameter b를 만드셨네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Function_C에 Parameter b가 없어요!\")\n",
        "        return\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if 'db' in params:\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_D에 Parameter b를 만드셨네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Function_D에 Parameter b가 없어요!\")\n",
        "        return\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Parameter 초기화 체크\n",
        "    if torch.all(torch.stack([torch.all(param == 1) for param in model.parameters()])):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Parameter W, b를 모두 1로 초기화시키셨네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Parameter W, b를 모두 1로 초기화시키세요!\")\n",
        "        return\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 모델 출력값 체크\n",
        "    if torch.isclose(output, friend_output):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m 두 모델이 동일한 출력값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m 두 모델이 다른 출력값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 출력값 : {output:.2f} 🐦 친구 모델 출력값 : {friend_output:.2f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 입력값 x gradient 체크\n",
        "    if torch.all(torch.isclose(our_grad, friend_grad)):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m 입력에 사용된 x가 동일한 Gradient 값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m 입력에 사용된 x가 다른 Gradient 값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 x grad 값\\n{our_grad}\\n🐦 친구 모델 x grad 값\\n{friend_grad}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Function A gradient 체크\n",
        "    if torch.all(torch.isclose(grads[0][1], friend_grads[0][1])):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_A Parameter W가 동일한 Gradient 값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Parameter W가 다른 Gradient 값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 Function_A Parameter W grad 값\\n{grads[0][1]}\\n🐦 친구 모델 nn.Linear Parameter W grad 값\\n{friend_grads[0][1]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if torch.all(torch.isclose(grads[1][1], friend_grads[1][1])):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_A Parameter b가 동일한 Gradient 값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Parameter b가 다른 Gradient 값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 Function_A Parameter b grad 값\\n{grads[1][1]}\\n🐦 친구 모델 nn.Linear Parameter b grad 값\\n{friend_grads[1][1]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Function B gradient 체크\n",
        "    if torch.all(torch.isclose(grads[2][1], friend_grads[2][1])):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_B Parameter W가 동일한 Gradient 값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Parameter W가 다른 Gradient 값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 Function_B Parameter W grad 값\\n{grads[2][1]}\\n🐦 친구 모델 nn.Linear Parameter W grad 값\\n{friend_grads[2][1]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if torch.all(torch.isclose(grads[3][1], friend_grads[3][1])):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_B Parameter b가 동일한 Gradient 값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Parameter b가 다른 Gradient 값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 Function_B Parameter b grad 값\\n{grads[3][1]}\\n🐦 친구 모델 nn.Linear Parameter b grad 값\\n{friend_grads[3][1]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Function C gradient 체크\n",
        "    if torch.all(torch.isclose(grads[4][1], friend_grads[4][1])):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_C Parameter W가 동일한 Gradient 값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Parameter W가 다른 Gradient 값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 Function_C Parameter W grad 값\\n{grads[4][1]}\\n🐦 친구 모델 nn.Linear Parameter W grad 값\\n{friend_grads[4][1]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if torch.all(torch.isclose(grads[5][1], friend_grads[5][1])):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_C Parameter b가 동일한 Gradient 값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Parameter b가 다른 Gradient 값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 Function_C Parameter b grad 값\\n{grads[5][1]}\\n🐦 친구 모델 nn.Linear Parameter b grad 값\\n{friend_grads[5][1]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Function D gradient 체크\n",
        "    if torch.all(torch.isclose(grads[6][1], friend_grads[6][1])):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_D Parameter W가 동일한 Gradient 값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Parameter W가 다른 Gradient 값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 Function_D Parameter W grad 값\\n{grads[6][1]}\\n🐦 친구 모델 nn.Linear Parameter W grad 값\\n{friend_grads[6][1]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if torch.all(torch.isclose(grads[7][1], friend_grads[7][1])):\n",
        "        print(\"\\x1b[32m[PASS]\\x1b[0m Function_D Parameter b가 동일한 Gradient 값을 가지네요!\")\n",
        "        total_result += 1\n",
        "    else:\n",
        "        print(\"\\x1b[31m[FAIL]\\x1b[0m Parameter b가 다른 Gradient 값을 가지네요!\")\n",
        "        print(f\"🦆 우리 모델 Function_D Parameter b grad 값\\n{grads[7][1]}\\n🐦 친구 모델 nn.Linear Parameter b grad 값\\n{friend_grads[7][1]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    if total_result == 16:\n",
        "        print(f\"\\x1b[32m[ALL PASS {total_result}/16]\\x1b[0m 🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "    else:\n",
        "        print(f\"\\x1b[31m[FAIL {total_result}/16]\\x1b[0m 🦆 다시 도전해봐요!\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fvtuqHDD4H2"
      },
      "source": [
        "#@title 부덕이 모델\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "# 실행만 시켜주시고 다음 셀로 넘어가주세요!\n",
        "\n",
        "# Function\n",
        "class Function_A(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(2, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.W\n",
        "\n",
        "class Function_B(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(2, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x - self.W\n",
        "\n",
        "class Function_C(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(2, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.W\n",
        "\n",
        "class Function_D(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.W = Parameter(torch.rand(2, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x / self.W\n",
        "\n",
        "\n",
        "# Layer\n",
        "class Layer_AB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.a = Function_A('plus')\n",
        "        self.b = Function_B('substract')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.a(x)\n",
        "        x = self.b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Layer_CD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = Function_C('multiply')\n",
        "        self.d = Function_D('divide')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c(x)\n",
        "        x = self.d(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ab = Layer_AB()\n",
        "        self.cd = Layer_CD()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ab(x)\n",
        "        x = self.cd(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE_Tl4H_kSp3",
        "outputId": "0a49568d-6b88-49c8-cf96-86a1875c72de"
      },
      "source": [
        "\n",
        "model = Model()\n",
        "\n",
        "\n",
        "\n",
        "def add_method(obj, func):\n",
        "\n",
        "    setattr(obj, func.__name__, partial(func, obj))\n",
        "\n",
        "\n",
        "\n",
        "# TODO : apply를 이용해 Parameter b를 추가해보세요!\n",
        "def add_bias(module):\n",
        "    module_name = module.__class__.__name__\n",
        "    search = 'Function_'\n",
        "    if search in module_name:\n",
        "        module.register_parameter(name='b',param=torch.nn.Parameter(torch.rand(1,2)))\n",
        "\n",
        "\n",
        "# TODO : apply를 이용해 추가된 b도 값을 1로 초기화해주세요!\n",
        "def weight_initialization(module):\n",
        "    module_name = module.__class__.__name__\n",
        "\n",
        "    if module_name.split('_')[0] == \"Function\":\n",
        "        module.W.data.fill_(1.)\n",
        "        module.b.data.fill_(1.)\n",
        "\n",
        "\n",
        "# TODO : apply를 이용해 모든 Function을 linear transformation으로 바꿔보세요!\n",
        "#        X @ W + b\n",
        "def linear_transformation(module):\n",
        "    module_name = module.__class__.__name__\n",
        "\n",
        "    if module_name.split('_')[0] == \"Function\":\n",
        "        module.register_forward_hook(lambda x,y,z : y[0] @ x.W.T + x.b)\n",
        "       \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "returned_module = model.apply(add_bias)\n",
        "returned_module = model.apply(weight_initialization)\n",
        "returned_module = model.apply(linear_transformation)\n",
        "\n",
        "\n",
        "print(model.get_parameter('ab.a.W'))\n",
        "\n",
        "# 🦆 친구가 비교를 위해서 작성해놓은 코드에요!\n",
        "class FriendLinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() \n",
        "        self.linear = nn.Sequential(nn.Linear(2, 2),\n",
        "                                    nn.Linear(2, 2),\n",
        "                                    nn.Linear(2, 2),\n",
        "                                    nn.Linear(2, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "def friends_init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.fill_(1.0)\n",
        "        m.bias.data.fill_(1.0)\n",
        "\n",
        "friend_model = FriendLinearModel()\n",
        "friend_model.apply(friends_init_weights)\n",
        "# print(friend_model.state_dict())\n",
        "# print(model.state_dict())\n",
        "# 🦆 체크해보세요!\n",
        "\n",
        "grads = tester(model, friend_model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "\u001b[32m[PASS]\u001b[0m 두 모델이 같은 Parameter 갯수를 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_A에 Parameter b를 만드셨네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_B에 Parameter b를 만드셨네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_C에 Parameter b를 만드셨네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_D에 Parameter b를 만드셨네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Parameter W, b를 모두 1로 초기화시키셨네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m 두 모델이 동일한 출력값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m 입력에 사용된 x가 동일한 Gradient 값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_A Parameter W가 동일한 Gradient 값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_A Parameter b가 동일한 Gradient 값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_B Parameter W가 동일한 Gradient 값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_B Parameter b가 동일한 Gradient 값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_C Parameter W가 동일한 Gradient 값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_C Parameter b가 동일한 Gradient 값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_D Parameter W가 동일한 Gradient 값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[PASS]\u001b[0m Function_D Parameter b가 동일한 Gradient 값을 가지네요!\n",
            "--------------------------------------------------\n",
            "\u001b[32m[ALL PASS 16/16]\u001b[0m 🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_Ka8OvED4H4"
      },
      "source": [
        "```python\n",
        "🐦\n",
        "아.. 아니 이걸 해내다니!!!!!\n",
        "```\n",
        "``` python\n",
        "🦆\n",
        "마음 먹으면 이쯤이야!\n",
        "\n",
        "휴 정말 어려웠어요! 그런데 정말 놀랐어요!\n",
        "이미 만들어진 모델을 코드를 전혀 건들이지 않고 새로운 모델로 탈바꿈시키다니!\n",
        "심지어 PyTorch의 \"nn.Linear\"모델과 완벽히 동일하게 동작하고 있어요!\n",
        "forward, backward 모두 말이죠!\n",
        "\n",
        "apply를 써야지만 할 수 있는 것은 아니었지만 apply를 사용해서 하니까\n",
        "함수가 깔끔하게 정리되어서 가독성이 증가한 느낌이에요!\n",
        "\n",
        "그런데 이걸 대체 어디다 쓰는거죠?\n",
        "```\n",
        "```python\n",
        "🐦\n",
        "반가워요! 제가 바로 부덕이 친구 부앵이에요!\n",
        "\n",
        "이 과제를 푸는데 큰 도움을 주었다고 들었어요!\n",
        "역시 부덕이가 이걸 혼자 해낼리가 없죠!\n",
        "\n",
        "앞으로 PyTorch를 사용하면서 Pretrained된 모델을 많이 사용하시게 될텐데\n",
        "모델 자체에 버그가 있거나, 혹은 수정해서 써야만 하는 등의 사항들이 발생할 수 있어요.\n",
        "이런 경우 오늘 여기에서 연습하신 모델을 수정하는 훈련이 큰 도움이 될 거예요!\n",
        "\n",
        "솔직히 풀 줄은 몰랐는데 정말 놀랐어요! 찬사를 보냅니다!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI-eQLReB_b0"
      },
      "source": [
        "### 🎉🎉🎉 nn.Module 완료! 🎉🎉🎉\n",
        "\n",
        "```python\n",
        "🦆\n",
        "여기까지 오다니 정말 믿기지 않아요!\n",
        "PyTorch에 대해서 아무것도 몰랐던 제가 이제는 원하는 모델을 만들 수 있게 되었어요!\n",
        "아직 배울 것이 많다는 것은 알고 있지만 그래도 너무 기뻐요!\n",
        "```\n",
        "\n",
        "Custom 모델을 만들기 위해 \n",
        "적지 않은 분량이었음에도 무사히 마무리 지으신 것을 정말 축하드립니다! 🎉<br>\n",
        "이번 챕터는 Documentation에 비할 바 없이 높은 난이도를 가지고 있었습니다.<br>\n",
        "하지만 모두 이겨내시고 여기까지 오셨군요! 이건 정말 대단한 일입니다.\n",
        "\n",
        "많이 지치셨을 겁니다! 하지만 안심하세요! 여기까지 온 이상 다 끝난것과 같습니다<br>\n",
        "Github과 관련된 내용이 나오지만 쉬어가는 코너에 가깝습니다.<br>\n",
        "시간도 얼마 안걸리고 내용도 짧으니 이제는 긴장을 푸셔도 좋습니다!<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsjw07-c6Vgf"
      },
      "source": [
        "## 🚀 <font color='yellow'><b>[ Optional ]</b></font> Custom 모델 제작을 위한 Github 참조\n",
        "\n",
        "```\n",
        "💡 다른 사람들은 어떻게 custom 모델을 만들고 있을까요?\n",
        "   우리는 Github을 방문하여 그 답을 알아보는 시간을 가질 것입니다.\n",
        "```\n",
        "\n",
        "# 🌍🐣\n",
        "Custom 모델을 제작하기 위해 필요한 준비를 마쳤습니다!<br>\n",
        "비록 쉽지 않은 여정이었지만 여러분은 해냈고 이제 여행을 떠날 준비가 되었습니다.<br>\n",
        "지구를 떠날 시간입니다.\n",
        "\n",
        "# ⭐🐤\n",
        "세상에는 다양하고 복잡한 모델들이 있습니다. 여기에서 배운 지식들로는 차마 다 이해하지 못할 거대하고 복잡한 현란한 모델들이 우주의 별만큼 많지는 않지만 아무튼 많습니다. 여기서 배운 지식은 많은 지식은 아니지만 이제 스스로 서서 그 모델들을 마주할 정도는 됩니다. 부족한 부분이 있다면 채워가면서 어려워서 물러서고 싶다면 버티면서 이제 나아갈 시간입니다. 빛나는 별을 향해 말이죠! \n",
        "\n",
        "# 🚀\n",
        "그럼 이제 별들을 창조한 외계의 존재들을 만나러 가볼까요?\n",
        "\n",
        "- 🛸 Github 모델 찾기\n",
        "- 🛸 Github 모델 라이센스 체크\n",
        "- 🛸 Github Repository 탐색\n",
        "- 🛸 Github 모델 인용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hmoOk31nohg"
      },
      "source": [
        "### 🛸 Github 모델 찾기\n",
        "> 원하는 Github 모델을 찾는 몇 가지 방법에 대해서 가볍게 살펴보는 시간입니다.\n",
        "\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 구글 고급 검색 (Google Advanced Search)\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 깃헙 고급 검색 (Github Advanced Search)\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 모델 큐레이션 사이트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhQbpvUgaT45"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 구글 고급 검색 (Google Advanced Search)\n",
        "``` python\n",
        "🦆\n",
        "이번 장을 오시자마자 떠올리셨을 것 같아요!\n",
        "현 시대의 필수 검색 도구 구글(Google)이죠!\n",
        "\n",
        "하지만 구글이 단지 입력한 단어만 검색하는게 아니라\n",
        "다양한 검색 옵션이 있다는 것을 알고 계신가요?\n",
        "\n",
        "구글 검색도 하나의 프로그램이에요!\n",
        "잘 만들어진 프로그램에 옵션이 없을리가 없죠!\n",
        "여러분들이 사용하는 구글 검색창은\n",
        "쉘로 따지자면 명령어를 입력받는 입력칸인 셈이에요!\n",
        "\n",
        "하지만 저희는 일반적으로 단어만 검색하고 끝나기에\n",
        "구글 검색 프로그램이 어떤 옵션이 있는지 잘 몰라요!\n",
        "이럴 때 누구나 쉽게 사용할 수 있는게 바로\n",
        "구글 고급 검색(Google Advanced Search)이에요!\n",
        "\n",
        "아래 링크를 타고 들어가서 한번 사용해보세요!\n",
        "원하는 항목을 기입하면 이를 구글 검색창에 입력할 수 있는\n",
        "적합한 Query로 바꾸어서 검색을 대신 해줘요!\n",
        "```\n",
        "\n",
        "- [Google Advanced Search](https://www.google.com/advanced_search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0IQvtE8aUEY"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 깃헙 고급 검색 (Github Advanced Search)\n",
        "``` python\n",
        "🦆\n",
        "Github도 고급 검색이 가능하다는 사실을 아시나요?\n",
        "구글처럼 여러 옵션을 제공하고 있으니 한번 살펴봐요!\n",
        "```\n",
        "\n",
        "- [Github Advanced Search](https://github.com/search/advanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb_DEzPpaRc9"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 모델 큐레이션 사이트\n",
        "``` python\n",
        "🦆\n",
        "특정 모델을 찾는 것이 아니라 전반적인 모델의 트렌드를 살펴보고자 하면\n",
        "큐레이션 사이트를 방문하는 것도 하나의 방법이에요!\n",
        "```\n",
        "\n",
        "- [Browse State-of-the-Art - Papers With Code](https://paperswithcode.com/sota)\n",
        "- [labml.ai Annotated PyTorch Paper Implementations - labml.ai](https://nn.labml.ai/)\n",
        "- [awesome-deeplearning-resources - endymecy](https://endymecy.github.io/awesome-deeplearning-resources/awesome_projects.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aowwiHAz3I2c"
      },
      "source": [
        "### 🛸 Github 모델 라이센스 체크\n",
        "> Github 모델을 찾으면 먼저 라이센스를 확인해야 합니다. 원작자가 이 모델을 자유롭게 쓸 수 있게 허용해줬는지, 아니면 코드를 가져다 쓰는데 제약을 걸어두었는지 체크해야 후에 생길 법적 문제를 방지할 수 있습니다. \n",
        "\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 오픈소스 라이센스 종류\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> Github 모델 라이센스 체크\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 라이센스가 표시되지 않은 모델을 사용해도 될까?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGO21TZ0N_Iu"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 오픈소스 라이센스 종류\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "Github에서 찾은 모델의 라이센스를 확인하기 전에\n",
        "오픈소스 라이센스에 먼저 어떤 종류들이 있는지 아는게 중요해요!\n",
        "\n",
        "아래 사이트에 잘 정리되어 있으니까 함께봐요!\n",
        "```\n",
        "\n",
        "- [Choosing the right license - Github Docs](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/licensing-a-repository#choosing-the-right-license)\n",
        "- [Choose an open source license - Choose AI License](https://choosealicense.com/)\n",
        "- [오픈소스를 사용하고 준비하는 개발자를 위한 가이드 - if(kakao) dev 2018](https://tv.kakao.com/channel/3150758/cliplink/391717603)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2IFscSgPqj0"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> Github 모델 라이센스 체크\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "Hugging Face는 자연어 처리에서 정말 유명한 라이브러리죠!\n",
        "transformers 모델의 라이센스를 같이 살펴볼까요?\n",
        "```\n",
        "\n",
        "- [transformers - Hugging Face Github](https://github.com/huggingface/transformers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oOO5qzH2vXq"
      },
      "source": [
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/hugging%20face%20license.png?raw=true)\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/apache%20license%202.0.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tApDeubq27rZ"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "Apache License 2.0을 따르고 있네요!\n",
        "상업적으로 이용이 가능한 자유로운 라이센스에요!\n",
        "하지만 소스 코드를 가져다가 사용하면 license와 copyright을 명시해줘야하고\n",
        "가져다가 사용한 소스코드를 수정했으면 이것 또한 명시해줘야 해요!\n",
        "\n",
        "제약 사항은 있지만 hugging face를 이용해서 상업적 제품을 만들어도\n",
        "코드를 공개하지 않아도 되기 때문에 안심하고 제품을 만드는데 이용하셔도 되요!\n",
        "\n",
        "마침 라이센스 이야기가 나오고 있어서 그런데 위에서처럼\n",
        "Github의 제품들의 스크린샷을 찍는 것은 과연 합법일까요? 불법일까요?\n",
        "\n",
        "궁금하신 분은 아래 screenshot관련 링크를 찾아보세요!\n",
        "```\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [Apache License 2.0 - OLIS](https://www.olis.or.kr/license/Detailselect.do?lId=1002&mapCode=010002)\n",
        "- [Commons:Screenshots - Wikimedia Commons](https://commons.wikimedia.org/wiki/Commons:Screenshots)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Jje0IwRejE"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> 라이센스가 표시되지 않은 모델을 사용해도 될까?\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "Github에 올라온 다양한 모델을 보다보면 라이센스 표시가 없을 때가 있어요!\n",
        "이런 모델의 경우 사용해도 될까요 안될까요?\n",
        "\n",
        "아래 문서를 보면서 같이 알아봐요!\n",
        "```\n",
        "\n",
        "- [No License - Choose AI License](https://choosealicense.com/no-permission/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5EzHAPj3LW8"
      },
      "source": [
        "### 🛸 Github Repository 탐색\n",
        "> Github은 Repository를 탐색할 수 있는 기능을 제공해줍니다. 이를 활용하는 방법을 가볍게 알아보는 시간을 가집니다.\n",
        "\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> Repository 속에 숨어 있는 모델 찾기\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> transformers.py 내부 목차 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSyB0wutN_Oi"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> Repository 속에 숨어 있는 모델 찾기\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "PyTorch 공식 Github 코드 베이스에서 Transformer 모델 코드를 찾으려고 하는데\n",
        "PyTorch 공식 Github 메인 페이지에 들어서자마자 그 방대한 양의 코드에 벌써 현기증이 나네요!\n",
        "\n",
        "옆에서 보다 못한 친구가 Github이 제공해주는 검색 기능을 사용하라고 말해주었어요! \n",
        "```\n",
        "\n",
        "- [PyTorch - Github](https://github.com/pytorch/pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4TqNcE3PZbi"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "Github 검색하는 곳을 찾았어요!\n",
        "```\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/github%20-%20pytorch.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og22SSY0QHxY"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "transformer를 입력하니까 3가지 종류의 검색이 나오네요!\n",
        "\n",
        "- In this repository\n",
        "- In this organization\n",
        "- All Github\n",
        "\n",
        "저희가 원하는 것은 Repository 내부 검색이니까 맨 위에 것을 눌렀어요!\n",
        "```\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/github%20-%20pytorch%20-%20search%20bar.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upJpvsqvQna2"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "PyTorch 공식 코드에서 transformer 검색 결과가 뜨네요!\n",
        "하지만 제가 원하는 것은 c++ 코드로 이루어진 transformer 모델이 아니라\n",
        "python으로 이루어진 transformer 모델이에요!\n",
        "\n",
        "스크롤을 조금 내려볼까요?\n",
        "```\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/github%20-%20pytorch%20-%20transformer.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnRVVIycQ8Cj"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "transformer.py! 찾았어요! 이제 클릭해서 들어가면 될 것 같아요!\n",
        "\n",
        "이렇게 편하다니! 검색창이 정말 좋네요!\n",
        "```\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/github%20-%20pytorch%20-%20transformer%20found.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5k7RfnzRRo9"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "class Transformer! 제가 정확히 원하던 모델이에요!\n",
        "\n",
        "경로를 보니까 pytorch/torch/nn/modules/transformer.py 네요!\n",
        "만약 직접 찾으려고 했다면 꽤 헤맸을 것 같아요!\n",
        "```\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/transformers.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSIGoFyWRoJ2"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> transformers.py 내부 목차 확인\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "transformer.py에 어떤 class와 function이 있는지 확인을 하고 싶을 때\n",
        "일일이 스크롤을 내리면서 확인하기에는 번거롭겠죠?\n",
        "\n",
        "transformers.py 내부의 목차는 어디에서 확인할까요?\n",
        "```\n",
        "\n",
        "- [transformer.py - Github](https://github.com/pytorch/pytorch/blob/35307b131df9d24bfa96103d6061cf14c797ee32/torch/nn/modules/transformer.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq6zcLvVRpXB"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "빨간색 박스의 Jump to를 이용하면 좋아요!\n",
        "```\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/transformer%20-%20jump%20to.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZziNxuyzSNKE"
      },
      "source": [
        "``` python\n",
        "🦆\n",
        "각 class와 function이 순서대로 나와있는 것이 보이시죠?\n",
        "복잡한 코드를 스크롤과 함께 탐험하지 않더라도 목차를 볼 수 있어요!\n",
        "```\n",
        "\n",
        "![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/transformers%20-%20insider%20search.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7iF0gyR3dZa"
      },
      "source": [
        "### 🛸 Github 모델 인용\n",
        "> Github에서 레퍼런스(Reference) 모델을 성공적으로 찾은 이후 pip install, 복사 붙여넣기, 혹은 git clone 등 자신만의 방법으로 모델 코드를 가져왔을 것입니다. 일반적으로 우리는 링크를 남기거나 그냥 넘기는 경우가 있습니다. 하지만 원칙적으로 정확한 인용은 원작자에 대한 예의이기 때문에 인용(cite)을 정확하게 하는 방법을 가볍게 알아보는 시간을 가집니다.\n",
        "\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> Citation이 제공될 경우\n",
        "- 📖 <font color='gold' ><b>[ 읽기 ]</b></font> Citation이 제공되지 않을 경우"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxAcbyy-axop"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> Citation이 제공될 경우\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "PyTorch나 Hugging Face와 같이 유명하고 공식적인\n",
        "Github 프로젝트들은 보통 Citation을 제공해줘요!\n",
        "\n",
        "하지만 Reference 모델이 속해있는\n",
        "각 Github 페이지별로 Citation을 제공하지는 않아요!\n",
        "\n",
        "그래서 여기서는 Github 모델 인용을 Github 모델이 속해있는\n",
        "Repository 인용으로 대신하는 것에 주의해주세요!\n",
        "```\n",
        "```python\n",
        "🐦 부앵이에요! Github Repository Citation 예시에요!\n",
        "```\n",
        "- [PyTorch/CITATION - Github](https://github.com/pytorch/pytorch/blob/master/CITATION)\n",
        "- [Huggingface/Transformers/citation - Github](https://github.com/huggingface/transformers#citation)\n",
        "\n",
        "```python\n",
        "🐦 대부분은 BibTex 형식을 따르기 때문에 아래에서 변환해서 사용하세요!\n",
        "```\n",
        "- [BibTeX Online Converter](https://bibtex.online/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY5Rm509gNHe"
      },
      "source": [
        "#### 📖 <font color='gold' ><b>[ 읽기 ]</b></font> Citation이 제공되지 않을 경우\n",
        "\n",
        "``` python\n",
        "🦆\n",
        "직접 인용문을 만들어내는 것은 언제나 까다로운 일이죠!\n",
        "인용 방법도 제각각이고 그 방법에 맞추는 것도 번거로워요!\n",
        "그래서 일반적으로 다들 대충 인용하고 넘기는 경우가 많아요!\n",
        "\n",
        "하지만 적어도 여기서는 제대로 인용하는 방법에 대해서 알아봐요!\n",
        "```\n",
        "```python\n",
        "🐦 다시 부앵이에요! Citation을 직접 작성하려면 아래 글을 추천드려요!\n",
        "```\n",
        "\n",
        "- [How to Cite a GitHub Repository - Wiki How](https://www.wikihow.com/Cite-a-GitHub-Repository)\n",
        "\n",
        "```python\n",
        "🐦 자동 완성 Citation을 원한다면 다음 사이트에서 하시면 되요!\n",
        "```\n",
        "- [Free Harvard Citation Generator - Cite This For Me](https://www.citethisforme.com/citation-generator/harvard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjV4kPz449Qf"
      },
      "source": [
        "### 🎉🎉🎉 Github 완료! 🎉🎉🎉\n",
        "\n",
        "```python\n",
        "🦆\n",
        "마침내 끝에 다다렀군요! 이제는 헤어질 시간이에요!\n",
        "함께 공부한 것을 잊지 못할거예요!\n",
        "\n",
        "그럼 부스트캠프의 여정 끝나면 지구에서 다시 만나요!\n",
        "저는 이만 지구로 돌아가볼게요!\n",
        "```\n",
        "```python\n",
        "🐦 저도 가볼게요! 안녕!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixzc0txuqmDb"
      },
      "source": [
        "## 🎉🎊🎉 축하드려요! 끝까지 해내셨군요! 🎉🎊🎉\n",
        "> 이번 과제에는 custom 모델에 대한 다양하면서도 결코 적지 않은 내용이 담겨져 있었습니다! 이제 여러분은 custom 모델을 만들기 위해서 무엇이 필요한지 알고 부족한 것이 있다면 스스로 찾아내 채워낼 수 있으며 스스로 원하는 모델을 구축해나갈 수 있는 지식을 얻었습니다! 여기까지 오는 과정이 결코 쉽지만은 않았겠지만 분명 가치가 있었기를 바랍니다. 행운을 빕니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "v_fiPQLq700o"
      },
      "source": [
        "#@title 부덕이가 축하의 춤을 춘대요!\n",
        "from IPython.display import Image\n",
        "Image(url='https://post-phinf.pstatic.net/MjAxODEyMzFfMTAw/MDAxNTQ2MjE0OTg5NjAz.EHOabmOFRb9Sd4H1C8xJWAjDd-AalUHZ0mGRQc8nLJgg.QaKd2fe0gct3mQ-Ex-8qqSS1RVXjoC_-NLXo80sAQNsg.GIF/mug_obj_201812310909504083.gif?type=w1080')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Eo7wLf2p7x_q"
      },
      "source": [
        "#@title 브레이크댄스!\n",
        "from IPython.display import Image\n",
        "Image(url='https://www.wetrend.co.kr/data/editor2/wit_board/2102/07/1612656261_3c92e94dc4e55df68f76b46053008df8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_9eeUgvl7M3O"
      },
      "source": [
        "#@title 여기서 멈출 수 없지! 춤춰 친구들!\n",
        "from IPython.display import Image\n",
        "Image(url='https://i.pinimg.com/originals/29/04/24/29042493fb118029b9014e4cb800c7ee.gif')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}